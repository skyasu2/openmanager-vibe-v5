# AI Engine Architecture

> **v5.88.0** | Updated 2026-01-18

## Overview

The AI Engine for OpenManager Vibe is a **Multi-Agent System** built on **Vercel AI SDK** with `@ai-sdk-tools/agents`. It uses a dual-mode Supervisor pattern with specialized agents for different tasks, running on **Google Cloud Run** with frontend on **Vercel**.

## Architecture (v5.87.0, Updated 2026-01-13)

### Deployment Mode

| Mode | Backend | Status |
|------|---------|--------|
| **Cloud Run** | `cloud-run/ai-engine/` (Vercel AI SDK) | âœ… Active (Primary) |
| **Vercel** | `src/app/` (Next.js Frontend) | âœ… Active (Frontend Only) |
| ~~Cloud Run~~ | ~~`cloud-run/rust-inference/`~~ | âŒ Removed |
| ~~Cloud Run~~ | ~~LangGraph/LangChain~~ | âŒ Removed (v5.92.0) |

> **Note**: LangGraph/LangChain migrated to Vercel AI SDK (2025-12-28) due to Cerebras multi-turn tool calling limitations. New architecture uses `@ai-sdk-tools/agents` for multi-agent orchestration.

### Agent Stack

| Agent | Primary Provider | Fallback | Role | Tools |
|-------|------------------|----------|------|-------|
| **Orchestrator** | Cerebras llama-3.3-70b | Mistral mistral-small-2506 | Fast intent routing (~200ms) | Agent handoffs |
| **NLQ Agent** | Cerebras llama-3.3-70b | Groq llama-3.3-70b-versatile | Server metrics queries (simple + complex) | `getServerMetrics`, `getServerMetricsAdvanced`, `filterServers` |
| **Analyst Agent** | Groq llama-3.3-70b-versatile | Cerebras llama-3.3-70b | Anomaly detection, trend prediction | `detectAnomalies`, `predictTrends`, `analyzePattern`, `correlateMetrics`, `findRootCause` |
| **Reporter Agent** | Groq llama-3.3-70b-versatile | Cerebras llama-3.3-70b | Incident reports, timeline | `buildIncidentTimeline`, `findRootCause`, `correlateMetrics`, `searchKnowledgeBase` |
| **Advisor Agent** | Mistral mistral-small-2506 | Groq llama-3.3-70b-versatile | Troubleshooting, knowledge search | `searchKnowledgeBase` (GraphRAG), `recommendCommands` |
| **Verifier** | Mistral mistral-small-2506 | Cerebras llama-3.3-70b | Response validation | N/A |

> **Note**: ì‹¤ì œ exportë˜ëŠ” AgentëŠ” 4ê°œ (NLQ, Analyst, Reporter, Advisor)ì…ë‹ˆë‹¤. VerifierëŠ” ë³„ë„ì˜ ê²€ì¦ ì»´í¬ë„ŒíŠ¸ë¡œ, ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ì„ ë‹´ë‹¹í•˜ë©° Agent handoff ëŒ€ìƒì´ ì•„ë‹™ë‹ˆë‹¤.

> **Dual-Mode Strategy**: Single-agent mode for simple queries (low latency), Multi-agent mode for complex queries (specialized handling). Cerebras for fast routing/NLQ, Groq for analysis/reporting stability.

### Frontend Features â†’ Agent Mapping

| Feature | Vercel API Route | Cloud Run Endpoint | Primary Agent | Handoff Agents |
|---------|------------------|-------------------|---------------|----------------|
| **AI Chat (NLQ)** | `/api/ai/supervisor` | `/api/ai/supervisor` | Orchestrator | NLQ, Analyst, Reporter, Advisor |
| **Auto Incident Report** | `/api/ai/incident-report` | `/api/ai/incident-report` | Reporter | - (Direct call) |
| **Intelligent Monitoring** | `/api/ai/intelligent-monitoring` | `/api/ai/analyze-server` | Analyst | - (Direct call) |

> **Note**: AdvisorëŠ” Chatì„ í†µí•œ Orchestrator handoffë¡œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤ (ì „ìš© UI ì—†ìŒ).

### Free Tier Limits (2025-01 ê¸°ì¤€)

| Provider | Daily Limit | TPM | RPM | Usage |
|----------|-------------|-----|-----|-------|
| **Cerebras** | 1M tokens/day | 60K | 30 | Orchestrator, NLQ Agent |
| **Groq** | ~1K requests/day | 12K | Variable | Analyst, Reporter Agent |
| **Mistral** | Limited (may require paid) | - | - | Advisor, Verifier |

> **ì£¼ì˜**: OpenRouter ë¬´ë£Œ ëª¨ë¸ì€ ì¼ì¼ 50íšŒ ì œí•œìœ¼ë¡œ ì €ì‚¬ìš©ëŸ‰ ì‹œë‚˜ë¦¬ì˜¤ì—ë§Œ ì í•©í•©ë‹ˆë‹¤.

### Key Features

- **Dual-Mode Supervisor**: Single-agent (simple) vs Multi-agent (complex) mode auto-selection
- **Agent Handoffs**: Pattern-based routing with `matchOn` keywords and regex
- **Multi-Step Tool Calling**: Vercel AI SDK `maxSteps` for reliable tool execution
- **Fallback Chains**: Per-agent provider fallbacks (Cerebras â†’ Groq, Groq â†’ Cerebras)
- **User-Triggered Design**: All AI features are explicitly user-initiated (no auto-triggers)
- **Circuit Breaker**: Model health monitoring with automatic failover
- **GraphRAG Integration**: Advisor agent uses hybrid vector + graph search
- **Protocol Adaptation**: SSE with Keep-Alive to prevent timeouts
- **Response Verification**: Verifier agent validates outputs before response

#### New in v5.87.0 (2026-01-13)

- **LangGraph â†’ Vercel AI SDK Migration**: Complete rewrite using `@ai-sdk-tools/agents`
- **Dual-Mode Supervisor**: Auto-selects single vs multi-agent based on query complexity
- **Agent Specialization**:
  - NLQ Agent (Cerebras â†’ Groq fallback): Simple + complex server queries
  - Analyst Agent (Groq â†’ Cerebras fallback): Anomaly detection, trend prediction
  - Reporter Agent (Groq â†’ Cerebras fallback): Incident reports, timeline
  - Advisor Agent (Mistral): Troubleshooting with GraphRAG
- **Fallback Optimization**: Cerebras for fast routing, Groq for analysis stability
- **Test Coverage**: 65 unit tests including multi-agent orchestrator tests

#### Previous Versions

<details>
<summary>v5.91.0 and earlier (LangGraph era)</summary>

**v5.91.0** (LangGraph)
- RCA Agent, Capacity Agent, Agent Dependencies
- Workflow Caching, Web Search Migration to Tavily

**v5.90.0**
- Triple-Provider Strategy, Rate Limit Distribution

**v5.89.0**
- Dual-Provider Architecture, Advanced NLQ Tool

**v5.88.0**
- Gemini API Key Failover, LangChain maxRetries Fix

**v5.87.0**
- GraphRAG Hybrid Search, Redis L2 Caching

</details>

### Resilience & Performance

#### 3-Way Provider Fallback

ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” 3ì¤‘ Fallback ì²´ê³„ë¥¼ ê°–ì¶”ê³  ìˆì–´ íŠ¹ì • ì œê³µì(API) ì¥ì•  ì‹œ ìë™ìœ¼ë¡œ ë‹¤ìŒ ìˆœìœ„ ëª¨ë¸ë¡œ ì „í™˜ë©ë‹ˆë‹¤.

```
Primary (Cerebras) â”€â”€[fail]â”€â”€â–º Secondary (Groq) â”€â”€[fail]â”€â”€â–º Tertiary (Mistral)
       â”‚                              â”‚                            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ Success
                         Response to User
```

| Agent | Primary | Secondary | Tertiary |
|-------|---------|-----------|----------|
| Orchestrator | Cerebras | Mistral | - |
| NLQ Agent | Cerebras | Groq | Mistral |
| Analyst Agent | Groq | Cerebras | Mistral |
| Reporter Agent | Groq | Cerebras | Mistral |
| Advisor Agent | Mistral | Groq | - |

#### Circuit Breaker & Retry

| ë©”ì»¤ë‹ˆì¦˜ | ë™ì‘ |
|---------|------|
| **Circuit Breaker** | API ì‹¤íŒ¨ ë°˜ë³µ ì‹œ í•´ë‹¹ ì œê³µìë¥¼ ì¼ì‹œ ì°¨ë‹¨í•˜ì—¬ ë¶ˆí•„ìš”í•œ ëŒ€ê¸° ì‹œê°„ ê°ì†Œ |
| **Exponential Backoff** | Rate Limit(429) ë°œìƒ ì‹œ ì§€ìˆ˜ì ìœ¼ë¡œ ì¬ì‹œë„ ê°„ê²© ì¦ê°€ (1s â†’ 2s â†’ 4s) |
| **Health Check** | ì£¼ê¸°ì ìœ¼ë¡œ ì°¨ë‹¨ëœ ì œê³µìì˜ ë³µêµ¬ ìƒíƒœ í™•ì¸ í›„ ìë™ ë³µì› |

#### Fast Path & Forced Routing

LLM í˜¸ì¶œ ì—†ì´ RegExp ê¸°ë°˜ Pre-filterê°€ ì²˜ë¦¬í•˜ì—¬ ì†ë„ì™€ ë¹„ìš©ì„ ìµœì í™”í•©ë‹ˆë‹¤.

| íŒ¨í„´ | ì²˜ë¦¬ ë°©ì‹ | ì˜ˆì‹œ |
|------|----------|------|
| **Fast Path** | ë‹¨ìˆœ ì¸ì‚¬ë§ì€ LLM ì—†ì´ ì¦‰ì‹œ ì‘ë‹µ | "ì•ˆë…•", "ê³ ë§ˆì›Œ" |
| **Forced Routing** | í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ íŠ¹ì • ì—ì´ì „íŠ¸ ì§ì ‘ í˜¸ì¶œ | "ë³´ê³ ì„œ ë§Œë“¤ì–´ì¤˜" â†’ Reporter |
| **LLM Routing** | ë³µì¡í•œ ì˜ë„ëŠ” Orchestratorê°€ LLMìœ¼ë¡œ íŒë‹¨ | "ì™œ ì„œë²„ê°€ ëŠë ¤ì¡Œì–´?" |

```typescript
// Pre-filter ìš°ì„ ìˆœìœ„
1. Fast Path Check (RegExp)     // ~1ms
2. Forced Routing (Keywords)    // ~1ms
3. LLM Intent Classification    // ~200ms (Cerebras)
```

#### Observability (Langfuse)

ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ì‹¤í–‰ ê³¼ì •ì´ Langfuseë¡œ ì¶”ì ë˜ì–´ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

| ì¶”ì  í•­ëª© | ì„¤ëª… |
|----------|------|
| **Input/Output** | ê° ì—ì´ì „íŠ¸ì˜ ì…ë ¥ ë©”ì‹œì§€ ë° ì‘ë‹µ |
| **Tool Calls** | í˜¸ì¶œëœ ë„êµ¬ ëª©ë¡ ë° ê²°ê³¼ |
| **Latency** | ì—ì´ì „íŠ¸ë³„ ì²˜ë¦¬ ì‹œê°„ |
| **Token Usage** | Prompt/Completion í† í° ìˆ˜ |
| **Handoff Chain** | ì—ì´ì „íŠ¸ ê°„ ìœ„ì„ ê²½ë¡œ |

### Agent Communication Patterns

| Pattern | Description | Use Case |
|---------|-------------|----------|
| **Return-to-Supervisor** | Agent sets `returnToSupervisor=true` | Need different agent's expertise |
| **Command Pattern** | Explicit `toAgent` in DelegationRequest | Direct delegation to specific agent |
| **Verification Loop** | Verifier checks output before response | Quality assurance & hallucination check |

## Architecture Diagram

> ğŸ“Š **High-Resolution Diagram**: [AI Engine Architecture (PNG)](../../system/diagrams/ai-engine-architecture.png) | [SVG](../../system/diagrams/ai-engine-architecture.svg)
>
> *Source: [`ai-engine-architecture.mmd`](../../system/ai-engine-architecture.mmd) - Updated 2025-12-30*

```mermaid
graph TD
    Client[Client UI] -->|POST /api/ai/supervisor| API[Next.js API Route]

    subgraph "Vercel (Frontend)"
        API -->|Proxy| CloudRun
    end

    subgraph "Google Cloud Run"
        CloudRun[AI Engine] --> ModeSelect{Mode Selection}

        ModeSelect -->|Simple Query| SingleAgent[Single-Agent Mode]
        ModeSelect -->|Complex Query| MultiAgent[Multi-Agent Mode]

        subgraph "Single-Agent (Low Latency)"
            SingleAgent --> Tools[Multi-Step Tool Calling]
            Tools -->|Cerebras/Mistral| Response1[Response]
        end

        subgraph "Multi-Agent (Specialized)"
            MultiAgent --> Orchestrator[Orchestrator Agent]
            Orchestrator -->|matchOn Pattern| NLQ[NLQ Agent<br/>Cerebras/Groq]
            Orchestrator -->|matchOn Pattern| Analyst[Analyst Agent<br/>Groq/Cerebras]
            Orchestrator -->|matchOn Pattern| Reporter[Reporter Agent<br/>Groq/Cerebras]
            Orchestrator -->|matchOn Pattern| Advisor[Advisor Agent<br/>Mistral]

            NLQ -->|Handoff| Analyst
            Analyst -->|Handoff| Reporter
            Reporter --> Verifier{Verifier}
            Advisor -->|GraphRAG| RAG[(Knowledge Base)]
        end

        Verifier --> Response2[Response]
    end

    subgraph "Data Layer"
        NLQ --> Metrics[(Server Metrics)]
        Analyst --> Metrics
        Reporter --> RAG
        Orchestrator --> Cache[(Redis L2 Cache)]
    end

    Response1 --> Client
    Response2 --> Client
```

<details>
<summary>ğŸ“‹ ASCII Fallback (Mermaid ë Œë”ë§ ì‹¤íŒ¨ ì‹œ)</summary>

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                  Client UI                          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚ POST /api/ai/supervisor
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              Vercel (Frontend)                      â”‚
                    â”‚  Next.js API Route â”€â”€â–º Proxy to Cloud Run          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            Google Cloud Run (AI Engine)             â”‚
                    â”‚                                                     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                    â”‚  â”‚            Mode Selection                    â”‚   â”‚
                    â”‚  â”‚    Simple â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€ Complex       â”‚   â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                    â”‚                       â”‚                              â”‚
                    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
                    â”‚    â”‚                  â”‚                   â”‚         â”‚
                    â”‚    â–¼                  â–¼                   â”‚         â”‚
                    â”‚  Single-Agent     Multi-Agent             â”‚         â”‚
                    â”‚  (Cerebras)       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
                    â”‚                   â”‚   Orchestrator        â”‚         â”‚
                    â”‚                   â”‚         â”‚             â”‚         â”‚
                    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”‚
                    â”‚           â–¼       â–¼    â–¼    â–¼    â–¼        â”‚         â”‚
                    â”‚         NLQ   Analyst Reporter Advisor    â”‚         â”‚
                    â”‚           â”‚       â”‚     â”‚       â”‚         â”‚         â”‚
                    â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚         â”‚
                    â”‚                   â”‚                        â”‚         â”‚
                    â”‚               Verifier                     â”‚         â”‚
                    â”‚                   â”‚                        â”‚         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
                                        â”‚                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                Data Layer                            â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
                    â”‚  â”‚   Redis    â”‚  â”‚  Supabase  â”‚  â”‚  pgVector  â”‚     â”‚
                    â”‚  â”‚   Cache    â”‚  â”‚  Metrics   â”‚  â”‚    RAG     â”‚     â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Interactive Diagrams (FigJam)

| Diagram | Description | Link |
|---------|-------------|------|
| **System Architecture** | Full AI engine overview | [View](https://www.figma.com/online-whiteboard/create-diagram/9a4b29bd-0376-4e0a-8e22-3b9bd008854a) |
| **Agent Routing Flow** | Supervisor â†’ Agent routing | [View](https://www.figma.com/online-whiteboard/create-diagram/22dbc5b3-44c1-44e7-9eee-1fa0cf8e402a) |
| **Multi-Agent Communication** | Inter-agent delegation | [View](https://www.figma.com/online-whiteboard/create-diagram/a32f26ab-5d3c-40f6-a8ed-4eb5ec0ed843) |
| **Supervisor Execution Flow** | Query â†’ Supervisor â†’ Agents â†’ Verifier flow | [View](https://www.figma.com/online-whiteboard/create-diagram/eb37f54b-2795-4320-bd2e-c41854a7ec52) |

## State Interfaces

### Vercel AI SDK Types (v5.87.0)

The core interfaces for AI SDK multi-agent orchestration:

### SupervisorRequest

```typescript
interface SupervisorRequest {
  messages: Array<{ role: 'user' | 'assistant'; content: string }>;
  sessionId: string;
  enableTracing?: boolean;
  mode?: 'single' | 'multi' | 'auto';  // Execution mode
}
```

### SupervisorResponse

```typescript
interface SupervisorResponse {
  success: boolean;
  response: string;
  toolsCalled: string[];              // Names of tools invoked
  toolResults: Record<string, unknown>[];
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  metadata: {
    provider: string;                 // 'cerebras' | 'groq' | 'mistral'
    modelId: string;
    stepsExecuted: number;            // Multi-step tool calling count
    durationMs: number;
    mode?: 'single' | 'multi';
    handoffs?: Array<{ from: string; to: string; reason?: string }>;
    finalAgent?: string;              // Last agent that handled the query
  };
}
```

### MultiAgentRequest/Response

```typescript
interface MultiAgentRequest {
  messages: Array<{ role: 'user' | 'assistant'; content: string }>;
  sessionId: string;
  enableTracing?: boolean;
}

interface MultiAgentResponse {
  success: true;
  response: string;
  toolsCalled: string[];
  handoffs: Array<{ from: string; to: string; reason?: string }>;
  finalAgent: string;
  usage: { promptTokens: number; completionTokens: number; totalTokens: number };
  metadata: { provider: string; modelId: string; totalRounds: number; durationMs: number };
}
```

<details>
<summary>Legacy LangGraph Types (v5.91.0 and earlier)</summary>

### AgentState (16 Fields)

| Field | Type | Purpose |
|-------|------|---------|
| `messages` | BaseMessage[] | Conversation history |
| `sessionId` | string | Session identifier |
| `routerDecision` | RouterDecision | Supervisor routing decision |
| `delegationRequest` | DelegationRequest | A2A delegation info |
| `agentResults` | AgentResult[] | Results from executed agents |
| `modelHealth` | CircuitBreakerState | Model health tracking |
| `finalResponse` | string | Final response to user |

</details>

## API Specification

### Main Endpoint

**`POST /api/ai/supervisor`** - Multi-Agent AI Processing (Cloud Run)

### Request Format

```json
{
  "messages": [
    { "role": "user", "content": "ì„œë²„ 5ë²ˆ CPU ìƒíƒœ ì•Œë ¤ì¤˜" }
  ],
  "sessionId": "optional-session-id"
}
```

### Response Format (Streaming - AI SDK v5 Protocol)
The Cloud Run engine uses a simulated streaming protocol compatible with Vercel AI SDK v5, enhanced with custom events.

```
Headers:
- Content-Type: text/event-stream; charset=utf-8
- X-Vercel-AI-Data-Stream: v1
- X-Backend: cloud-run

Body Parts:
0:"Hello! I'm checking the server status..."  // Text content
8:[{"type":"progress","message":"Analyzing metrics..."}] // Custom annotation (Keep-alive)
8:[{"type":"verification","isValid":true,"confidence":0.98}] // Verification result
d:{"finishReason":"stop","verified":true}     // Finish signal
```

> **Note**: The frontend proxy (`src/app/api/ai/supervisor/route.ts`) parses this stream and converts `0:` parts to plain text for the client, while handling `8:` parts for UI updates (progress, verification status).

### Additional Cloud Run Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/api/ai/embedding` | POST | Single text embedding |
| `/api/ai/embedding/batch` | POST | Batch text embeddings |
| `/api/ai/embedding/stats` | GET | Embedding service statistics |
| `/api/ai/generate` | POST | Text generation (non-streaming) |
| `/api/ai/generate/stream` | POST | Text generation (streaming SSE) |
| `/api/ai/generate/stats` | GET | Generate service statistics |
| `/api/ai/graphrag/search` | POST | GraphRAG hybrid search |
| `/api/ai/graphrag/stats` | GET | GraphRAG statistics |
| `/api/ai/cache/stats` | GET | Redis cache statistics |
| `/api/ai/cache/invalidate` | POST | Invalidate cache entries |
| `/rag/sync-incidents` | POST | Manual RAG incident sync |
| `/rag/stats` | GET | RAG injection statistics |
| `/health` | GET | Health check |
| `/warmup` | GET | Cold start warmup |
| `/api/jobs/process` | POST | Async job processing (from Vercel) |
| `/api/jobs/:id` | GET | Get job result from Redis |
| `/api/jobs/:id/progress` | GET | Get job progress for UI feedback |

### Async Job Queue (v5.89.1, Updated 2025-12-30)

For long-running AI queries that may exceed Vercel's 120s timeout, an async job queue pattern is implemented:

```mermaid
sequenceDiagram
    participant Client
    participant Vercel as Vercel API
    participant CloudRun as Cloud Run
    participant Redis as Upstash Redis
    participant Supabase

    Client->>Vercel: POST /api/ai/jobs (query)
    Vercel->>Supabase: Insert job (status: queued)
    Vercel->>Redis: Set initial status (pending, 5% progress)
    Vercel->>CloudRun: POST /api/jobs/process (fire-and-forget)
    Vercel-->>Client: { jobId, pollUrl } (immediate)

    Client->>Vercel: GET /api/ai/jobs/:id/stream (SSE)
    Vercel->>Redis: Poll every 100ms
    Redis-->>Vercel: Initial status (pending)
    Vercel-->>Client: SSE event: progress (5%)

    CloudRun->>Redis: Mark job processing
    CloudRun->>CloudRun: AI Supervisor (Vercel AI SDK)
    CloudRun->>Redis: Update progress (50%)
    Vercel->>Redis: Poll
    Redis-->>Vercel: Progress update
    Vercel-->>Client: SSE event: progress (50%)

    CloudRun->>Redis: Store final result
    Vercel->>Redis: Poll
    Redis-->>Vercel: Job result
    Vercel-->>Client: SSE event: result
```

**Key Components:**
- **Job Creation**: `/api/ai/jobs` creates job in Supabase + Redis initial state
- **Vercel SSE**: `/api/ai/jobs/:id/stream` - Server-side Redis polling with SSE streaming to client
- **Cloud Run Worker**: `/api/jobs/process` - Background job processing
- **Redis Store**: Job results with 5-min TTL (job:*), progress with 5-min TTL (job:progress:*)

**Job Status Flow:**
```
queued (Supabase) â†’ pending (Redis) â†’ processing (Redis) â†’ completed/failed (Redis)
```

**Graceful Degradation:**
- Redis ì¥ì•  ì‹œì—ë„ Job ìƒì„± ì§„í–‰ (Supabase ê¸°ë°˜ í´ë°±)
- SSE ìŠ¤íŠ¸ë¦¼ì€ í´ë§ APIë¡œ í´ë°± ê°€ëŠ¥

**Benefits:**
- No Vercel timeout issues (job creation returns immediately)
- Real-time progress updates via SSE (100ms polling interval)
- Immediate progress feedback (5% on creation, updates during processing)
- 93% reduction in Redis commands vs client polling (6K vs 90K/month)
- Cancellable long-running queries

## Data & Memory

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Vector Store** | Supabase (pgvector) | RAG knowledge base |
| **GraphRAG** | Supabase + pgvector | Hybrid vector + graph knowledge retrieval |
| **Checkpointer** | PostgresCheckpointer | Session state persistence |
| **Redis L2 Cache** | Upstash Redis | Response caching, session optimization (TTL: 1hr) |
| **Metrics History** | Supabase `server_metrics_history` | Server metrics for anomaly detection (6hr window) |
| **Conversation History** | Supabase `conversation_history` | Compressed conversation storage |
| **Realtime** | Supabase Realtime | Live dashboard updates |
| **Client State** | Zustand | Chat history, UI state |

### RAG Incident Injection (v5.85.0, 2025-12-30)

Generated incident reports can be synced to the `knowledge_base` table for RAG search by Reporter Agent.

```mermaid
sequenceDiagram
    participant User
    participant Reporter as Reporter Agent
    participant Injector as RAG Injector
    participant Mistral as Mistral Embedding
    participant KB as knowledge_base

    User->>Reporter: Request Incident Report
    Reporter->>Injector: Trigger Sync
    Injector->>Mistral: embedText(title + content)
    Mistral-->>Injector: 1024-dim Vector
    Injector->>KB: INSERT (embedding, category='incident')
    KB-->>Injector: Success
    Note over KB: pgvector HNSW Index
```

**Key Features:**
- **Sync Trigger**: Manual or auto-sync from incident reports
- **Embedding Model**: Mistral `mistral-embed` (1024-dim)
- **Dedup Logic**: Uses `session_id` in tags to prevent duplicates
- **Content Extraction**: Title, root cause, recommendations, timeline from payload

**Endpoints:**
| Endpoint | Method | Description |
|----------|--------|-------------|
| `/rag/sync-incidents` | POST | Manual sync trigger (limit, daysBack params) |
| `/rag/stats` | GET | Sync statistics (total, synced, pending) |

### GraphRAG Architecture

```mermaid
graph LR
    Query[User Query] --> Embedding[Text Embedding]
    Embedding --> Vector[Vector Search]
    Embedding --> Graph[Graph Traversal]
    Vector --> Merge[Result Merger]
    Graph --> Merge
    Merge --> Rerank[Re-ranking]
    Rerank --> Response[Enhanced Response]
```

GraphRAG combines:
- **Vector Search**: Semantic similarity via pgvector (cosine distance)
- **Graph Traversal**: Entity-relationship exploration for context
- **Hybrid Scoring**: Weighted combination of vector + graph relevance

### Redis Caching Strategy

| Cache Type | TTL | Key Pattern | Purpose |
|------------|-----|-------------|---------|
| **Response Cache** | 1 hour | `ai:response:{hash}` | Repeated query optimization |
| **Session Cache** | 24 hours | `ai:session:{sessionId}` | Conversation state |
| **Embedding Cache** | 7 days | `ai:embed:{hash}` | Embedding reuse |
| **Job Result** | 5 min | `job:{jobId}` | Async job status & result (created by Vercel on job creation) |
| **Job Progress** | 5 min | `job:progress:{jobId}` | Real-time progress updates (SSE polling) |

> **Note (2025-12-30)**: Job entries are initialized by Vercel at job creation time to enable immediate SSE progress feedback. Cloud Run updates the status as processing progresses.

## Environment Variables

### Vercel (Frontend + Proxy)

| Variable | Required | Description |
|----------|----------|-------------|
| `CLOUD_RUN_AI_URL` | Yes | Cloud Run AI Engine endpoint |
| `CLOUD_RUN_API_SECRET` | Yes | Cloud Run authentication secret |
| `CLOUD_RUN_ENABLED` | Yes | Enable Cloud Run backend (`true`) |
| `NEXT_PUBLIC_SUPABASE_URL` | Yes | Supabase project URL |
| `SUPABASE_SERVICE_ROLE_KEY` | Yes | Supabase service role key |
| `USE_LOCAL_DOCKER` | Dev | Force local Docker in development |
| `AI_ENGINE_MODE` | Dev | `AUTO` (local) or `CLOUD` (Cloud Run) |

### Cloud Run (AI Engine)

| Variable | Required | Description |
|----------|----------|-------------|
| `CEREBRAS_API_KEY` | Yes | Cerebras (Llama 3.3-70b) API key - Orchestrator, NLQ |
| `GROQ_API_KEY` | Yes | Groq (Llama 3.3-70b) API key - Analyst, Reporter |
| `MISTRAL_API_KEY` | Yes | Mistral API key - Advisor, Embedding (1024-dim) |
| `CLOUD_RUN_API_SECRET` | Yes | API authentication secret |
| `UPSTASH_REDIS_URL` | Yes | Upstash Redis REST URL |
| `UPSTASH_REDIS_TOKEN` | Yes | Upstash Redis REST token |
| `SUPABASE_URL` | Yes | Supabase project URL (for GraphRAG) |
| `SUPABASE_SERVICE_KEY` | Yes | Supabase service role key |
| `PORT` | No | Server port (default: 8080) |

## File Structure

```
# Cloud Run AI Engine (Primary Backend)
cloud-run/ai-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.ts               # Hono HTTP server (main entry)
â”‚   â”œâ”€â”€ tools-ai-sdk/           # Vercel AI SDK tool definitions
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ cache-layer.ts      # Multi-tier caching (L1: Memory, L2: Redis)
â”‚   â”‚   â”œâ”€â”€ redis-client.ts     # Upstash Redis client (L2 cache)
â”‚   â”‚   â”œâ”€â”€ hybrid-cache.ts     # Multi-tier caching orchestration
â”‚   â”‚   â”œâ”€â”€ graph-rag-service.ts # GraphRAG hybrid search service
â”‚   â”‚   â”œâ”€â”€ embedding.ts        # Text embedding utilities
â”‚   â”‚   â”œâ”€â”€ config-parser.ts    # YAML config parsing
â”‚   â”‚   â”œâ”€â”€ incident-rag-injector.ts # RAG incident injection (v5.85.0)
â”‚   â”‚   â””â”€â”€ context-compression/ # Context compression for long conversations
â”‚   â”‚       â”œâ”€â”€ compression-trigger.ts    # Token threshold detection
â”‚   â”‚       â”œâ”€â”€ summary-generator.ts      # Conversation summarization
â”‚   â”‚       â””â”€â”€ context-compressor.ts     # Compression orchestration
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ jobs.ts             # Async job processing endpoints
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ ai-sdk/             # Vercel AI SDK (v5.87.0 - Primary)
â”‚       â”‚   â”œâ”€â”€ supervisor.ts   # Dual-mode supervisor (single/multi)
â”‚       â”‚   â”œâ”€â”€ supervisor.test.ts
â”‚       â”‚   â”œâ”€â”€ model-provider.ts # Provider factory (Cerebras/Groq/Mistral)
â”‚       â”‚   â””â”€â”€ agents/         # @ai-sdk-tools/agents multi-agent system
â”‚       â”‚       â”œâ”€â”€ index.ts    # Agent exports
â”‚       â”‚       â”œâ”€â”€ orchestrator.ts      # Orchestrator with handoffs
â”‚       â”‚       â”œâ”€â”€ orchestrator.test.ts # Unit tests (19 tests)
â”‚       â”‚       â”œâ”€â”€ nlq-agent.ts         # NLQ (Cerebras â†’ Groq fallback)
â”‚       â”‚       â”œâ”€â”€ analyst-agent.ts     # Analyst (Groq â†’ Cerebras fallback)
â”‚       â”‚       â”œâ”€â”€ reporter-agent.ts    # Reporter (Groq â†’ Cerebras fallback)
â”‚       â”‚       â””â”€â”€ advisor-agent.ts     # Advisor (Mistral + GraphRAG)
â”‚       â”œâ”€â”€ langgraph/          # LangGraph (Legacy - Deprecated)
â”‚       â”‚   â””â”€â”€ multi-agent-supervisor.ts
â”‚       â”œâ”€â”€ embedding/          # Embedding service
â”‚       â”‚   â””â”€â”€ embedding-service.ts
â”‚       â”œâ”€â”€ generate/           # Generate service
â”‚       â”‚   â””â”€â”€ generate-service.ts
â”‚       â””â”€â”€ scenario/           # Demo data loader
â”‚           â””â”€â”€ scenario-loader.ts
â”œâ”€â”€ package.json                # ai, @ai-sdk-tools/agents, hono, @upstash/redis
â””â”€â”€ Dockerfile

# Vercel Proxy Layer
src/lib/ai-proxy/
â””â”€â”€ proxy.ts                    # Cloud Run proxy with env detection

# Vercel API Routes (Proxy to Cloud Run)
src/app/api/ai/
â”œâ”€â”€ supervisor/route.ts         # Main AI endpoint proxy
â”œâ”€â”€ embedding/route.ts          # Embedding proxy
â”œâ”€â”€ generate/route.ts           # Generate proxy
â””â”€â”€ jobs/
    â”œâ”€â”€ route.ts                # Job creation (POST), list (GET)
    â””â”€â”€ [id]/
        â”œâ”€â”€ route.ts            # Job status (GET), cancel (DELETE)
        â”œâ”€â”€ stream/route.ts     # SSE streaming (Redis polling)
        â””â”€â”€ progress/route.ts   # Progress update (PATCH)

# Frontend Hooks
src/hooks/ai/
â”œâ”€â”€ useAsyncAIQuery.ts          # Async AI query with SSE
â””â”€â”€ useJobPolling.ts            # Job polling (legacy fallback)

# Legacy (Deprecated)
src/services/langgraph/         # Superseded by cloud-run/ai-engine/
cloud-run/supabase-mcp/         # Deprecated - direct Supabase JS client
```

## Deprecated Components

| Component | Status | Replacement |
|-----------|--------|-------------|
| `services/langgraph/` (Cloud Run) | Deprecated (2025-12-28) | `services/ai-sdk/` (Vercel AI SDK) |
| `src/services/langgraph/` (Vercel) | Deprecated (2025-12-16) | `cloud-run/ai-engine/` |
| `cloud-run/supabase-mcp/` | Deprecated (2025-12-16) | Direct Supabase JS client |
| GCP VM | Removed (2025-12-16) | Cloud Run |
| `/api/ai/query` | Removed | `/api/ai/supervisor` |
| Python Unified Processor | Removed | Vercel AI SDK agents |
| GCP Cloud Functions | Removed | Cloud Run |
| `ml-analytics-engine` (Python) | Removed | Vercel AI SDK agents |
| `cloud-run/rust-inference/` (Rust) | Removed (2025-12-24) | Vercel AI SDK agents |
| `SmartRoutingEngine` | Removed | AI SDK Dual-mode Supervisor |

## Cloud Run Services

### ai-engine (Vercel AI SDK v5.87.0)

- **Runtime**: Node.js 22 + Hono
- **Framework**: Vercel AI SDK with `@ai-sdk-tools/agents`
- **Architecture**: Dual-mode Supervisor (Single-Agent / Multi-Agent)
- **Models**:
  - Cerebras Llama 3.3-70b (Orchestrator, NLQ Agent primary)
  - Groq Llama 3.3-70b-versatile (Analyst, Reporter primary, NLQ fallback)
  - Mistral mistral-small-2506 (Advisor, Verifier, Embedding)
- **Endpoint**: `https://ai-engine-xxxxx.run.app`
- **Test Coverage**: 65 unit tests (vitest)

---

## Quick Reference

### Health Check

```bash
# AI Engine ìƒíƒœ í™•ì¸
curl https://ai-engine-490817238363.asia-northeast1.run.app/health

# ì˜ˆìƒ ì‘ë‹µ
{
  "status": "healthy",
  "apiKeys": { "mistral": true, "groq": true, "cerebras": true, "tavily": true },
  "supabase": true
}
```

### Query Examples

```bash
# ì„œë²„ ìƒíƒœ ì¡°íšŒ (NLQ Agent)
curl -X POST /api/supervisor \
  -H "Content-Type: application/json" \
  -d '{"query": "web-01 ì„œë²„ CPU ì‚¬ìš©ëŸ‰ ì•Œë ¤ì¤˜"}'

# ì¥ì•  ë¶„ì„ (Multi-Agent)
curl -X POST /api/supervisor \
  -H "Content-Type: application/json" \
  -d '{"query": "ì–´ì œ ë°œìƒí•œ ì¥ì• ì˜ ì›ì¸ì„ ë¶„ì„í•˜ê³  í•´ê²° ë°©ë²•ì„ ì¶”ì²œí•´ì¤˜"}'
```
