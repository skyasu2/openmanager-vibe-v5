# ðŸ—ï¸ Local Full Stack Setup Guide

ì´ ê°€ì´ë“œëŠ” ë¡œì»¬ Docker í™˜ê²½ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì¸í”„ë¼ë¥¼ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤. í˜„ìž¬ í”„ë¡œì íŠ¸ì˜ Docker í™˜ê²½ì€ ë‹¤ìŒ ë‘ ê°€ì§€ ì£¼ìš” ëª©ì ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤:

1.  **GCP Cloud Run ë°°í¬ ì‹œë®¬ë ˆì´ì…˜**: Python ê¸°ë°˜ì˜ Cloud Functions(Unified AI Processor ë“±)ë¥¼ ì‹¤ì œ GCP í™˜ê²½ê³¼ ë™ì¼í•˜ê²Œ ë¡œì»¬ì—ì„œ ì‹¤í–‰í•˜ê³  í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
2.  **í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ë¡œì»¬ ì—ë®¬ë ˆì´ì…˜**: Supabase(Postgres, Auth, Realtime ë“±)ì™€ ê°™ì€ ê´€ë¦¬í˜• í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ë¥¼ ë¡œì»¬ Docker ì»¨í…Œì´ë„ˆë¡œ ëŒ€ì²´í•˜ì—¬ ê°œë°œ ë° í…ŒìŠ¤íŠ¸ ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.

## ðŸŽ¯ Architecture

| Service | Cloud | Local Replacement | Port | Purpose |
|---------|-------|-------------------|------|---------|
| **Database** | Supabase (Cloud) | **Supabase** (Local Docker) | `54322`, `54321` | í´ë¼ìš°ë“œ DB ë“± ì „ì²´ í™˜ê²½ ì—ë®¬ë ˆì´ì…˜ |
| **AI Processing** | Google Cloud Run | **Unified AI Processor** | `8082` | Cloud Run ë°°í¬ ì „ ë¡œì»¬ ì‹œë®¬ë ˆì´ì…˜ |
| **GCP Functions** | Google Cloud Run | **Enhanced Korean NLP** | `8081` | NLP/ML ì—”ì§„ ë¡œì»¬ í…ŒìŠ¤íŠ¸ |
| **AI Intelligence** | Gemini API | **Mock AI** | `8083` | LLM API ë¹„ìš© ì ˆê° ë° ì‹œë®¬ë ˆì´ì…˜ |

---

## ðŸš€ 1. Database Setup (Supabase)

### Start Local Supabase
This starts Postgres, Auth, Realtime, and Studio dashboard locally.

```bash
# Start local Supabase (Docker required)
npx supabase start -y
```

- **Dashboard**: `http://localhost:54323`
- **API URL**: `http://localhost:54321`
- **DB URL**: `postgresql://postgres:postgres@localhost:54322/postgres`

---

## ðŸ¤– 2. AI Services Setup

### Start Local AI Container (Mock Mode)
Use the `mock-ai` service for fast, free testing without needing an LLM key.

```bash
# 1. Update scripts/dev/run-docker-functions.sh first (if needed for new service)
# OR directly use docker-compose:
cd gcp-functions
docker-compose -f docker-compose.dev.yml up mock-ai
```

### Enable Mock Mode in App
Update `.env.local` to point to the Mock AI service instead of the real AI Processor (or configure Processor to use Mock).

**Scenario A: Direct Mock (Bypass Processor)**
```env
# .env.local
# âš ï¸ Note: mock-ai requires /process path (unlike unified-processor which accepts both / and /process)
NEXT_PUBLIC_GCP_UNIFIED_PROCESSOR_ENDPOINT="http://localhost:8083/process"
```

> ðŸ’¡ **Endpoint ì°¨ì´ì **:
> - `unified-ai-processor` (8082): `http://localhost:8082` (Base URL ê¶Œìž¥)
> - `mock-ai` (8083): `http://localhost:8083/process` (`/process` í•„ìˆ˜)

**Scenario B: Processor -> Mock (Advanced)**
If you want to test the Processor logic but mock the LLM, you would need to configure the Unified Processor to call the Mock AI. (Currently configured for Scenario A).

---

## ðŸ§¹ Cleanup

```bash
# Stop AI Services
cd gcp-functions && docker-compose -f docker-compose.dev.yml down

# Stop Supabase
npx supabase stop
```
