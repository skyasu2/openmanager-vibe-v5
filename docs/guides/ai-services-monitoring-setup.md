# ğŸ¤– AI ì„œë¹„ìŠ¤ & ëª¨ë‹ˆí„°ë§ ì„¤ì • ê°€ì´ë“œ

> **AI í†µí•© + í—¬ìŠ¤ì²´í¬ + ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**  
> ìµœì¢… ì—…ë°ì´íŠ¸: 2025-08-18  
> ì„œë¹„ìŠ¤: Claude + Google AI + OpenAI + í†µí•© ëª¨ë‹ˆí„°ë§

## ğŸ¯ ê°œìš”

OpenManager VIBE v5ì˜ AI ì„œë¹„ìŠ¤ í†µí•© ê´€ë¦¬, ì‹¤ì‹œê°„ í—¬ìŠ¤ì²´í¬ ì‹œìŠ¤í…œ, ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ì„ ì™„ì „íˆ ì„¤ì •í•˜ê³  ê´€ë¦¬í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

1. [AI ì„œë¹„ìŠ¤ í†µí•©](#ai-ì„œë¹„ìŠ¤-í†µí•©)
2. [ëª¨ë‹ˆí„°ë§ ë° í—¬ìŠ¤ì²´í¬](#ëª¨ë‹ˆí„°ë§-ë°-í—¬ìŠ¤ì²´í¬)
3. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
4. [ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ê°€ì´ë“œ](#ì²´í¬ë¦¬ìŠ¤íŠ¸-ë°-ê°€ì´ë“œ)
5. [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)

## ğŸ¤– AI ì„œë¹„ìŠ¤ í†µí•©

### 1ë‹¨ê³„: AI ì„œë¹„ìŠ¤ ì¶”ìƒí™”

```typescript
// src/lib/ai/ai-service-manager.ts
export interface AIServiceConfig {
  name: string;
  endpoint: string;
  apiKey: string;
  enabled: boolean;
  rateLimit: number;
}

export interface AIResponse {
  content: string;
  model: string;
  usage: {
    inputTokens: number;
    outputTokens: number;
    totalTokens: number;
  };
  timestamp: Date;
}

export class AIServiceManager {
  private services: Map<string, AIServiceConfig> = new Map();

  constructor() {
    this.initializeServices();
  }

  private initializeServices() {
    // Claude AI
    this.services.set('claude', {
      name: 'Claude',
      endpoint: 'https://api.anthropic.com',
      apiKey: process.env.CLAUDE_API_KEY || '',
      enabled: !!process.env.CLAUDE_API_KEY,
      rateLimit: 60, // requests per minute
    });

    // Google AI
    this.services.set('google', {
      name: 'Google AI',
      endpoint: 'https://generativelanguage.googleapis.com',
      apiKey: process.env.GOOGLE_AI_API_KEY || '',
      enabled: !!process.env.GOOGLE_AI_API_KEY,
      rateLimit: 15,
    });

    // OpenAI
    this.services.set('openai', {
      name: 'OpenAI',
      endpoint: 'https://api.openai.com',
      apiKey: process.env.OPENAI_API_KEY || '',
      enabled: !!process.env.OPENAI_API_KEY,
      rateLimit: 50,
    });
  }

  async callAI(
    serviceName: string,
    prompt: string,
    options: {
      model?: string;
      maxTokens?: number;
      temperature?: number;
    } = {}
  ): Promise<AIResponse> {
    const service = this.services.get(serviceName);

    if (!service || !service.enabled) {
      throw new Error(`AI ì„œë¹„ìŠ¤ '${serviceName}'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤`);
    }

    // ì‹¤ì œ AI ì„œë¹„ìŠ¤ í˜¸ì¶œ êµ¬í˜„
    // ê° ì„œë¹„ìŠ¤ë³„ API í˜¸ì¶œ ë¡œì§
    return this.executeAICall(service, prompt, options);
  }

  private async executeAICall(
    service: AIServiceConfig,
    prompt: string,
    options: any
  ): Promise<AIResponse> {
    // ì„œë¹„ìŠ¤ë³„ êµ¬í˜„
    switch (service.name) {
      case 'Claude':
        return this.callClaude(service, prompt, options);
      case 'Google AI':
        return this.callGoogleAI(service, prompt, options);
      case 'OpenAI':
        return this.callOpenAI(service, prompt, options);
      default:
        throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” AI ì„œë¹„ìŠ¤: ${service.name}`);
    }
  }

  private async callClaude(
    service: AIServiceConfig,
    prompt: string,
    options: any
  ): Promise<AIResponse> {
    // Claude API í˜¸ì¶œ êµ¬í˜„
    const response = await fetch(`${service.endpoint}/v1/messages`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'x-api-key': service.apiKey,
        'anthropic-version': '2023-06-01',
      },
      body: JSON.stringify({
        model: options.model || 'claude-3-sonnet-20240229',
        max_tokens: options.maxTokens || 1024,
        temperature: options.temperature || 0.1,
        messages: [{ role: 'user', content: prompt }],
      }),
    });

    if (!response.ok) {
      throw new Error(`Claude API ì˜¤ë¥˜: ${response.statusText}`);
    }

    const data = await response.json();
    
    return {
      content: data.content[0].text,
      model: data.model,
      usage: {
        inputTokens: data.usage.input_tokens,
        outputTokens: data.usage.output_tokens,
        totalTokens: data.usage.input_tokens + data.usage.output_tokens,
      },
      timestamp: new Date(),
    };
  }

  private async callGoogleAI(
    service: AIServiceConfig,
    prompt: string,
    options: any
  ): Promise<AIResponse> {
    // Google AI API í˜¸ì¶œ êµ¬í˜„
    const model = options.model || 'gemini-pro';
    const response = await fetch(
      `${service.endpoint}/v1/models/${model}:generateContent?key=${service.apiKey}`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [{ parts: [{ text: prompt }] }],
          generationConfig: {
            temperature: options.temperature || 0.1,
            maxOutputTokens: options.maxTokens || 1024,
          },
        }),
      }
    );

    if (!response.ok) {
      throw new Error(`Google AI API ì˜¤ë¥˜: ${response.statusText}`);
    }

    const data = await response.json();
    const content = data.candidates[0].content.parts[0].text;
    
    return {
      content,
      model,
      usage: {
        inputTokens: data.usageMetadata?.promptTokenCount || 0,
        outputTokens: data.usageMetadata?.candidatesTokenCount || 0,
        totalTokens: data.usageMetadata?.totalTokenCount || 0,
      },
      timestamp: new Date(),
    };
  }

  private async callOpenAI(
    service: AIServiceConfig,
    prompt: string,
    options: any
  ): Promise<AIResponse> {
    // OpenAI API í˜¸ì¶œ êµ¬í˜„
    const response = await fetch(`${service.endpoint}/v1/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${service.apiKey}`,
      },
      body: JSON.stringify({
        model: options.model || 'gpt-3.5-turbo',
        messages: [{ role: 'user', content: prompt }],
        max_tokens: options.maxTokens || 1024,
        temperature: options.temperature || 0.1,
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API ì˜¤ë¥˜: ${response.statusText}`);
    }

    const data = await response.json();
    
    return {
      content: data.choices[0].message.content,
      model: data.model,
      usage: {
        inputTokens: data.usage.prompt_tokens,
        outputTokens: data.usage.completion_tokens,
        totalTokens: data.usage.total_tokens,
      },
      timestamp: new Date(),
    };
  }

  getAvailableServices(): string[] {
    return Array.from(this.services.entries())
      .filter(([, service]) => service.enabled)
      .map(([name]) => name);
  }

  getServiceStatus(): Record<string, boolean> {
    const status: Record<string, boolean> = {};
    this.services.forEach((service, name) => {
      status[name] = service.enabled;
    });
    return status;
  }
}
```

### 2ë‹¨ê³„: AI ì„œë¹„ìŠ¤ ë ˆì´íŠ¸ ë¦¬ë¯¸í„°

```typescript
// src/lib/ai/rate-limiter.ts
export class AIRateLimiter {
  private requestCounts: Map<string, number[]> = new Map();
  private limits: Map<string, number> = new Map();

  constructor() {
    this.limits.set('claude', 60); // 60 requests per minute
    this.limits.set('google', 15); // 15 requests per minute
    this.limits.set('openai', 50); // 50 requests per minute
  }

  async checkRateLimit(serviceName: string): Promise<boolean> {
    const now = Date.now();
    const windowStart = now - 60000; // 1 minute window
    
    const requests = this.requestCounts.get(serviceName) || [];
    const recentRequests = requests.filter(time => time > windowStart);
    
    const limit = this.limits.get(serviceName) || 10;
    
    if (recentRequests.length >= limit) {
      return false; // Rate limit exceeded
    }
    
    // Add current request
    recentRequests.push(now);
    this.requestCounts.set(serviceName, recentRequests);
    
    return true;
  }

  getRemainingRequests(serviceName: string): number {
    const now = Date.now();
    const windowStart = now - 60000;
    
    const requests = this.requestCounts.get(serviceName) || [];
    const recentRequests = requests.filter(time => time > windowStart);
    
    const limit = this.limits.get(serviceName) || 10;
    return Math.max(0, limit - recentRequests.length);
  }

  getResetTime(serviceName: string): Date {
    const requests = this.requestCounts.get(serviceName) || [];
    if (requests.length === 0) {
      return new Date();
    }
    
    const oldestRequest = Math.min(...requests);
    return new Date(oldestRequest + 60000);
  }
}
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° í—¬ìŠ¤ì²´í¬

### í†µí•© í—¬ìŠ¤ì²´í¬ ì‹œìŠ¤í…œ

```typescript
// src/lib/monitoring/integrated-health-check.ts
import { SupabaseService } from '@/lib/supabase/client';
import { VectorService } from '@/lib/pgvector/vector-service';
import { MCPManager } from '@/lib/mcp/mcp-manager';
import { AIServiceManager } from '@/lib/ai/ai-service-manager';

export interface ServiceHealthStatus {
  service: string;
  status: 'healthy' | 'degraded' | 'unhealthy';
  responseTime: number;
  lastCheck: Date;
  details?: any;
}

export class IntegratedHealthCheck {
  private aiManager = new AIServiceManager();

  async checkAllServices(): Promise<ServiceHealthStatus[]> {
    const checks = await Promise.allSettled([
      this.checkSupabase(),
      this.checkPgVector(),
      this.checkMCPServers(),
      this.checkAIServices(),
    ]);

    return checks
      .map((result, index) => {
        if (result.status === 'fulfilled') {
          return result.value;
        } else {
          const serviceNames = ['Supabase', 'pgvector', 'MCP', 'AI Services'];
          return {
            service: serviceNames[index],
            status: 'unhealthy' as const,
            responseTime: 0,
            lastCheck: new Date(),
            details: { error: result.reason?.message },
          };
        }
      })
      .flat();
  }

  private async checkSupabase(): Promise<ServiceHealthStatus> {
    const startTime = Date.now();

    try {
      // ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸
      await SupabaseService.getServerStatistics('test-user-id');

      return {
        service: 'Supabase',
        status: 'healthy',
        responseTime: Date.now() - startTime,
        lastCheck: new Date(),
      };
    } catch (error) {
      return {
        service: 'Supabase',
        status: 'unhealthy',
        responseTime: Date.now() - startTime,
        lastCheck: new Date(),
        details: { error: error.message },
      };
    }
  }

  private async checkPgVector(): Promise<ServiceHealthStatus> {
    const startTime = Date.now();

    try {
      const stats = await VectorService.getVectorStats();

      return {
        service: 'pgvector',
        status: 'healthy',
        responseTime: Date.now() - startTime,
        lastCheck: new Date(),
        details: stats,
      };
    } catch (error) {
      return {
        service: 'pgvector',
        status: 'unhealthy',
        responseTime: Date.now() - startTime,
        lastCheck: new Date(),
        details: { error: error.message },
      };
    }
  }

  private async checkMCPServers(): Promise<ServiceHealthStatus> {
    const startTime = Date.now();

    try {
      const healthReport = await MCPManager.generateHealthReport();

      const status =
        healthReport.healthyPercentage >= 80
          ? 'healthy'
          : healthReport.healthyPercentage >= 50
            ? 'degraded'
            : 'unhealthy';

      return {
        service: 'MCP Servers',
        status,
        responseTime: Date.now() - startTime,
        lastCheck: new Date(),
        details: healthReport,
      };
    } catch (error) {
      return {
        service: 'MCP Servers',
        status: 'unhealthy',
        responseTime: Date.now() - startTime,
        lastCheck: new Date(),
        details: { error: error.message },
      };
    }
  }

  private async checkAIServices(): Promise<ServiceHealthStatus> {
    const startTime = Date.now();

    try {
      const serviceStatus = this.aiManager.getServiceStatus();
      const availableServices = this.aiManager.getAvailableServices();

      const status = availableServices.length > 0 ? 'healthy' : 'unhealthy';

      return {
        service: 'AI Services',
        status,
        responseTime: Date.now() - startTime,
        lastCheck: new Date(),
        details: {
          available: availableServices,
          status: serviceStatus,
        },
      };
    } catch (error) {
      return {
        service: 'AI Services',
        status: 'unhealthy',
        responseTime: Date.now() - startTime,
        lastCheck: new Date(),
        details: { error: error.message },
      };
    }
  }

  async generateDashboard(): Promise<object> {
    const services = await this.checkAllServices();

    const totalServices = services.length;
    const healthyServices = services.filter(
      (s) => s.status === 'healthy'
    ).length;
    const degradedServices = services.filter(
      (s) => s.status === 'degraded'
    ).length;
    const unhealthyServices = services.filter(
      (s) => s.status === 'unhealthy'
    ).length;

    const overallStatus =
      unhealthyServices > 0
        ? 'critical'
        : degradedServices > 0
          ? 'warning'
          : 'healthy';

    return {
      timestamp: new Date().toISOString(),
      overallStatus,
      services: {
        total: totalServices,
        healthy: healthyServices,
        degraded: degradedServices,
        unhealthy: unhealthyServices,
      },
      details: services,
      uptime: process.uptime(),
      version: process.env.npm_package_version || '1.0.0',
    };
  }
}
```

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ API

```typescript
// src/app/api/health/route.ts
import { NextResponse } from 'next/server';
import { IntegratedHealthCheck } from '@/lib/monitoring/integrated-health-check';

const healthChecker = new IntegratedHealthCheck();

export async function GET() {
  try {
    const dashboard = await healthChecker.generateDashboard();
    
    return NextResponse.json(dashboard, {
      status: 200,
      headers: {
        'Cache-Control': 'no-cache, no-store, must-revalidate',
        'Pragma': 'no-cache',
        'Expires': '0',
      },
    });
  } catch (error) {
    return NextResponse.json(
      {
        error: 'Health check failed',
        message: error.message,
        timestamp: new Date().toISOString(),
      },
      { status: 500 }
    );
  }
}
```

## âš¡ ì„±ëŠ¥ ìµœì í™”

### AI ì„œë¹„ìŠ¤ ìµœì í™” ì „ëµ

```typescript
// src/lib/ai/ai-optimizer.ts
export class AIOptimizer {
  private static readonly PERFORMANCE_THRESHOLDS = {
    responseTime: 5000, // 5ì´ˆ
    tokenEfficiency: 0.8, // 80% íš¨ìœ¨ì„±
    errorRate: 0.05, // 5% ì´í•˜
  };

  static async optimizeAICall(
    serviceName: string,
    prompt: string,
    options: any = {}
  ): Promise<any> {
    // 1. í”„ë¡¬í”„íŠ¸ ìµœì í™”
    const optimizedPrompt = this.optimizePrompt(prompt);
    
    // 2. ëª¨ë¸ ì„ íƒ ìµœì í™”
    const optimizedModel = this.selectOptimalModel(serviceName, prompt.length);
    
    // 3. í† í° ì‚¬ìš©ëŸ‰ ìµœì í™”
    const optimizedOptions = this.optimizeTokenUsage(options, prompt.length);
    
    return {
      prompt: optimizedPrompt,
      model: optimizedModel,
      options: optimizedOptions,
    };
  }

  private static optimizePrompt(prompt: string): string {
    // ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    let optimized = prompt.trim().replace(/\s+/g, ' ');
    
    // ë°˜ë³µì ì¸ êµ¬ë¬¸ ì œê±°
    optimized = optimized.replace(/(.{10,})\1+/g, '$1');
    
    // ê¸¸ì´ ì œí•œ ì ìš©
    if (optimized.length > 4000) {
      optimized = optimized.substring(0, 3900) + '...';
    }
    
    return optimized;
  }

  private static selectOptimalModel(serviceName: string, promptLength: number): string {
    switch (serviceName) {
      case 'claude':
        return promptLength > 2000 ? 'claude-3-opus-20240229' : 'claude-3-haiku-20240307';
      case 'google':
        return promptLength > 2000 ? 'gemini-pro' : 'gemini-pro';
      case 'openai':
        return promptLength > 2000 ? 'gpt-4' : 'gpt-3.5-turbo';
      default:
        return 'default';
    }
  }

  private static optimizeTokenUsage(options: any, promptLength: number): any {
    const baseTokens = Math.ceil(promptLength / 4); // ëŒ€ëµì ì¸ í† í° ìˆ˜
    
    return {
      ...options,
      maxTokens: Math.min(options.maxTokens || 1024, baseTokens * 2),
      temperature: options.temperature || 0.1,
    };
  }

  static async measurePerformance(
    serviceName: string,
    executionFn: () => Promise<any>
  ): Promise<{
    result: any;
    metrics: {
      responseTime: number;
      success: boolean;
      tokenEfficiency: number;
    };
  }> {
    const startTime = Date.now();
    let success = false;
    let result: any;
    
    try {
      result = await executionFn();
      success = true;
    } catch (error) {
      result = { error: error.message };
    }
    
    const responseTime = Date.now() - startTime;
    const tokenEfficiency = this.calculateTokenEfficiency(result);
    
    return {
      result,
      metrics: {
        responseTime,
        success,
        tokenEfficiency,
      },
    };
  }

  private static calculateTokenEfficiency(result: any): number {
    if (!result.usage) return 0;
    
    const { inputTokens, outputTokens } = result.usage;
    return outputTokens / (inputTokens + outputTokens);
  }
}
```

## ğŸ“š ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ê°€ì´ë“œ

### ì„œë¹„ìŠ¤ í†µí•© ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ì´ˆê¸° ì„¤ì •

- [ ] Supabase í”„ë¡œì íŠ¸ ìƒì„± ë° ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì ìš©
- [ ] pgvector í™•ì¥ í™œì„±í™” ë° ë„¤ì´í‹°ë¸Œ í•¨ìˆ˜ ì„¤ì¹˜
- [ ] MCP ì„œë²„ 12ê°œ ì„¤ì¹˜ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- [ ] í…ŒìŠ¤íŠ¸ í™˜ê²½ Vitest + Mock ë°ì´í„° êµ¬ì„±
- [ ] AI ì„œë¹„ìŠ¤ API í‚¤ ì„¤ì • ë° ì—°ë™ í…ŒìŠ¤íŠ¸

#### ê°œë°œ ê³¼ì •

- [ ] ìƒˆ ê¸°ëŠ¥ ê°œë°œ ì‹œ ê´€ë ¨ ì„œë¹„ìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸
- [ ] í™˜ê²½ë³€ìˆ˜ ë³€ê²½ ì‹œ ëª¨ë“  í™˜ê²½ ì—…ë°ì´íŠ¸
- [ ] Mock ë°ì´í„° ìµœì‹  ìƒíƒœ ìœ ì§€
- [ ] ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬ í•¸ë“¤ë§ êµ¬í˜„

#### ë°°í¬ ì „

- [ ] ì „ì²´ ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬ í†µê³¼
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì „ì²´ í†µê³¼
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (pgvector 3.6x í–¥ìƒ í™•ì¸)
- [ ] MCP ì„œë²„ ì •ìƒ ì‘ë™ í™•ì¸
- [ ] AI ì„œë¹„ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸

### ëª¨ë‹ˆí„°ë§ ì„¤ì • ê°€ì´ë“œ

#### í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •

```typescript
// next.config.jsì— í—¬ìŠ¤ì²´í¬ ë¦¬ë¼ì´íŠ¸ ì¶”ê°€
module.exports = {
  async rewrites() {
    return [
      {
        source: '/health',
        destination: '/api/health',
      },
    ];
  },
};
```

#### ì•Œë¦¼ ì„¤ì •

```typescript
// src/lib/monitoring/alerts.ts
export class AlertManager {
  static async sendAlert(
    service: string,
    status: string,
    details: any
  ): Promise<void> {
    if (status === 'unhealthy') {
      console.error(`ğŸš¨ ${service} ì„œë¹„ìŠ¤ ì¥ì• :`, details);
      
      // ì‹¤ì œ ì•Œë¦¼ ì„œë¹„ìŠ¤ ì—°ë™ (Slack, Discord ë“±)
      // await this.sendSlackAlert(service, details);
    }
  }
}
```

## ğŸš¨ ë¬¸ì œ í•´ê²°

### AI ì„œë¹„ìŠ¤ ì—°ê²° ì˜¤ë¥˜

**ì¦ìƒ**: `API key invalid` ë˜ëŠ” `Rate limit exceeded`

**í•´ê²°ì±…**:
```typescript
// AI ì„œë¹„ìŠ¤ ì§„ë‹¨
const diagnoseAIServices = async () => {
  const aiManager = new AIServiceManager();
  
  console.log('ğŸ¤– AI ì„œë¹„ìŠ¤ ì§„ë‹¨...');
  
  const services = aiManager.getAvailableServices();
  console.log('ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤:', services);
  
  for (const service of ['claude', 'google', 'openai']) {
    try {
      const testResponse = await aiManager.callAI(service, 'í…ŒìŠ¤íŠ¸', {
        maxTokens: 10,
      });
      console.log(`âœ… ${service}: ì •ìƒ`);
    } catch (error) {
      console.error(`âŒ ${service}: ${error.message}`);
    }
  }
};
```

### ì„±ëŠ¥ ìµœì í™” ë¬¸ì œ

**ì¦ìƒ**: AI ì‘ë‹µ ì‹œê°„ì´ ëŠë¦¼ (>10ì´ˆ)

**í•´ê²°ì±…**:
```typescript
// ì„±ëŠ¥ ì§„ë‹¨ ë° ìµœì í™”
const optimizeAIPerformance = async () => {
  const optimizer = new AIOptimizer();
  
  const testPrompt = "ê¸´ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸...";
  
  // ìµœì í™” ì „í›„ ë¹„êµ
  const beforeOptimization = await AIOptimizer.measurePerformance(
    'claude',
    () => aiManager.callAI('claude', testPrompt)
  );
  
  const optimized = await AIOptimizer.optimizeAICall('claude', testPrompt);
  
  const afterOptimization = await AIOptimizer.measurePerformance(
    'claude',
    () => aiManager.callAI('claude', optimized.prompt, optimized.options)
  );
  
  console.log('ì„±ëŠ¥ ê°œì„ :', {
    before: beforeOptimization.metrics.responseTime,
    after: afterOptimization.metrics.responseTime,
    improvement: beforeOptimization.metrics.responseTime - afterOptimization.metrics.responseTime,
  });
};
```

### ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì˜¤ë¥˜

**ì¦ìƒ**: í—¬ìŠ¤ì²´í¬ê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‘ë‹µí•˜ì§€ ì•ŠìŒ

**í•´ê²°ì±…**:
```typescript
// ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì§„ë‹¨
const diagnoseMonitoring = async () => {
  console.log('ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì§„ë‹¨...');
  
  try {
    const healthChecker = new IntegratedHealthCheck();
    const dashboard = await healthChecker.generateDashboard();
    
    console.log('âœ… ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì •ìƒ');
    console.log('ì„œë¹„ìŠ¤ ìƒíƒœ:', dashboard.services);
  } catch (error) {
    console.error('âŒ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì˜¤ë¥˜:', error.message);
    
    // ê°œë³„ ì„œë¹„ìŠ¤ ì²´í¬
    console.log('ê°œë³„ ì„œë¹„ìŠ¤ ì§„ë‹¨ ì‹œì‘...');
    // ... ê°œë³„ ì§„ë‹¨ ë¡œì§
  }
};
```

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- [ë°ì´í„°ë² ì´ìŠ¤ & ìŠ¤í† ë¦¬ì§€ ì„¤ì •](./database-storage-setup.md)
- [ì¸í”„ë¼ í†µí•© ê°€ì´ë“œ](./infrastructure-integration-setup.md)
- [ê°œë°œ í™˜ê²½ ê°€ì´ë“œ](./development-environment-complete.md)
- [ì¸ì¦ ë³´ì•ˆ ê°€ì´ë“œ](./auth-security-complete-setup.md)
- [MCP ì¢…í•© ê°€ì´ë“œ](../MCP-GUIDE.md)

---

**ğŸ’¡ í•µì‹¬ ì›ì¹™**: ì„œë¹„ìŠ¤ ê°„ ëŠìŠ¨í•œ ê²°í•© + ê°•ë ¥í•œ í†µí•© í…ŒìŠ¤íŠ¸ + ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§

ğŸ¤– **ì„±ê³µ ìš”ì†Œ**: AI ì„œë¹„ìŠ¤ í†µí•© + ìë™í™”ëœ í—¬ìŠ¤ì²´í¬ + ì„±ëŠ¥ ìµœì í™”