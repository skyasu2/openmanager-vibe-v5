"""
ğŸ§  Korean NLP Function (Python Version)

í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ì „ë¬¸ Function - ë¬´ë£Œí‹°ì–´ ìµœì í™”
ë©”ëª¨ë¦¬: 512MB, íƒ€ì„ì•„ì›ƒ: 180ì´ˆ
"""

import json
import time
import re
from typing import Dict, List, Tuple, Any
from datetime import datetime

# Google Cloud Functions Framework
import functions_framework

# ê²½ëŸ‰ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (ë¬´ë£Œí‹°ì–´ ìµœì í™”)
try:
    from kiwipiepy import Kiwi
    kiwi = Kiwi()
    USE_KIWI = True
except ImportError:
    USE_KIWI = False
    print("Warning: kiwipiepy not installed. Using basic morphology analysis.")


# í•œêµ­ì–´ ì–¸ì–´ ìì›
KOREAN_PARTICLES = [
    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ì™€', 'ê³¼', 
    'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë§Œ', 'ë„', 'ë¼ë„', 'ì¡°ì°¨', 'ë§ˆì €'
]

KOREAN_ENDINGS = [
    'ë‹¤', 'ìš”', 'ìŠµë‹ˆë‹¤', 'ì´ë‹¤', 'ì˜€ë‹¤', 'í–ˆë‹¤', 'ë ', 'ë ê¹Œ', 
    'í•˜ì', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ëŸ°ë°'
]

# ì˜ë„ ë¶„ë¥˜ë¥¼ ìœ„í•œ íŒ¨í„´
INTENT_PATTERNS = {
    'question': ['ë¬´ì—‡', 'ë­', 'ë­˜', 'ì–´ë–»ê²Œ', 'ì–´ë–¤', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””', 'ëˆ„ê°€', '?'],
    'command': ['í•´ì¤˜', 'í•´ì£¼ì„¸ìš”', 'í•˜ì', 'í•˜ì„¸ìš”', 'í•´ë¼', 'í•˜ë¼'],
    'request': ['ì•Œë ¤', 'ë³´ì—¬', 'ì„¤ëª…', 'ì°¾ì•„', 'í™•ì¸í•´'],
    'check': ['í™•ì¸', 'ì²´í¬', 'ê²€ì‚¬', 'ì ê²€', 'ì‚´í´'],
    'analysis': ['ë¶„ì„', 'ì¡°ì‚¬', 'íŒŒì•…', 'ì§„ë‹¨', 'í‰ê°€'],
    'server-info': ['ì„œë²„', 'ì‹œìŠ¤í…œ', 'ìƒíƒœ', 'ëª¨ë‹ˆí„°ë§', 'CPU', 'ë©”ëª¨ë¦¬', 'ë””ìŠ¤í¬']
}

# ê°ì • ë¶„ì„ì„ ìœ„í•œ ë‹¨ì–´
SENTIMENT_WORDS = {
    'positive': ['ì¢‹ë‹¤', 'ì¢‹ì•„', 'í›Œë¥­', 'ì™„ë²½', 'ìµœê³ ', 'ê°ì‚¬', 'ê³ ë§ˆì›Œ', 'ì˜ëë‹¤', 'ì„±ê³µ', 'ì•ˆì •ì '],
    'negative': ['ë‚˜ì˜ë‹¤', 'ì•ˆì¢‹ë‹¤', 'ë¬¸ì œ', 'ì˜¤ë¥˜', 'ì—ëŸ¬', 'ì‹¤íŒ¨', 'ì•ˆë¨', 'ëª»í•¨', 'ì–´ë ¤ì›Œ', 'ìœ„í—˜'],
    'urgent': ['ê¸‰í•´', 'ë¹¨ë¦¬', 'ì¦‰ì‹œ', 'ì§€ê¸ˆ', 'ë‹¹ì¥', 'ê¸´ê¸‰', 'ì¤‘ìš”í•´', 'ì‹œê¸‰']
}


def is_korean(text: str) -> bool:
    """í…ìŠ¤íŠ¸ê°€ í•œêµ­ì–´ì¸ì§€ í™•ì¸"""
    korean_pattern = re.compile('[ê°€-í£]+')
    return bool(korean_pattern.search(text))


def classify_intent(query: str) -> str:
    """í•œêµ­ì–´ ì˜ë„ ë¶„ë¥˜"""
    query_lower = query.lower()
    
    # íŒ¨í„´ ë§¤ì¹­ì„ í†µí•œ ì˜ë„ ë¶„ë¥˜
    intent_scores = {}
    for intent, patterns in INTENT_PATTERNS.items():
        score = sum(1 for pattern in patterns if pattern in query_lower)
        if score > 0:
            intent_scores[intent] = score
    
    # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì˜ë„ ë°˜í™˜
    if intent_scores:
        return max(intent_scores, key=intent_scores.get)
    
    return 'general'


def analyze_sentiment(query: str) -> str:
    """í•œêµ­ì–´ ê°ì • ë¶„ì„"""
    query_lower = query.lower()
    
    # ê¸´ê¸‰ì„± ìš°ì„  í™•ì¸
    if any(word in query_lower for word in SENTIMENT_WORDS['urgent']):
        return 'urgent'
    
    # ê¸ì •/ë¶€ì • ê°ì • ì ìˆ˜ ê³„ì‚°
    positive_score = sum(1 for word in SENTIMENT_WORDS['positive'] if word in query_lower)
    negative_score = sum(1 for word in SENTIMENT_WORDS['negative'] if word in query_lower)
    
    if positive_score > negative_score:
        return 'positive'
    elif negative_score > positive_score:
        return 'negative'
    
    return 'neutral'


def extract_entities(query: str) -> List[str]:
    """í•œêµ­ì–´ ì—”í‹°í‹° ì¶”ì¶œ"""
    entities = []
    
    # ì„œë²„ ê´€ë ¨ ì—”í‹°í‹° íŒ¨í„´
    server_patterns = [
        r'([a-zA-Z0-9\-]+)\s*ì„œë²„',
        r'ì„œë²„\s*([a-zA-Z0-9\-]+)',
        r'(web|api|db|cache|app)-\d+',
        r'server-\d+'
    ]
    
    for pattern in server_patterns:
        matches = re.findall(pattern, query, re.IGNORECASE)
        entities.extend(matches)
    
    # ìˆ«ì ì—”í‹°í‹°
    numbers = re.findall(r'\d+', query)
    entities.extend([f"NUMBER:{num}" for num in numbers])
    
    # ì‹œê°„ ì—”í‹°í‹°
    time_pattern = r'(ì˜¤ëŠ˜|ì–´ì œ|ë‚´ì¼|ì´ë²ˆ|ì§€ë‚œ|ë‹¤ìŒ)\s*(ì£¼|ë‹¬|ë…„|ì‹œê°„|ë¶„|ì´ˆ)?'
    times = re.findall(time_pattern, query)
    entities.extend([f"TIME:{''.join(time)}" for time in times])
    
    # ë©”íŠ¸ë¦­ ê´€ë ¨ ì—”í‹°í‹°
    metric_keywords = ['CPU', 'RAM', 'ë©”ëª¨ë¦¬', 'ë””ìŠ¤í¬', 'ë„¤íŠ¸ì›Œí¬', 'íŠ¸ë˜í”½']
    for keyword in metric_keywords:
        if keyword in query.upper():
            entities.append(f"METRIC:{keyword}")
    
    # ì¤‘ë³µ ì œê±°
    return list(set(entities))


def analyze_morphology(query: str) -> Dict[str, List[str]]:
    """í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„"""
    result = {
        'nouns': [],
        'verbs': [],
        'adjectives': [],
        'tokens': []
    }
    
    if USE_KIWI:
        # kiwipiepyë¥¼ ì‚¬ìš©í•œ ì •í™•í•œ í˜•íƒœì†Œ ë¶„ì„
        try:
            tokens = kiwi.tokenize(query)
            for token in tokens:
                result['tokens'].append({
                    'form': token.form,
                    'tag': token.tag,
                    'start': token.start,
                    'end': token.end
                })
                
                # í’ˆì‚¬ë³„ ë¶„ë¥˜
                if token.tag.startswith('N'):  # ëª…ì‚¬ë¥˜
                    result['nouns'].append(token.form)
                elif token.tag.startswith('V'):  # ë™ì‚¬ë¥˜
                    result['verbs'].append(token.form)
                elif token.tag.startswith('VA') or token.tag.startswith('XS'):  # í˜•ìš©ì‚¬ë¥˜
                    result['adjectives'].append(token.form)
        except Exception as e:
            print(f"Kiwi analysis error: {e}")
            # í´ë°±ìœ¼ë¡œ ê¸°ë³¸ ë¶„ì„ ì‚¬ìš©
            return _basic_morphology_analysis(query)
    else:
        # ê¸°ë³¸ í˜•íƒœì†Œ ë¶„ì„ (kiwipiepy ì—†ì„ ë•Œ)
        return _basic_morphology_analysis(query)
    
    return result


def _basic_morphology_analysis(query: str) -> Dict[str, List[str]]:
    """ê¸°ë³¸ í˜•íƒœì†Œ ë¶„ì„ (í´ë°±)"""
    words = query.split()
    result = {
        'nouns': [],
        'verbs': [],
        'adjectives': [],
        'tokens': []
    }
    
    for word in words:
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
        has_particle = any(particle in word for particle in KOREAN_PARTICLES)
        
        if any(ending in word for ending in ['í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤']):
            result['verbs'].append(word)
        elif any(adj in word for adj in ['ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'í¬ë‹¤', 'ì‘ë‹¤', 'ë§ë‹¤', 'ì ë‹¤']):
            result['adjectives'].append(word)
        elif not has_particle:
            result['nouns'].append(word)
        
        result['tokens'].append({'form': word, 'tag': 'UNKNOWN'})
    
    return result


def generate_korean_response(analysis: Dict[str, Any], query: str) -> str:
    """í•œêµ­ì–´ ì‘ë‹µ ìƒì„±"""
    intent = analysis['intent']
    sentiment = analysis['sentiment']
    entities = analysis['entities']
    
    # ì˜ë„ë³„ ì‘ë‹µ í…œí”Œë¦¿
    templates = {
        'question': [
            "ì§ˆë¬¸í•´ì£¼ì‹  '{}'ì— ëŒ€í•´ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.",
            "ê¶ê¸ˆí•´í•˜ì‹œëŠ” ë‚´ìš©ì„ í™•ì¸í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ë¬¸ì˜í•˜ì‹  ì‚¬í•­ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        ],
        'command': [
            "ìš”ì²­í•˜ì‹  ì‘ì—…ì„ ì¦‰ì‹œ ìˆ˜í–‰í•˜ê² ìŠµë‹ˆë‹¤.",
            "ì§€ì‹œì‚¬í•­ì„ í™•ì¸í•˜ì—¬ ì²˜ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.",
            "ëª…ë ¹ì„ ì‹¤í–‰í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
        ],
        'request': [
            "ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ì—¬ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "í•„ìš”í•˜ì‹  ë‚´ìš©ì„ ì¡°íšŒí•˜ì—¬ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ì›í•˜ì‹œëŠ” ì •ë³´ë¥¼ ì°¾ì•„ì„œ ì „ë‹¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        ],
        'check': [
            "ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•˜ì—¬ ê²°ê³¼ë¥¼ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ìš”ì²­í•˜ì‹  í•­ëª©ì„ ì ê²€í•˜ì—¬ ë³´ê³ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ìƒíƒœ ì²´í¬ë¥¼ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤."
        ],
        'analysis': [
            "ìš”ì²­í•˜ì‹  ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.",
            "ë°ì´í„°ë¥¼ ìƒì„¸íˆ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ê² ìŠµë‹ˆë‹¤.",
            "ì‹¬ì¸µ ë¶„ì„ì„ í†µí•´ ë¬¸ì œì ê³¼ ê°œì„ ë°©ì•ˆì„ ì œì‹œí•˜ê² ìŠµë‹ˆë‹¤."
        ],
        'server-info': [
            "ì„œë²„ ëª¨ë‹ˆí„°ë§ ì •ë³´ë¥¼ í™•ì¸í•˜ì—¬ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤.",
            "ì‹œìŠ¤í…œ ìƒíƒœì™€ ì„±ëŠ¥ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ ë³´ê³ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ì„œë²„ ìš´ì˜ í˜„í™©ì„ ì ê²€í•˜ì—¬ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        ],
        'general': [
            "ìš”ì²­ì‚¬í•­ì„ ì²˜ë¦¬í•˜ì—¬ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•´ í™•ì¸í•˜ì—¬ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
            "í•„ìš”í•˜ì‹  ì§€ì›ì„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."
        ]
    }
    
    # ê¸°ë³¸ ì‘ë‹µ ì„ íƒ
    base_response = templates[intent][0]
    
    # ê°ì •ì— ë”°ë¥¸ ì‘ë‹µ ìˆ˜ì •
    if sentiment == 'urgent':
        base_response = f"ğŸš¨ ê¸´ê¸‰ ìš”ì²­ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. {base_response}"
    elif sentiment == 'negative':
        base_response = f"ë¬¸ì œ ìƒí™©ì„ ì¸ì§€í–ˆìŠµë‹ˆë‹¤. {base_response}"
    elif sentiment == 'positive':
        base_response = f"ê¸ì •ì ì¸ í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤. {base_response}"
    
    # ì—”í‹°í‹° ì •ë³´ ì¶”ê°€
    server_entities = [e for e in entities if not e.startswith(('NUMBER:', 'TIME:', 'METRIC:'))]
    metric_entities = [e.replace('METRIC:', '') for e in entities if e.startswith('METRIC:')]
    
    if server_entities:
        base_response += f" íŠ¹íˆ {', '.join(server_entities)}ì— ì¤‘ì ì„ ë‘ì–´ í™•ì¸í•˜ê² ìŠµë‹ˆë‹¤."
    
    if metric_entities:
        base_response += f" {', '.join(metric_entities)} ì§€í‘œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤."
    
    return base_response


def process_korean_nlp(query: str) -> Dict[str, Any]:
    """í•œêµ­ì–´ NLP ì „ì²´ ì²˜ë¦¬"""
    start_time = time.time()
    
    # í•œêµ­ì–´ ê²€ì¦
    if not is_korean(query):
        return {
            'success': False,
            'confidence': 0.1,
            'error': 'Not a Korean query'
        }
    
    # ë¶„ì„ ìˆ˜í–‰
    intent = classify_intent(query)
    sentiment = analyze_sentiment(query)
    entities = extract_entities(query)
    morphology = analyze_morphology(query)
    
    analysis = {
        'intent': intent,
        'sentiment': sentiment,
        'entities': entities,
        'morphology': morphology
    }
    
    # ì‘ë‹µ ìƒì„±
    response = generate_korean_response(analysis, query)
    
    # ì‹ ë¢°ë„ ê³„ì‚°
    confidence = 0.85  # ê¸°ë³¸ ì‹ ë¢°ë„
    if entities:
        confidence += 0.05
    if intent != 'general':
        confidence += 0.05
    if sentiment != 'neutral':
        confidence += 0.03
    if USE_KIWI:
        confidence += 0.02  # kiwipiepy ì‚¬ìš© ì‹œ ì¶”ê°€ ì‹ ë¢°ë„
    
    confidence = min(confidence, 0.98)
    
    processing_time = (time.time() - start_time) * 1000  # ms
    
    return {
        'success': True,
        'response': response,
        'confidence': confidence,
        'analysis': analysis,
        'processingTime': processing_time
    }


def create_response(data: Dict[str, Any], status_code: int = 200) -> Tuple[str, int, Dict[str, str]]:
    """HTTP ì‘ë‹µ ìƒì„±"""
    headers = {
        'Content-Type': 'application/json',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'POST, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type, Authorization'
    }
    return json.dumps(data), status_code, headers


@functions_framework.http
def korean_nlp(request):
    """ë©”ì¸ HTTP í•¸ë“¤ëŸ¬"""
    # CORS preflight
    if request.method == 'OPTIONS':
        return '', 204, {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization'
        }
    
    if request.method != 'POST':
        return create_response({
            'success': False,
            'error': 'Method not allowed',
            'engine': 'korean-nlp-python'
        }, 405)
    
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        request_data = request.get_json()
        if not request_data or 'query' not in request_data:
            return create_response({
                'success': False,
                'error': 'Invalid request format',
                'engine': 'korean-nlp-python'
            }, 400)
        
        query = request_data['query']
        mode = request_data.get('mode', 'normal')
        context = request_data.get('context', {})
        
        print(f"Korean NLP: Processing '{query}' (mode: {mode})")
        
        # NLP ì²˜ë¦¬
        result = process_korean_nlp(query)
        
        if not result['success']:
            return create_response({
                'success': False,
                'error': result.get('error', 'Processing failed'),
                'engine': 'korean-nlp-python'
            }, 400)
        
        # ì„±ê³µ ì‘ë‹µ
        response_data = {
            'success': True,
            'response': result['response'],
            'confidence': result['confidence'],
            'engine': 'korean-nlp-python',
            'processingTime': result['processingTime'],
            'metadata': {
                'analysis': result['analysis'],
                'koreanSpecific': True,
                'morphologyEngine': 'kiwipiepy' if USE_KIWI else 'basic',
                'mode': mode
            }
        }
        
        print(f"Korean NLP: Completed in {result['processingTime']:.2f}ms (confidence: {result['confidence']:.2f})")
        
        return create_response(response_data, 200)
        
    except Exception as e:
        print(f"Korean NLP error: {str(e)}")
        return create_response({
            'success': False,
            'error': str(e),
            'engine': 'korean-nlp-python'
        }, 500)


@functions_framework.http
def korean_nlp_health(request):
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    if request.method == 'OPTIONS':
        return '', 204, {
            'Access-Control-Allow-Origin': '*'
        }
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    test_result = process_korean_nlp('ì„œë²„ ìƒíƒœê°€ ì–´ë–»ìŠµë‹ˆê¹Œ?')
    
    health_data = {
        'status': 'healthy' if test_result['success'] else 'unhealthy',
        'timestamp': datetime.utcnow().isoformat(),
        'function': 'korean-nlp-python',
        'memory': '512MB',
        'timeout': '180s',
        'version': '2.0.0',
        'features': {
            'kiwipiepy': USE_KIWI,
            'morphology': 'advanced' if USE_KIWI else 'basic'
        },
        'test': {
            'query': 'ì„œë²„ ìƒíƒœê°€ ì–´ë–»ìŠµë‹ˆê¹Œ?',
            'confidence': test_result.get('confidence', 0),
            'processingTime': test_result.get('processingTime', 0)
        }
    }
    
    return create_response(health_data, 200 if test_result['success'] else 500)