"""
ğŸ¤– Basic ML Function (Python Version)

ê¸°ë³¸ ë¨¸ì‹ ëŸ¬ë‹ ì²˜ë¦¬ ì „ë¬¸ Function - ë¬´ë£Œí‹°ì–´ ìµœì í™”
ë©”ëª¨ë¦¬: 512MB, íƒ€ì„ì•„ì›ƒ: 120ì´ˆ
"""

import json
import time
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
import pickle
import base64

# Google Cloud Functions Framework
import functions_framework

# ê²½ëŸ‰ ML ë¼ì´ë¸ŒëŸ¬ë¦¬ (ë¬´ë£Œí‹°ì–´ ìµœì í™”)
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("Warning: scikit-learn not installed. Using basic ML functions.")


# í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¹´í…Œê³ ë¦¬ì™€ í‚¤ì›Œë“œ
CATEGORIES = {
    'technical': {
        'keywords': ['ì„œë²„', 'ì‹œìŠ¤í…œ', 'ë„¤íŠ¸ì›Œí¬', 'ë°ì´í„°ë² ì´ìŠ¤', 'CPU', 'ë©”ëª¨ë¦¬', 
                    'ë””ìŠ¤í¬', 'ë¡œê·¸', 'ëª¨ë‹ˆí„°ë§', 'ì„±ëŠ¥', 'ì—ëŸ¬', 'ì˜¤ë¥˜', 'API', 
                    'ë ˆì´í„´ì‹œ', 'ì²˜ë¦¬ëŸ‰', 'ìš©ëŸ‰'],
        'weight': 1.0
    },
    'operational': {
        'keywords': ['ìš´ì˜', 'ê´€ë¦¬', 'ì„¤ì •', 'ë°°í¬', 'ë°±ì—…', 'ë³µì›', 'ìœ ì§€ë³´ìˆ˜', 
                    'ì—…ë°ì´íŠ¸', 'ë³´ì•ˆ', 'ê¶Œí•œ', 'ì ‘ê·¼', 'ì œì–´', 'ì •ì±…', 'í”„ë¡œì„¸ìŠ¤'],
        'weight': 0.9
    },
    'analysis': {
        'keywords': ['ë¶„ì„', 'í†µê³„', 'ì˜ˆì¸¡', 'ì¶”ì´', 'íŠ¸ë Œë“œ', 'íŒ¨í„´', 'ì§€í‘œ', 
                    'ë©”íŠ¸ë¦­', 'ë¦¬í¬íŠ¸', 'ì¸ì‚¬ì´íŠ¸', 'ì‹œê°í™”', 'ëŒ€ì‹œë³´ë“œ'],
        'weight': 0.8
    },
    'support': {
        'keywords': ['ë„ì›€', 'ë¬¸ì œ', 'í•´ê²°', 'ì§€ì›', 'ê°€ì´ë“œ', 'ì„¤ëª…', 'ë°©ë²•', 
                    'ì ˆì°¨', 'ë¬¸ì˜', 'ìš”ì²­', 'í™•ì¸', 'ì ê²€'],
        'weight': 0.7
    },
    'general': {
        'keywords': ['ì•ˆë…•', 'ê°ì‚¬', 'í™•ì¸', 'ìƒíƒœ', 'ì •ë³´', 'ì•Œë¦¼', 'ì§ˆë¬¸', 
                    'ì¼ë°˜', 'ê¸°íƒ€', 'ì „ì²´'],
        'weight': 0.5
    }
}

# í•œêµ­ì–´ ë¶ˆìš©ì–´ (ê°„ë‹¨í•œ ë²„ì „)
KOREAN_STOPWORDS = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 
                   'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë§Œ', 'ë„', 'ê¹Œì§€', 'ë¶€í„°']

# ê¸€ë¡œë²Œ ëª¨ë¸ ìºì‹œ (ì½œë“œ ìŠ¤íƒ€íŠ¸ ìµœì í™”)
_model_cache = {}


def classify_text(text: str) -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ ë¶„ë¥˜ - scikit-learn ë˜ëŠ” ê·œì¹™ ê¸°ë°˜"""
    if SKLEARN_AVAILABLE and 'text_classifier' in _model_cache:
        # í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
        vectorizer = _model_cache['vectorizer']
        classifier = _model_cache['text_classifier']
        
        features = vectorizer.transform([text])
        probabilities = classifier.predict_proba(features)[0]
        
        # í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ì¹´í…Œê³ ë¦¬
        best_idx = np.argmax(probabilities)
        categories = list(CATEGORIES.keys())
        
        return {
            'classification': categories[best_idx],
            'confidence': float(probabilities[best_idx]),
            'probabilities': {cat: float(prob) for cat, prob in zip(categories, probabilities)}
        }
    else:
        # ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ (í´ë°±)
        return _rule_based_classification(text)


def _rule_based_classification(text: str) -> Dict[str, Any]:
    """ê·œì¹™ ê¸°ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
    text_lower = text.lower()
    scores = {}
    
    for category, data in CATEGORIES.items():
        score = 0
        matches = 0
        
        for keyword in data['keywords']:
            if keyword in text_lower:
                score += data['weight']
                matches += 1
        
        scores[category] = {
            'score': score,
            'matches': matches,
            'normalized': score / len(data['keywords']) if data['keywords'] else 0
        }
    
    # ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜
    total_score = sum(s['score'] for s in scores.values())
    if total_score > 0:
        probabilities = {cat: s['score'] / total_score for cat, s in scores.items()}
    else:
        probabilities = {cat: 1.0 / len(CATEGORIES) for cat in CATEGORIES}
    
    best_category = max(probabilities, key=probabilities.get)
    
    return {
        'classification': best_category,
        'confidence': probabilities[best_category],
        'probabilities': probabilities
    }


def generate_embedding(text: str, max_features: int = 100) -> np.ndarray:
    """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± - TF-IDF ê¸°ë°˜"""
    if SKLEARN_AVAILABLE:
        # TF-IDF ë²¡í„°í™”
        if 'tfidf_vectorizer' not in _model_cache:
            _model_cache['tfidf_vectorizer'] = TfidfVectorizer(
                max_features=max_features,
                ngram_range=(1, 2),  # ìœ ë‹ˆê·¸ë¨ê³¼ ë°”ì´ê·¸ë¨
                stop_words=None  # í•œêµ­ì–´ ë¶ˆìš©ì–´ëŠ” ìˆ˜ë™ ì²˜ë¦¬
            )
            # ë”ë¯¸ ë°ì´í„°ë¡œ í•™ìŠµ (ì‹¤ì œë¡œëŠ” ì½”í¼ìŠ¤ê°€ í•„ìš”)
            sample_texts = []
            for cat_data in CATEGORIES.values():
                sample_texts.extend(cat_data['keywords'])
            _model_cache['tfidf_vectorizer'].fit(sample_texts)
        
        vectorizer = _model_cache['tfidf_vectorizer']
        
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì œê±°
        words = text.split()
        filtered_text = ' '.join([w for w in words if w not in KOREAN_STOPWORDS])
        
        # ë²¡í„°í™”
        embedding = vectorizer.transform([filtered_text]).toarray()[0]
        
        # L2 ì •ê·œí™”
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    else:
        # ê¸°ë³¸ ì„ë² ë”© (ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜)
        return _basic_embedding(text, max_features)


def _basic_embedding(text: str, max_features: int) -> np.ndarray:
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì„ë² ë”© (í´ë°±)"""
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œë¥¼ vocabularyë¡œ ì‚¬ìš©
    vocab = []
    for cat_data in CATEGORIES.values():
        vocab.extend(cat_data['keywords'])
    vocab = list(set(vocab))[:max_features]
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    text_lower = text.lower()
    embedding = np.zeros(len(vocab))
    
    for i, word in enumerate(vocab):
        embedding[i] = text_lower.count(word)
    
    # L2 ì •ê·œí™”
    norm = np.linalg.norm(embedding)
    if norm > 0:
        embedding = embedding / norm
    
    return embedding


def predict_trend(data: List[float]) -> Dict[str, Any]:
    """ì‹œê³„ì—´ ì˜ˆì¸¡ - ì„ í˜• íšŒê·€ + ì´ìƒì¹˜ íƒì§€"""
    if len(data) < 2:
        return {
            'trend': 'insufficient_data',
            'confidence': 0.1,
            'prediction': data[0] if data else 0,
            'anomalies': []
        }
    
    X = np.arange(len(data)).reshape(-1, 1)
    y = np.array(data)
    
    # ì´ìƒì¹˜ íƒì§€ (IQR ë°©ë²•)
    q1, q3 = np.percentile(y, [25, 75])
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    anomalies = [(i, val) for i, val in enumerate(data) 
                 if val < lower_bound or val > upper_bound]
    
    # ì´ìƒì¹˜ ì œê±°í•œ ë°ì´í„°ë¡œ í•™ìŠµ
    mask = (y >= lower_bound) & (y <= upper_bound)
    X_clean = X[mask]
    y_clean = y[mask]
    
    if len(X_clean) < 2:
        # ì´ìƒì¹˜ ì œê±° í›„ ë°ì´í„° ë¶€ì¡±
        return {
            'trend': 'anomaly_dominated',
            'confidence': 0.2,
            'prediction': np.mean(data),
            'anomalies': anomalies
        }
    
    if SKLEARN_AVAILABLE:
        # scikit-learn ì„ í˜• íšŒê·€
        model = LinearRegression()
        model.fit(X_clean, y_clean)
        
        # ë‹¤ìŒ ê°’ ì˜ˆì¸¡
        next_X = np.array([[len(data)]])
        prediction = float(model.predict(next_X)[0])
        
        # RÂ² ì ìˆ˜ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
        r2_score = model.score(X_clean, y_clean)
        confidence = max(0.1, min(0.95, r2_score))
        
        # íŠ¸ë Œë“œ íŒë‹¨
        slope = model.coef_[0]
        if slope > 0.1:
            trend = 'increasing'
        elif slope < -0.1:
            trend = 'decreasing'
        else:
            trend = 'stable'
        
        # ê³„ì ˆì„± íƒì§€ (ê°„ë‹¨í•œ FFT)
        if len(data) >= 8:
            fft = np.fft.fft(y - np.mean(y))
            frequencies = np.fft.fftfreq(len(y))
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì°¾ê¸°
            idx = np.argsort(np.abs(fft))[::-1][1:4]  # ìƒìœ„ 3ê°œ (DC ì œì™¸)
            periods = [int(1/abs(frequencies[i])) for i in idx if frequencies[i] != 0]
            seasonality = periods[0] if periods and 2 <= periods[0] <= len(data)//2 else None
        else:
            seasonality = None
        
        return {
            'trend': trend,
            'confidence': confidence,
            'prediction': prediction,
            'slope': float(slope),
            'anomalies': anomalies,
            'seasonality': seasonality,
            'period': seasonality
        }
    else:
        # ê¸°ë³¸ ì„ í˜• íšŒê·€ (í´ë°±)
        return _basic_linear_regression(X_clean, y_clean, len(data), anomalies)


def _basic_linear_regression(X: np.ndarray, y: np.ndarray, 
                            next_idx: int, anomalies: List) -> Dict[str, Any]:
    """ê¸°ë³¸ ì„ í˜• íšŒê·€ êµ¬í˜„"""
    # ìµœì†ŒììŠ¹ë²•
    n = len(X)
    sum_x = np.sum(X)
    sum_y = np.sum(y)
    sum_xy = np.sum(X.flatten() * y)
    sum_xx = np.sum(X.flatten() ** 2)
    
    slope = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x ** 2)
    intercept = (sum_y - slope * sum_x) / n
    
    # ì˜ˆì¸¡
    prediction = slope * next_idx + intercept
    
    # íŠ¸ë Œë“œ íŒë‹¨
    if slope > 0.1:
        trend = 'increasing'
    elif slope < -0.1:
        trend = 'decreasing'
    else:
        trend = 'stable'
    
    # RÂ² ê³„ì‚°
    y_pred = X.flatten() * slope + intercept
    ss_res = np.sum((y - y_pred) ** 2)
    ss_tot = np.sum((y - np.mean(y)) ** 2)
    r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
    
    return {
        'trend': trend,
        'confidence': max(0.1, min(0.95, r2)),
        'prediction': float(prediction),
        'slope': float(slope),
        'anomalies': anomalies,
        'seasonality': None,
        'period': None
    }


def analyze_statistics(data: List[float]) -> Dict[str, Any]:
    """í†µê³„ ë¶„ì„"""
    if not data:
        return {
            'count': 0,
            'mean': 0,
            'median': 0,
            'std': 0,
            'min': 0,
            'max': 0,
            'p25': 0,
            'p50': 0,
            'p75': 0,
            'outliers': []
        }
    
    arr = np.array(data)
    
    # ê¸°ë³¸ í†µê³„
    stats = {
        'count': len(data),
        'mean': float(np.mean(arr)),
        'median': float(np.median(arr)),
        'std': float(np.std(arr)),
        'min': float(np.min(arr)),
        'max': float(np.max(arr))
    }
    
    # ë°±ë¶„ìœ„ìˆ˜
    percentiles = np.percentile(arr, [25, 50, 75])
    stats['p25'] = float(percentiles[0])
    stats['p50'] = float(percentiles[1])
    stats['p75'] = float(percentiles[2])
    
    # IQR ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
    q1, q3 = stats['p25'], stats['p75']
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    
    outliers = [float(x) for x in arr if x < lower_bound or x > upper_bound]
    stats['outliers'] = outliers
    
    # ì¶”ê°€ í†µê³„ (ë¶„í¬ íŠ¹ì„±)
    if len(data) > 3:
        # ì™œë„ (Skewness)
        mean_centered = arr - stats['mean']
        stats['skewness'] = float(np.mean(mean_centered ** 3) / (stats['std'] ** 3))
        
        # ì²¨ë„ (Kurtosis)
        stats['kurtosis'] = float(np.mean(mean_centered ** 4) / (stats['std'] ** 4) - 3)
    
    return stats


def process_basic_ml(query: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """í†µí•© ML ì²˜ë¦¬"""
    start_time = time.time()
    
    if context is None:
        context = {}
    
    # 1. í…ìŠ¤íŠ¸ ë¶„ë¥˜
    classification = classify_text(query)
    
    # 2. í…ìŠ¤íŠ¸ ì„ë² ë”©
    embeddings = generate_embedding(query)
    
    # 3. ë©”íŠ¸ë¦­ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
    predictions = None
    statistics = None
    
    if 'metrics' in context and isinstance(context['metrics'], list) and context['metrics']:
        predictions = predict_trend(context['metrics'])
        statistics = analyze_statistics(context['metrics'])
    
    # 4. ì‘ë‹µ ìƒì„±
    response = _generate_response(classification, predictions, statistics, query)
    
    processing_time = (time.time() - start_time) * 1000  # ms
    
    return {
        'success': True,
        'response': response,
        'confidence': classification['confidence'],
        'classification': classification['classification'],
        'embeddings': embeddings.tolist(),
        'predictions': predictions,
        'statistics': statistics,
        'processingTime': processing_time
    }


def _generate_response(classification: Dict[str, Any], 
                      predictions: Optional[Dict[str, Any]],
                      statistics: Optional[Dict[str, Any]], 
                      query: str) -> str:
    """ì‘ë‹µ ìƒì„±"""
    category_responses = {
        'technical': 'ê¸°ìˆ ì  ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ìƒíƒœì™€ ì„±ëŠ¥ ì§€í‘œë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.',
        'operational': 'ìš´ì˜ ê´€ë ¨ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ê´€ë¦¬ ìµœì í™” ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.',
        'analysis': 'ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. í†µê³„ì  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí–ˆìŠµë‹ˆë‹¤.',
        'support': 'ì§€ì› ìš”ì²­ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ë¬¸ì œ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.',
        'general': 'ìš”ì²­ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.'
    }
    
    response = category_responses.get(
        classification['classification'], 
        category_responses['general']
    )
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
    if predictions:
        trend_desc = {
            'increasing': 'ì¦ê°€',
            'decreasing': 'ê°ì†Œ',
            'stable': 'ì•ˆì •',
            'anomaly_dominated': 'ì´ìƒì¹˜ ê³¼ë‹¤',
            'insufficient_data': 'ë°ì´í„° ë¶€ì¡±'
        }
        
        trend = trend_desc.get(predictions['trend'], predictions['trend'])
        response += f" íŠ¸ë Œë“œëŠ” '{trend}' ì¶”ì„¸ì´ë©°, ë‹¤ìŒ ì˜ˆì¸¡ê°’ì€ {predictions['prediction']:.2f}ì…ë‹ˆë‹¤."
        
        if predictions.get('anomalies'):
            response += f" {len(predictions['anomalies'])}ê°œì˜ ì´ìƒì¹˜ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤."
        
        if predictions.get('seasonality'):
            response += f" ì£¼ê¸°ì„± íŒ¨í„´(ì£¼ê¸°: {predictions['seasonality']})ì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤."
    
    # í†µê³„ ì •ë³´ ì¶”ê°€
    if statistics:
        response += f" í‰ê· ê°’ì€ {statistics['mean']:.2f}, í‘œì¤€í¸ì°¨ëŠ” {statistics['std']:.2f}ì…ë‹ˆë‹¤."
        
        if statistics['outliers']:
            response += f" {len(statistics['outliers'])}ê°œì˜ í†µê³„ì  ì´ìƒì¹˜ê°€ ìˆìŠµë‹ˆë‹¤."
    
    # ì‹ ë¢°ë„ ì¶”ê°€
    confidence_level = "ë†’ìŒ" if classification['confidence'] > 0.8 else "ë³´í†µ"
    response += f" (ë¶„ì„ ì‹ ë¢°ë„: {confidence_level})"
    
    return response


def create_response(data: Dict[str, Any], status_code: int = 200) -> Tuple[str, int, Dict[str, str]]:
    """HTTP ì‘ë‹µ ìƒì„±"""
    headers = {
        'Content-Type': 'application/json',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Methods': 'POST, OPTIONS',
        'Access-Control-Allow-Headers': 'Content-Type, Authorization'
    }
    return json.dumps(data), status_code, headers


@functions_framework.http
def basic_ml(request):
    """ë©”ì¸ HTTP í•¸ë“¤ëŸ¬"""
    # CORS preflight
    if request.method == 'OPTIONS':
        return '', 204, {
            'Access-Control-Allow-Origin': '*',
            'Access-Control-Allow-Methods': 'POST, OPTIONS',
            'Access-Control-Allow-Headers': 'Content-Type, Authorization'
        }
    
    if request.method != 'POST':
        return create_response({
            'success': False,
            'error': 'Method not allowed',
            'engine': 'basic-ml-python'
        }, 405)
    
    try:
        # ìš”ì²­ ë°ì´í„° íŒŒì‹±
        request_data = request.get_json()
        if not request_data or 'query' not in request_data:
            return create_response({
                'success': False,
                'error': 'Invalid request format',
                'engine': 'basic-ml-python'
            }, 400)
        
        query = request_data['query']
        context = request_data.get('context', {})
        mode = request_data.get('mode', 'normal')
        
        print(f"Basic ML: Processing '{query}' (mode: {mode})")
        
        # ML ì²˜ë¦¬
        result = process_basic_ml(query, context)
        
        # ì„±ê³µ ì‘ë‹µ
        response_data = {
            'success': True,
            'response': result['response'],
            'confidence': result['confidence'],
            'engine': 'basic-ml-python',
            'processingTime': result['processingTime'],
            'metadata': {
                'classification': result['classification'],
                'embeddingDimension': len(result['embeddings']),
                'predictions': result['predictions'],
                'statistics': result['statistics'],
                'mlLibrary': 'scikit-learn' if SKLEARN_AVAILABLE else 'numpy',
                'mode': mode
            }
        }
        
        print(f"Basic ML: Completed in {result['processingTime']:.2f}ms")
        
        return create_response(response_data, 200)
        
    except Exception as e:
        print(f"Basic ML error: {str(e)}")
        return create_response({
            'success': False,
            'error': str(e),
            'engine': 'basic-ml-python'
        }, 500)


@functions_framework.http
def basic_ml_health(request):
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    if request.method == 'OPTIONS':
        return '', 204, {
            'Access-Control-Allow-Origin': '*'
        }
    
    # í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    test_result = process_basic_ml(
        'ì„œë²„ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”',
        {'metrics': [70, 75, 80, 85, 90, 85, 95]}
    )
    
    health_data = {
        'status': 'healthy' if test_result['success'] else 'unhealthy',
        'timestamp': datetime.utcnow().isoformat(),
        'function': 'basic-ml-python',
        'memory': '512MB',
        'timeout': '120s',
        'version': '2.0.0',
        'features': {
            'scikit-learn': SKLEARN_AVAILABLE,
            'classification': True,
            'prediction': True,
            'statistics': True,
            'embeddings': True
        },
        'test': {
            'query': 'ì„œë²„ ì„±ëŠ¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”',
            'classification': test_result['classification'],
            'confidence': test_result['confidence'],
            'processingTime': test_result['processingTime']
        }
    }
    
    return create_response(health_data, 200 if test_result['success'] else 500)