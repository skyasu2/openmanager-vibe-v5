# AI 자연어 질의 시스템 개선 계획서

**작성일**: 2026-01-08
**버전**: v5.84.1 → v5.85.0
**작성자**: Claude Code

---

## 📋 개선 목표

| 항목 | 현재 상태 | 목표 | 기대 효과 |
|------|----------|------|----------|
| 서버 사이드 캐싱 | supervisor 미적용 | 캐시 적용 | 동일 질의 90% 응답시간 감소 |
| 토큰 스트리밍 | JSON→텍스트 변환 | 실시간 스트리밍 | UX 체감 속도 향상 |
| 컨텍스트 압축 | 전체 메시지 전달 | 요약 기반 압축 | 토큰 비용 30-50% 절감 |

---

## 1️⃣ 서버 사이드 응답 캐싱

### 현재 상태
- `ai-response-cache.ts` 구현 완료 (Memory + Redis 다층 캐시)
- `incident-report`, `intelligent-monitoring` 라우트에서 사용 중
- **`/api/ai/supervisor` 라우트에서 미사용** ← 개선 대상

### 구현 계획

```
┌─────────────────────────────────────────────────────────────────┐
│  사용자 질의: "CPU 80% 이상 서버"                                │
└────────────────────────┬────────────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│  1. 캐시 키 생성                                                 │
│     - 쿼리 정규화 (소문자, 공백 정리)                            │
│     - 해시 생성: hash(normalizedQuery)                          │
│     - 키: "supervisor:{hash}"                                   │
└────────────────────────┬────────────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│  2. 캐시 조회 (Memory → Redis 순서)                             │
│     - Memory 히트: ~1ms 응답                                    │
│     - Redis 히트: ~10ms 응답                                    │
│     - 캐시 미스: Cloud Run 호출                                  │
└────────────────────────┬────────────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│  3. 캐시 저장 (응답 후)                                          │
│     - TTL: 5분 (상태 질의) / 15분 (일반 질의)                    │
│     - Memory + Redis 동시 저장                                   │
└─────────────────────────────────────────────────────────────────┘
```

### 캐시 제외 조건
- 세션 컨텍스트가 필요한 대화 (이전 메시지 참조)
- 실시간 데이터 요청 ("지금", "현재", "방금")
- 명시적 새로고침 요청

### 파일 변경
- `src/app/api/ai/supervisor/route.ts`: 캐시 로직 추가
- `src/lib/ai/cache/ai-response-cache.ts`: supervisor TTL 추가

---

## 2️⃣ 실시간 토큰 스트리밍

### 현재 상태
- Cloud Run → JSON 응답 → 전체 텍스트 추출 → 반환
- 사용자는 전체 응답 완료 후에만 텍스트를 봄

### 개선 방안

**Option A: Cloud Run 스트리밍 지원** (권장)
```
Cloud Run AI Engine
├─ generateText() → streamText() 변경
├─ text-stream 프로토콜로 응답
└─ Vercel Edge가 스트림 중계

효과: 첫 토큰 ~500ms, 점진적 렌더링
복잡도: 높음 (Cloud Run 수정 필요)
```

**Option B: 프론트엔드 시뮬레이션** (대안)
```
Frontend
├─ 전체 응답 수신
├─ 토큰 단위로 분할
└─ setInterval로 점진적 표시 (20ms 간격)

효과: 시각적 UX 개선
복잡도: 낮음
```

### 구현 계획 (Option A)
1. Cloud Run `streamText()` 활성화
2. Vercel Edge 스트림 중계 로직
3. 프론트엔드 `TextStreamChatTransport` 유지

---

## 3️⃣ 컨텍스트 압축

### 현재 상태
- 전체 메시지 배열을 Cloud Run에 전달
- 10개 메시지 × 평균 200토큰 = 2,000 토큰/요청

### 개선 방안

```
┌─────────────────────────────────────────────────────────────────┐
│  대화 이력 (10개 메시지)                                         │
│  ├─ User: "서버 상태 알려줘"                                     │
│  ├─ AI: "현재 15개 서버 중 12개 정상..."                         │
│  ├─ User: "CPU 높은 서버는?"                                     │
│  ├─ AI: "server-03, server-07이 80% 이상..."                    │
│  └─ ... (6개 더)                                                │
└────────────────────────┬────────────────────────────────────────┘
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│  압축 결과                                                       │
│  ├─ 요약: "서버 상태 및 CPU 관련 대화, server-03/07 주목"        │
│  ├─ 최근 3개 메시지 (원본 유지)                                  │
│  └─ 현재 질의                                                   │
└─────────────────────────────────────────────────────────────────┘

토큰 절감: 2,000 → 800 (60% 감소)
```

### 압축 전략
1. **최근 N개 유지**: 최근 3개 메시지는 원본 유지
2. **이전 메시지 요약**: LLM으로 핵심 컨텍스트 요약
3. **메타데이터 추출**: 언급된 서버 ID, 메트릭 종류

### 파일 변경
- `src/lib/ai/utils/context-compressor.ts`: 새 파일
- `src/app/api/ai/supervisor/route.ts`: 압축 로직 통합

---

## 📅 구현 일정

| 단계 | 작업 | 예상 코드량 |
|------|------|------------|
| Phase 1 | 서버 사이드 캐싱 적용 | ~50줄 |
| Phase 2 | 컨텍스트 압축 구현 | ~150줄 |
| Phase 3 | 스트리밍 개선 (Option B) | ~30줄 |
| Phase 4 | 테스트 및 검증 | - |

---

## ✅ 성공 기준

| 지표 | 현재 | 목표 |
|------|------|------|
| 캐시 히트율 | 0% | >30% |
| 동일 질의 응답시간 | 5-10초 | <100ms |
| 토큰 사용량/요청 | ~2,000 | ~1,000 |
| 첫 토큰 표시 시간 | 5-10초 | <1초 |

---

## 🚀 즉시 구현 가능 항목

1. **서버 사이드 캐싱** - 기존 인프라 활용, 즉시 적용 가능
2. **프론트엔드 스트리밍 시뮬레이션** - 간단한 UX 개선
3. **기본 컨텍스트 압축** - 최근 N개만 전달하는 단순 버전

---

**승인 후 Phase 1부터 구현을 시작합니다.**
