/**
 * ğŸ¤– API v1 - AI í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±
 *
 * AI ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë¼ë²¨ë§ëœ ë°ì´í„°ì…‹ ì œê³µ
 * - ë¶„ë¥˜ í•™ìŠµìš© ë°ì´í„°
 * - ì´ìƒ íƒì§€ í•™ìŠµìš© ë°ì´í„°
 * - ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµìš© ì‹œê³„ì—´ ë°ì´í„°
 */

import { NextRequest, NextResponse } from 'next/server';
import {
  RealisticDataGenerator,
  DemoScenario,
  ServerMetrics,
  DemoLogEntry,
} from '@/services/data-generator/RealisticDataGenerator';

// ğŸ¤– AI í•™ìŠµìš© ë°ì´í„° ìƒì„±ê¸°
const aiTrainingGenerator = new RealisticDataGenerator();

// ğŸ¯ AI í•™ìŠµ íƒ€ì… ì •ì˜
type TrainingType =
  | 'classification'
  | 'anomaly_detection'
  | 'prediction'
  | 'clustering'
  | 'mixed';

interface TrainingDataset {
  features: number[][];
  labels: (string | number)[];
  metadata: {
    featureNames: string[];
    labelMapping?: Record<string, number>;
    datasetInfo: {
      totalSamples: number;
      featureCount: number;
      classCount?: number;
      anomalyRate?: number;
    };
  };
  scenarios: Array<{
    scenario: DemoScenario;
    samples: number;
    timeRange: { start: string; end: string };
  }>;
}

/**
 * ğŸ¯ AI í•™ìŠµìš© ë°ì´í„°ì…‹ ìƒì„±
 */
export async function POST(request: NextRequest) {
  const startTime = Date.now();

  try {
    const body = await request.json();
    const {
      trainingType = 'classification',
      samplesPerScenario = 100,
      includeAnomalies = true,
      normalizationMode = 'standard',
      timeSeriesLength = 50,
      testSplit = 0.2,
    } = body;

    console.log(
      `ğŸ¤– AI í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±: ${trainingType}, ì‹œë‚˜ë¦¬ì˜¤ë‹¹ ${samplesPerScenario}ê°œ ìƒ˜í”Œ`
    );

    // ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„° ìƒì„±
    const scenarios: DemoScenario[] = [
      'normal',
      'spike',
      'memory_leak',
      'ddos',
      'performance_degradation',
    ];
    const trainingData: TrainingDataset = {
      features: [],
      labels: [],
      metadata: {
        featureNames: [],
        datasetInfo: {
          totalSamples: 0,
          featureCount: 0,
        },
      },
      scenarios: [],
    };

    for (const scenario of scenarios) {
      aiTrainingGenerator.setScenario(scenario);

      const scenarioMetrics =
        aiTrainingGenerator.generateTimeSeriesData(samplesPerScenario);
      const scenarioLogs = aiTrainingGenerator.generateLogEntries(
        Math.floor(samplesPerScenario / 3)
      );

      // í•™ìŠµ íƒ€ì…ì— ë”°ë¥¸ íŠ¹ì„± ì¶”ì¶œ ë° ë¼ë²¨ë§
      const processedData = processDataForTraining(
        scenarioMetrics,
        scenarioLogs,
        scenario,
        trainingType
      );

      trainingData.features.push(...processedData.features);
      trainingData.labels.push(...processedData.labels);

      trainingData.scenarios.push({
        scenario,
        samples: processedData.features.length,
        timeRange: {
          start: scenarioMetrics[0]?.timestamp || '',
          end: scenarioMetrics[scenarioMetrics.length - 1]?.timestamp || '',
        },
      });
    }

    // ë©”íƒ€ë°ì´í„° ì„¤ì •
    trainingData.metadata.featureNames = getFeatureNames(trainingType);
    trainingData.metadata.datasetInfo.totalSamples =
      trainingData.features.length;
    trainingData.metadata.datasetInfo.featureCount =
      trainingData.features[0]?.length || 0;

    if (trainingType === 'classification') {
      const uniqueLabels = [...new Set<string | number>(trainingData.labels)];
      trainingData.metadata.datasetInfo.classCount = uniqueLabels.length;
      trainingData.metadata.labelMapping = Object.fromEntries(
        uniqueLabels.map((label, index) => [label.toString(), index])
      );
    }

    // ë°ì´í„° ì •ê·œí™”
    const normalizedData = normalizeData(
      trainingData.features,
      normalizationMode
    );

    // Train/Test ë¶„í• 
    const splitData = splitTrainTest(
      [...normalizedData],
      [...trainingData.labels],
      testSplit
    );

    // ì‘ë‹µ êµ¬ì„±
    const response = {
      success: true,
      dataset: {
        type: trainingType,
        train: {
          features: splitData.trainFeatures,
          labels: splitData.trainLabels,
          size: splitData.trainFeatures.length,
        },
        test: {
          features: splitData.testFeatures,
          labels: splitData.testLabels,
          size: splitData.testFeatures.length,
        },
        metadata: trainingData.metadata,
        scenarios: trainingData.scenarios,
      },
      config: {
        samplesPerScenario,
        normalizationMode,
        testSplit,
        includeAnomalies,
        timeSeriesLength,
      },
      stats: {
        classDistribution: getClassDistribution(trainingData.labels),
        featureStatistics: getFeatureStatistics(normalizedData),
        dataQuality: assessDataQuality(normalizedData, trainingData.labels),
      },
      meta: {
        processingTime: Date.now() - startTime,
        apiVersion: 'v1.0.0',
        generator: 'AI Training Dataset Generator',
        timestamp: new Date().toISOString(),
      },
    };

    console.log(`âœ… AI í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ`, {
      type: trainingType,
      totalSamples: trainingData.metadata.datasetInfo.totalSamples,
      features: trainingData.metadata.datasetInfo.featureCount,
      processingTime: Date.now() - startTime,
    });

    return NextResponse.json(response);
  } catch (error: any) {
    console.error('âŒ AI í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì˜¤ë¥˜:', error);

    return NextResponse.json(
      {
        success: false,
        error: 'AI í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
        code: 'TRAINING_DATA_ERROR',
        message: error.message,
        meta: {
          processingTime: Date.now() - startTime,
          timestamp: new Date().toISOString(),
        },
      },
      { status: 500 }
    );
  }
}

/**
 * ğŸ” AI í•™ìŠµ ë°ì´í„° ì •ë³´ ë° í…œí”Œë¦¿
 */
export async function GET(request: NextRequest) {
  try {
    const url = new URL(request.url);
    const action = url.searchParams.get('action');

    switch (action) {
      case 'templates':
        return NextResponse.json({
          trainingTypes: {
            classification: {
              description: 'ì‹œë‚˜ë¦¬ì˜¤ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°',
              features: [
                'cpu',
                'memory',
                'disk',
                'network',
                'responseTime',
                'errorRate',
              ],
              labels: [
                'normal',
                'spike',
                'memory_leak',
                'ddos',
                'performance_degradation',
              ],
              useCase: 'ì‹¤ì‹œê°„ ì„œë²„ ìƒíƒœ ë¶„ë¥˜',
            },
            anomaly_detection: {
              description: 'ì´ìƒ íƒì§€ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°',
              features: [
                'cpu',
                'memory',
                'disk',
                'network',
                'responseTime',
                'patterns',
              ],
              labels: [0, 1], // 0: normal, 1: anomaly
              useCase: 'ë¹„ì •ìƒ ì„œë²„ ë™ì‘ íƒì§€',
            },
            prediction: {
              description: 'ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµìš© ë°ì´í„°',
              features: [
                'sequence_data',
                'temporal_patterns',
                'seasonal_effects',
              ],
              labels: ['future_values'],
              useCase: 'ì„œë²„ ë¶€í•˜ ì˜ˆì¸¡',
            },
            clustering: {
              description: 'í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ìš© ë°ì´í„°',
              features: ['cpu', 'memory', 'disk', 'patterns'],
              labels: [], // ë¬´ê°ë… í•™ìŠµ
              useCase: 'ì„œë²„ ë™ì‘ íŒ¨í„´ ê·¸ë£¹í™”',
            },
          },
          timestamp: new Date().toISOString(),
        });

      case 'sample':
        // ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        const sampleType =
          (url.searchParams.get('type') as TrainingType) || 'classification';
        const sampleSize = parseInt(url.searchParams.get('size') || '10');

        aiTrainingGenerator.setScenario('normal');
        const sampleMetrics =
          aiTrainingGenerator.generateTimeSeriesData(sampleSize);
        const processed = processDataForTraining(
          sampleMetrics,
          [],
          'normal',
          sampleType
        );

        return NextResponse.json({
          type: sampleType,
          sampleData: {
            features: processed.features.slice(0, 3), // ì²« 3ê°œë§Œ
            labels: processed.labels.slice(0, 3),
            featureNames: getFeatureNames(sampleType),
          },
          metadata: {
            totalSamples: processed.features.length,
            featureCount: processed.features[0]?.length || 0,
          },
          timestamp: new Date().toISOString(),
        });

      default:
        return NextResponse.json({
          name: 'AI Training Dataset API v1',
          version: 'v1.0.0',
          description: 'AI ëª¨ë¸ í•™ìŠµìš© ë¼ë²¨ë§ëœ ë°ì´í„°ì…‹ ìƒì„±',
          supportedTypes: [
            'classification',
            'anomaly_detection',
            'prediction',
            'clustering',
            'mixed',
          ],
          endpoints: {
            'POST /api/v1/demo/ai-training': 'í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±',
            'GET /api/v1/demo/ai-training?action=templates': 'í•™ìŠµ íƒ€ì… í…œí”Œë¦¿',
            'GET /api/v1/demo/ai-training?action=sample&type=<type>&size=<n>':
              'ìƒ˜í”Œ ë°ì´í„°',
          },
          usage: {
            generateDataset:
              'POST { "trainingType": "classification", "samplesPerScenario": 100 }',
            getSample: 'GET ?action=sample&type=anomaly_detection&size=5',
          },
          timestamp: new Date().toISOString(),
        });
    }
  } catch (error: any) {
    return NextResponse.json(
      {
        success: false,
        error: 'Failed to process AI training request',
        message: error.message,
        timestamp: new Date().toISOString(),
      },
      { status: 500 }
    );
  }
}

/**
 * ğŸ”§ í•™ìŠµ íƒ€ì…ë³„ ë°ì´í„° ì²˜ë¦¬
 */
function processDataForTraining(
  metrics: ServerMetrics[],
  logs: DemoLogEntry[],
  scenario: DemoScenario,
  trainingType: TrainingType
) {
  const features: number[][] = [];
  const labels: (string | number)[] = [];

  for (const metric of metrics) {
    let featureVector: number[] = [];
    let label: string | number = '';

    switch (trainingType) {
      case 'classification':
        featureVector = [
          metric.cpu,
          metric.memory,
          metric.disk,
          metric.networkIn / 1000, // ì •ê·œí™”
          metric.networkOut / 1000,
          metric.responseTime / 100,
          metric.errorRate || 0,
          metric.throughput || 0,
        ];
        label = scenario;
        break;

      case 'anomaly_detection':
        featureVector = [
          metric.cpu,
          metric.memory,
          metric.disk,
          metric.responseTime,
          metric.errorRate || 0,
          calculateAnomalyScore(metric),
        ];
        label = scenario === 'normal' ? 0 : 1;
        break;

      case 'prediction':
        // ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ
        featureVector = [
          metric.cpu,
          metric.memory,
          metric.responseTime,
          metric.activeConnections / 100,
          // ì¶”ê°€ ì‹œê³„ì—´ íŠ¹ì„±ë“¤
          Math.sin((new Date(metric.timestamp).getHours() / 24) * 2 * Math.PI), // ì‹œê°„ ì£¼ê¸°ì„±
          Math.cos((new Date(metric.timestamp).getHours() / 24) * 2 * Math.PI),
        ];
        label = metric.cpu; // ë‹¤ìŒ CPU ì‚¬ìš©ë¥  ì˜ˆì¸¡
        break;

      case 'clustering':
        featureVector = [
          metric.cpu,
          metric.memory,
          metric.disk,
          metric.responseTime,
          metric.activeConnections / 100,
        ];
        label = ''; // ë¬´ê°ë… í•™ìŠµ
        break;

      default:
        featureVector = [metric.cpu, metric.memory, metric.disk];
        label = scenario;
    }

    features.push(featureVector);
    labels.push(label);
  }

  return { features, labels };
}

/**
 * ğŸ“Š ì´ìƒ ì ìˆ˜ ê³„ì‚°
 */
function calculateAnomalyScore(metric: ServerMetrics): number {
  // ê°„ë‹¨í•œ ì´ìƒ ì ìˆ˜ ê³„ì‚° (ë³µí•© ë©”íŠ¸ë¦­ ê¸°ë°˜)
  const cpuWeight = metric.cpu > 80 ? 1 : 0;
  const memoryWeight = metric.memory > 85 ? 1 : 0;
  const responseWeight = metric.responseTime > 500 ? 1 : 0;
  const errorWeight = (metric.errorRate || 0) > 5 ? 1 : 0;

  return (cpuWeight + memoryWeight + responseWeight + errorWeight) / 4;
}

/**
 * ğŸ·ï¸ íŠ¹ì„± ì´ë¦„ ë°˜í™˜
 */
function getFeatureNames(trainingType: TrainingType): string[] {
  switch (trainingType) {
    case 'classification':
      return [
        'cpu',
        'memory',
        'disk',
        'networkIn',
        'networkOut',
        'responseTime',
        'errorRate',
        'throughput',
      ];
    case 'anomaly_detection':
      return [
        'cpu',
        'memory',
        'disk',
        'responseTime',
        'errorRate',
        'anomalyScore',
      ];
    case 'prediction':
      return [
        'cpu',
        'memory',
        'responseTime',
        'activeConnections',
        'hourSin',
        'hourCos',
      ];
    case 'clustering':
      return ['cpu', 'memory', 'disk', 'responseTime', 'activeConnections'];
    default:
      return ['cpu', 'memory', 'disk'];
  }
}

/**
 * ğŸ“ˆ ë°ì´í„° ì •ê·œí™”
 */
function normalizeData(features: number[][], mode: string): number[][] {
  if (features.length === 0) return [];

  const featureCount = features[0].length;
  const normalized: number[][] = [];

  for (let i = 0; i < featureCount; i++) {
    const column = features.map(row => row[i]);
    const mean = column.reduce((sum, val) => sum + val, 0) / column.length;
    const std = Math.sqrt(
      column.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) /
        column.length
    );

    for (let j = 0; j < features.length; j++) {
      if (!normalized[j]) normalized[j] = [];
      normalized[j][i] =
        mode === 'standard'
          ? (features[j][i] - mean) / (std || 1)
          : features[j][i];
    }
  }

  return normalized;
}

/**
 * âœ‚ï¸ Train/Test ë¶„í• 
 */
function splitTrainTest(
  features: number[][],
  labels: (string | number)[],
  testSplit: number
) {
  const totalSamples = features.length;
  const testSize = Math.floor(totalSamples * testSplit);
  const trainSize = totalSamples - testSize;

  // ì…”í”Œì„ ìœ„í•œ ì¸ë±ìŠ¤ ë°°ì—´
  const indices = Array.from({ length: totalSamples }, (_, i) => i);
  for (let i = indices.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [indices[i], indices[j]] = [indices[j], indices[i]];
  }

  const trainIndices = indices.slice(0, trainSize);
  const testIndices = indices.slice(trainSize);

  return {
    trainFeatures: trainIndices.map(i => features[i]),
    trainLabels: trainIndices.map(i => labels[i]),
    testFeatures: testIndices.map(i => features[i]),
    testLabels: testIndices.map(i => labels[i]),
  };
}

/**
 * ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ ê³„ì‚°
 */
function getClassDistribution(
  labels: (string | number)[]
): Record<string, number> {
  const distribution: Record<string, number> = {};
  for (const label of labels) {
    const key = label.toString();
    distribution[key] = (distribution[key] || 0) + 1;
  }
  return distribution;
}

/**
 * ğŸ“ˆ íŠ¹ì„± í†µê³„ ê³„ì‚°
 */
function getFeatureStatistics(features: number[][]): Record<string, any> {
  if (features.length === 0) return {};

  const featureCount = features[0].length;
  const stats: Record<string, any> = {};

  for (let i = 0; i < featureCount; i++) {
    const column = features.map(row => row[i]);
    const mean = column.reduce((sum, val) => sum + val, 0) / column.length;
    const min = Math.min(...column);
    const max = Math.max(...column);
    const std = Math.sqrt(
      column.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) /
        column.length
    );

    stats[`feature_${i}`] = { mean, min, max, std };
  }

  return stats;
}

/**
 * âœ… ë°ì´í„° í’ˆì§ˆ í‰ê°€
 */
function assessDataQuality(
  features: number[][],
  labels: (string | number)[]
): Record<string, any> {
  const totalSamples = features.length;
  const featureCount = features[0]?.length || 0;

  // ê²°ì¸¡ê°’ í™•ì¸
  let missingValues = 0;
  for (const row of features) {
    for (const value of row) {
      if (isNaN(value) || value === null || value === undefined) {
        missingValues++;
      }
    }
  }

  // ë¼ë²¨ ì™„ì„±ë„
  const missingLabels = labels.filter(
    label => label === null || label === undefined || label === ''
  ).length;

  return {
    completeness: 1 - missingValues / (totalSamples * featureCount),
    labelCompleteness: 1 - missingLabels / totalSamples,
    balanceScore: calculateClassBalance(labels),
    featureVariance: calculateFeatureVariance(features),
    overallScore: calculateOverallQuality(features, labels),
  };
}

/**
 * âš–ï¸ í´ë˜ìŠ¤ ê· í˜• ì ìˆ˜
 */
function calculateClassBalance(labels: (string | number)[]): number {
  const distribution = getClassDistribution(labels);
  const counts = Object.values(distribution);
  const mean = counts.reduce((sum, count) => sum + count, 0) / counts.length;
  const variance =
    counts.reduce((sum, count) => sum + Math.pow(count - mean, 2), 0) /
    counts.length;

  return 1 / (1 + variance / mean); // 0-1 ì‚¬ì´ ê°’, 1ì´ ì™„ë²½í•œ ê· í˜•
}

/**
 * ğŸ“Š íŠ¹ì„± ë¶„ì‚° ê³„ì‚°
 */
function calculateFeatureVariance(features: number[][]): number {
  if (features.length === 0) return 0;

  const featureCount = features[0].length;
  let totalVariance = 0;

  for (let i = 0; i < featureCount; i++) {
    const column = features.map(row => row[i]);
    const mean = column.reduce((sum, val) => sum + val, 0) / column.length;
    const variance =
      column.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) /
      column.length;
    totalVariance += variance;
  }

  return totalVariance / featureCount;
}

/**
 * ğŸ¯ ì „ì²´ í’ˆì§ˆ ì ìˆ˜
 */
function calculateOverallQuality(
  features: number[][],
  labels: (string | number)[]
): number {
  const completeness = 1; // ìƒì„±ëœ ë°ì´í„°ì´ë¯€ë¡œ ì™„ì „
  const balance = calculateClassBalance(labels);
  const variance = calculateFeatureVariance(features);
  const normalizedVariance = Math.min(variance / 100, 1); // ì •ê·œí™”

  return completeness * 0.4 + balance * 0.4 + normalizedVariance * 0.2;
}
