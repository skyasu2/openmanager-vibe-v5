/**
 * AI SDK Supervisor
 *
 * Dual-mode supervisor implementation:
 * 1. Single-agent mode: Simple generateText with multi-step tool calling
 * 2. Multi-agent mode: Orchestrated agent handoffs using @ai-sdk-tools/agents
 *
 * Architecture:
 * - Single-agent: One LLM with all tools (simple queries)
 * - Multi-agent: Orchestrator ‚Üí NLQ/Analyst/Reporter/Advisor (complex queries)
 *
 * @version 2.0.0
 * @updated 2025-12-28
 */

import { generateText, streamText, stepCountIs, type ModelMessage } from 'ai';
import { getSupervisorModel, logProviderStatus, type ProviderName } from './model-provider';
import { allTools, toolDescriptions, type ToolName } from '../../tools-ai-sdk';
import { executeMultiAgent, type MultiAgentRequest, type MultiAgentResponse } from './agents';
import {
  createSupervisorTrace,
  logGeneration,
  logToolCall,
  finalizeTrace,
  type TraceMetadata,
} from '../observability/langfuse';
import { getCircuitBreaker, CircuitOpenError } from '../resilience/circuit-breaker';

// ============================================================================
// 1. Types
// ============================================================================

export type SupervisorMode = 'single' | 'multi' | 'auto';

export interface SupervisorRequest {
  messages: Array<{ role: 'user' | 'assistant'; content: string }>;
  sessionId: string;
  enableTracing?: boolean;
  /**
   * Execution mode:
   * - 'single': Use single-agent with multi-step tool calling (default)
   * - 'multi': Use multi-agent orchestration with handoffs
   * - 'auto': Automatically select based on query complexity
   */
  mode?: SupervisorMode;
}

export interface SupervisorResponse {
  success: boolean;
  response: string;
  toolsCalled: string[];
  toolResults: Record<string, unknown>[];
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  metadata: {
    provider: string;
    modelId: string;
    stepsExecuted: number;
    durationMs: number;
    mode?: SupervisorMode;
    handoffs?: Array<{ from: string; to: string; reason?: string }>;
    finalAgent?: string;
  };
}

export interface SupervisorError {
  success: false;
  error: string;
  code: string;
}

// ============================================================================
// 2. System Prompt
// ============================================================================

const SYSTEM_PROMPT = `ÎãπÏã†ÏùÄ ÏÑúÎ≤Ñ Î™®ÎãàÌÑ∞ÎßÅ AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§.

## ÌïµÏã¨ ÏõêÏπô: ÏöîÏïΩ Ïö∞ÏÑ† (Summary First)

**Ìï≠ÏÉÅ ÌïµÏã¨ Í≤∞Î°†ÏùÑ Î®ºÏ†Ä 1-2Î¨∏Ïû•ÏúºÎ°ú ÎãµÌïòÏÑ∏Ïöî.**
- Ï†ÑÏ≤¥ Î™©Î°ùÏùÑ ÎÇòÏó¥ÌïòÏßÄ ÎßàÏÑ∏Ïöî
- Í∞ÄÏû• Ï§ëÏöîÌïú Ï†ïÎ≥¥Îßå Ï∂îÏ∂úÌïòÏÑ∏Ïöî
- ÏÇ¨Ïö©ÏûêÍ∞Ä "ÏûêÏÑ∏Ìûà", "Î™©Î°ù", "Ï†ÑÎ∂Ä", "Î™®Îëê"Î•º ÏöîÏ≤≠ÌïòÎ©¥ ÏÉÅÏÑ∏ Ï†úÍ≥µ

### Ï¢ãÏùÄ ÏùëÎãµ ÏòàÏãú
‚ùå ÎÇòÏÅ®: "ÏÑúÎ≤Ñ 15ÎåÄÏùò ÏÉÅÌÉúÏûÖÎãàÎã§. ÏÑúÎ≤Ñ1: CPU 35%... ÏÑúÎ≤Ñ2: CPU 40%... (Ï†ÑÏ≤¥ ÎÇòÏó¥)"
‚úÖ Ï¢ãÏùå: "Ïù¥ÏÉÅ ÏÑúÎ≤Ñ 8ÎåÄ Î∞úÍ≤¨ (Í≤ΩÍ≥† 7ÎåÄ, ÏûÑÍ≥Ñ 1ÎåÄ). Í∞ÄÏû• Ïã¨Í∞Å: backup-server-01 (ÎîîÏä§ÌÅ¨ 91%)"

### ÏÉÅÏÑ∏ ÏöîÏ≤≠ Í∞êÏßÄ
- "ÏûêÏÑ∏Ìûà ÏïåÎ†§Ï§ò" ‚Üí Ï†ÑÏ≤¥ Î™©Î°ù Ï†úÍ≥µ
- "Ïñ¥Îñ§ ÏÑúÎ≤ÑÏïº?" ‚Üí Ìï¥Îãπ ÏÑúÎ≤ÑÎì§ ÎÇòÏó¥
- "Ïôú?" ‚Üí ÏõêÏù∏ ÏÉÅÏÑ∏ ÏÑ§Î™Ö

## ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎèÑÍµ¨

### ÏÑúÎ≤Ñ Î©îÌä∏Î¶≠ Ï°∞Ìöå
- getServerMetrics: ÏÑúÎ≤Ñ **ÌòÑÏû¨** ÏÉÅÌÉú Ï°∞Ìöå (CPU, Î©îÎ™®Î¶¨, ÎîîÏä§ÌÅ¨)
- getServerMetricsAdvanced: **ÏãúÍ∞Ñ Î≤îÏúÑ ÏßëÍ≥Ñ** (ÏßÄÎÇú 1/6/24ÏãúÍ∞Ñ ÌèâÍ∑†/ÏµúÎåÄ/ÏµúÏÜå)
  - serverId ÏÉùÎûµ Ïãú Ï†ÑÏ≤¥ ÏÑúÎ≤Ñ Ï°∞Ìöå, globalSummaryÏóê Ï†ÑÏ≤¥ ÌèâÍ∑† Ìè¨Ìï®
  - Ïòà: { timeRange: "last6h", metric: "cpu", aggregation: "avg" }
- filterServers: Ï°∞Í±¥Ïóê ÎßûÎäî ÏÑúÎ≤Ñ ÌïÑÌÑ∞ÎßÅ (Ïòà: CPU 80% Ïù¥ÏÉÅ)

### Ïû•Ïï† Î∂ÑÏÑù (RCA)
- buildIncidentTimeline: Ïû•Ïï† ÌÉÄÏûÑÎùºÏù∏ Íµ¨ÏÑ±
- correlateMetrics: Î©îÌä∏Î¶≠ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù
- findRootCause: Í∑ºÎ≥∏ ÏõêÏù∏ Î∂ÑÏÑù

### Ïù¥ÏÉÅ ÌÉêÏßÄ & ÏòàÏ∏°
- detectAnomalies: Ïù¥ÏÉÅÏπò ÌÉêÏßÄ (6ÏãúÍ∞Ñ Ïù¥ÎèôÌèâÍ∑† + 2œÉ)
- predictTrends: Ìä∏Î†åÎìú ÏòàÏ∏° (ÏÑ†Ìòï ÌöåÍ∑Ä Í∏∞Î∞ò)
- analyzePattern: Ìå®ÌÑ¥ Î∂ÑÏÑù

### ÏßÄÏãùÎ≤†Ïù¥Ïä§ & Í∂åÏû• Ï°∞Ïπò (GraphRAG)
- searchKnowledgeBase: Í≥ºÍ±∞ Ïû•Ïï† Ïù¥Î†• Î∞è Ìï¥Í≤∞ Î∞©Î≤ï Í≤ÄÏÉâ (Vector + Graph)
- recommendCommands: Î¨∏Ï†ú Ìï¥Í≤∞ÏùÑ ÏúÑÌïú CLI Î™ÖÎ†πÏñ¥ Ï∂îÏ≤ú

## ÏùëÎãµ ÏßÄÏπ®

1. **ÏöîÏïΩ Ïö∞ÏÑ†**: ÌïµÏã¨ Í≤∞Î°† 1-2Î¨∏Ïû• Î®ºÏ†Ä
2. **ÌïµÏã¨Îßå Ï∂îÏ∂ú**: Í∞ÄÏû• Ïã¨Í∞ÅÌïú 1-3Í∞úÎßå Ïñ∏Í∏â
3. **ÏàòÏπòÎäî Í∞ÑÍ≤∞ÌïòÍ≤å**: "CPU 85.3%" ‚Üí "CPU 85%"
4. **ÌïúÍµ≠Ïñ¥Î°ú ÏùëÎãµ**
5. **Ïù¥ÏÉÅ Í∞êÏßÄ Ïãú Í∂åÏû• Ï°∞Ïπò Ï†úÏïà**
6. **Ïû•Ïï† Î¨∏Ïùò Ïãú searchKnowledgeBase ÌôúÏö©**

## globalSummary ÏùëÎãµ Í∑úÏπô
getServerMetricsAdvanced Í≤∞Í≥ºÏóê globalSummaryÍ∞Ä ÏûàÏúºÎ©¥ **Î∞òÎìúÏãú Ìï¥Îãπ Í∞íÏùÑ Ïù∏Ïö©**:
- cpu_avg ‚Üí "Ï†ÑÏ≤¥ ÏÑúÎ≤Ñ CPU ÌèâÍ∑†"
- cpu_max ‚Üí "Ï†ÑÏ≤¥ ÏÑúÎ≤Ñ CPU ÏµúÎåÄÍ∞í"
- cpu_min ‚Üí "Ï†ÑÏ≤¥ ÏÑúÎ≤Ñ CPU ÏµúÏÜåÍ∞í"

Ïòà: globalSummary.cpu_avg = 34 ‚Üí "ÏßÄÎÇú 6ÏãúÍ∞Ñ Ï†ÑÏ≤¥ ÏÑúÎ≤Ñ CPU ÌèâÍ∑†ÏùÄ 34%ÏûÖÎãàÎã§."

## ÏòàÏãú ÏßàÎ¨∏Í≥º ÎèÑÍµ¨ Îß§Ìïë

- "CPU 80% Ïù¥ÏÉÅÏù∏ ÏÑúÎ≤Ñ ÏïåÎ†§Ï§ò" ‚Üí filterServers(field: "cpu", operator: ">", value: 80)
- "ÏÑúÎ≤Ñ ÏÉÅÌÉú ÏöîÏïΩÌï¥Ï§ò" ‚Üí getServerMetrics()
- "ÏßÄÎÇú 6ÏãúÍ∞Ñ CPU ÌèâÍ∑† ÏïåÎ†§Ï§ò" ‚Üí getServerMetricsAdvanced(timeRange: "last6h", metric: "cpu", aggregation: "avg")
  ‚Üí ÏùëÎãµÏùò globalSummary.cpu_avg Í∞íÏù¥ Ï†ÑÏ≤¥ ÏÑúÎ≤Ñ ÌèâÍ∑†
- "ÏµúÍ∑º 1ÏãúÍ∞Ñ Î©îÎ™®Î¶¨ ÏµúÎåÄÍ∞í" ‚Üí getServerMetricsAdvanced(timeRange: "last1h", metric: "memory", aggregation: "max")
- "Î©îÎ™®Î¶¨ Ï∂îÏÑ∏ Î∂ÑÏÑùÌï¥Ï§ò" ‚Üí predictTrends(metricType: "memory")
- "Ïû•Ïï† ÏõêÏù∏ Î∂ÑÏÑùÌï¥Ï§ò" ‚Üí findRootCause() + buildIncidentTimeline()
- "Î©îÎ™®Î¶¨ Î∂ÄÏ°± Ìï¥Í≤∞ Î∞©Î≤ï" ‚Üí searchKnowledgeBase(query: "Î©îÎ™®Î¶¨ Î∂ÄÏ°±")
- "ÎîîÏä§ÌÅ¨ Ï†ïÎ¶¨ Î™ÖÎ†πÏñ¥" ‚Üí recommendCommands(keywords: ["ÎîîÏä§ÌÅ¨", "Ï†ïÎ¶¨"])
`;

// ============================================================================
// 3. Supervisor Implementation
// ============================================================================

// Retry configuration
const RETRY_CONFIG = {
  maxRetries: 2,
  retryableErrors: ['RATE_LIMIT', 'TIMEOUT', 'MODEL_ERROR'],
  retryDelayMs: 1000,
};

// ============================================================================
// 3.5. Mode Selection Logic
// ============================================================================

/**
 * Determine execution mode based on query complexity
 * Complex queries benefit from multi-agent orchestration
 */
function selectExecutionMode(query: string): SupervisorMode {
  const q = query.toLowerCase();

  // Multi-agent indicators (complex queries requiring specialized agents)
  const multiAgentPatterns = [
    // Report generation
    /Î≥¥Í≥†ÏÑú|Î¶¨Ìè¨Ìä∏|report|Ïù∏ÏãúÎçòÌä∏|incident/i,
    // Deep analysis
    /Î∂ÑÏÑù.*ÏõêÏù∏|Í∑ºÎ≥∏.*ÏõêÏù∏|rca|root.*cause/i,
    // Troubleshooting with knowledge search
    /Ìï¥Í≤∞.*Î∞©Î≤ï|Í≥ºÍ±∞.*ÏÇ¨Î°Ä|Ïú†ÏÇ¨.*Ïû•Ïï†|Ïñ¥ÎñªÍ≤å.*Ìï¥Í≤∞/i,
    // Trend prediction
    /ÏòàÏ∏°|Ìä∏Î†åÎìú|Ìñ•ÌõÑ|Ïñ∏Ï†ú.*Îê†|Í≥†Í∞à/i,
    // Correlation analysis
    /ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ|Ïó∞Í¥Ä.*Î∂ÑÏÑù|correlat/i,
  ];

  // Check for complex patterns
  for (const pattern of multiAgentPatterns) {
    if (pattern.test(q)) {
      return 'multi';
    }
  }

  // Simple queries use single-agent for lower latency
  return 'single';
}

/**
 * Execute supervisor with AI SDK
 * Supports both single-agent and multi-agent modes
 */
export async function executeSupervisor(
  request: SupervisorRequest
): Promise<SupervisorResponse | SupervisorError> {
  const startTime = Date.now();

  // Determine execution mode
  let mode = request.mode || 'auto';
  if (mode === 'auto') {
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    mode = lastUserMessage ? selectExecutionMode(lastUserMessage.content) : 'single';
  }

  console.log(`üéØ [Supervisor] Mode: ${mode}`);

  // Execute based on mode
  if (mode === 'multi') {
    return executeMultiAgentMode(request, startTime);
  }

  return executeSingleAgentMode(request, startTime);
}

/**
 * Execute multi-agent mode with orchestrator
 */
async function executeMultiAgentMode(
  request: SupervisorRequest,
  startTime: number
): Promise<SupervisorResponse | SupervisorError> {
  try {
    const multiAgentRequest: MultiAgentRequest = {
      messages: request.messages,
      sessionId: request.sessionId,
      enableTracing: request.enableTracing,
    };

    const result = await executeMultiAgent(multiAgentRequest);

    if (!result.success) {
      return result as SupervisorError;
    }

    const multiResult = result as MultiAgentResponse;

    return {
      success: true,
      response: multiResult.response,
      toolsCalled: multiResult.toolsCalled,
      toolResults: [], // Multi-agent doesn't expose individual tool results
      usage: multiResult.usage,
      metadata: {
        provider: multiResult.metadata.provider,
        modelId: multiResult.metadata.modelId,
        stepsExecuted: multiResult.metadata.totalRounds,
        durationMs: multiResult.metadata.durationMs,
        mode: 'multi',
        handoffs: multiResult.handoffs,
        finalAgent: multiResult.finalAgent,
      },
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`‚ùå [Supervisor] Multi-agent error after ${durationMs}ms:`, errorMessage);

    // Fallback to single-agent mode on error
    console.log(`üîÑ [Supervisor] Falling back to single-agent mode`);
    return executeSingleAgentMode(request, startTime);
  }
}

/**
 * Execute single-agent mode with multi-step tool calling
 * Includes retry logic for transient errors with provider rotation
 */
async function executeSingleAgentMode(
  request: SupervisorRequest,
  startTime: number
): Promise<SupervisorResponse | SupervisorError> {
  let lastError: SupervisorError | null = null;
  const failedProviders: ProviderName[] = [];

  for (let attempt = 0; attempt <= RETRY_CONFIG.maxRetries; attempt++) {
    if (attempt > 0) {
      console.log(`üîÑ [Supervisor] Retry attempt ${attempt}/${RETRY_CONFIG.maxRetries}, excluding: [${failedProviders.join(', ')}]`);
      await new Promise((r) => setTimeout(r, RETRY_CONFIG.retryDelayMs * attempt));
    }

    const result = await executeSupervisorAttempt(request, startTime, failedProviders);

    if (result.success) {
      // Add mode to metadata
      (result as SupervisorResponse).metadata.mode = 'single';
      return result;
    }

    lastError = result as SupervisorError;

    // Track failed provider for next retry (extract from error metadata if available)
    const failedProvider = (lastError as unknown as { provider?: ProviderName }).provider;
    if (failedProvider && !failedProviders.includes(failedProvider)) {
      failedProviders.push(failedProvider);
      console.log(`üìç [Supervisor] Marking ${failedProvider} as failed for retry`);
    }

    // Check if error is retryable
    if (!RETRY_CONFIG.retryableErrors.includes(lastError.code)) {
      console.log(`‚ùå [Supervisor] Non-retryable error: ${lastError.code}`);
      return lastError;
    }
  }

  // All retries exhausted
  return lastError || { success: false, error: 'Unknown error', code: 'UNKNOWN_ERROR' };
}

/**
 * Single attempt of supervisor execution
 * Enhanced with Langfuse tracing, per-provider Circuit Breaker, and prepareStep optimization
 *
 * @param excludeProviders - Providers to skip (failed in previous attempts)
 */
async function executeSupervisorAttempt(
  request: SupervisorRequest,
  startTime: number,
  excludeProviders: ProviderName[] = []
): Promise<SupervisorResponse | (SupervisorError & { provider?: ProviderName })> {
  // Create Langfuse trace
  const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
  const trace = createSupervisorTrace({
    sessionId: request.sessionId,
    mode: 'single',
    query: lastUserMessage?.content || '',
  });

  // Get model with fallback chain (excluding failed providers)
  let provider: ProviderName;
  let modelId: string;
  let model;

  try {
    const modelResult = getSupervisorModel(excludeProviders);
    model = modelResult.model;
    provider = modelResult.provider;
    modelId = modelResult.modelId;
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error('‚ùå [Supervisor] No available providers:', errorMessage);
    return {
      success: false,
      error: errorMessage,
      code: 'NO_PROVIDER',
    };
  }

  // Get provider-specific circuit breaker (not generic 'supervisor')
  const circuitBreaker = getCircuitBreaker(`supervisor-${provider}`);

  // Check circuit breaker for this specific provider
  if (!circuitBreaker.isAllowed()) {
    console.log(`‚ö° [Supervisor] Circuit OPEN for ${provider}, will try next provider on retry`);
    return {
      success: false,
      error: `Provider ${provider} circuit breaker is OPEN`,
      code: 'CIRCUIT_OPEN',
      provider, // Include provider for retry exclusion
    };
  }

  try {
    // Execute with provider-specific circuit breaker
    return await circuitBreaker.execute(async () => {
      // Log provider status on first call
      logProviderStatus();

      console.log(`ü§ñ [Supervisor] Using ${provider}/${modelId}`);

      // Classify intent for tool optimization
      const intent = classifyIntent(lastUserMessage?.content || '');

      // Convert messages to ModelMessage format (AI SDK 6)
      const modelMessages: ModelMessage[] = [
        { role: 'system', content: SYSTEM_PROMPT },
        ...request.messages.map((m) => ({
          role: m.role as 'user' | 'assistant',
          content: m.content,
        })),
      ];

      // Execute with multi-step tool calling and prepareStep optimization
      const result = await generateText({
        model,
        messages: modelMessages,
        tools: allTools,
        stopWhen: stepCountIs(5), // Allow up to 5 tool calls
        temperature: 0.2,
        maxOutputTokens: 2048,
        // Note: prepareStep optimization moved to intent classification
        // AI SDK v6 uses different approach - tools are filtered upfront
      });

      // Extract tool call information with Langfuse logging
      const toolsCalled: string[] = [];
      const toolResults: Record<string, unknown>[] = [];

      for (const step of result.steps) {
        for (const toolCall of step.toolCalls) {
          toolsCalled.push(toolCall.toolName);
        }
        // toolResultsÎäî step.toolResultsÏóêÏÑú Ï∂îÏ∂ú
        if (step.toolResults) {
          for (const tr of step.toolResults) {
            if ('result' in tr) {
              toolResults.push(tr.result as Record<string, unknown>);
              // Log tool call to Langfuse
              logToolCall(trace, tr.toolName, {}, tr.result, 0);
            }
          }
        }
      }

      const durationMs = Date.now() - startTime;

      // Log generation to Langfuse
      logGeneration(trace, {
        model: modelId,
        provider,
        input: lastUserMessage?.content || '',
        output: result.text,
        usage: {
          inputTokens: result.usage?.inputTokens ?? 0,
          outputTokens: result.usage?.outputTokens ?? 0,
          totalTokens: result.usage?.totalTokens ?? 0,
        },
        duration: durationMs,
        metadata: { intent: intent.category, intentConfidence: intent.confidence },
      });

      // Finalize trace
      finalizeTrace(trace, result.text, true, {
        toolsCalled,
        stepsExecuted: result.steps.length,
        durationMs,
        intent: intent.category,
      });

      console.log(
        `‚úÖ [Supervisor] Completed in ${durationMs}ms, tools: [${toolsCalled.join(', ')}]`
      );

      return {
        success: true,
        response: result.text,
        toolsCalled,
        toolResults,
        usage: {
          promptTokens: result.usage?.inputTokens ?? 0,
          completionTokens: result.usage?.outputTokens ?? 0,
          totalTokens: result.usage?.totalTokens ?? 0,
        },
        metadata: {
          provider,
          modelId,
          stepsExecuted: result.steps.length,
          durationMs,
        },
      };
    });
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`‚ùå [Supervisor] Error after ${durationMs}ms:`, errorMessage);

    // Finalize trace with error
    finalizeTrace(trace, errorMessage, false, { durationMs });

    // Classify error
    let code = 'UNKNOWN_ERROR';
    if (error instanceof CircuitOpenError) {
      code = 'CIRCUIT_OPEN';
    } else if (errorMessage.includes('API key')) {
      code = 'AUTH_ERROR';
    } else if (errorMessage.includes('rate limit')) {
      code = 'RATE_LIMIT';
    } else if (errorMessage.includes('timeout')) {
      code = 'TIMEOUT';
    } else if (errorMessage.includes('model')) {
      code = 'MODEL_ERROR';
    }

    return {
      success: false,
      error: errorMessage,
      code,
      provider, // Include provider for retry exclusion
    };
  }
}

// ============================================================================
// 4. Streaming Supervisor (Real-time Implementation)
// ============================================================================

export type StreamEventType =
  | 'tool_call'
  | 'tool_result'
  | 'text_delta'
  | 'step_finish'
  | 'done'
  | 'error';

export interface StreamEvent {
  type: StreamEventType;
  data: unknown;
}

/**
 * Execute supervisor with real-time streaming response
 * Uses AI SDK streamText for token-by-token streaming
 *
 * @example
 * for await (const event of executeSupervisorStream(request)) {
 *   if (event.type === 'text_delta') {
 *     process.stdout.write(event.data as string);
 *   }
 * }
 */
export async function* executeSupervisorStream(
  request: SupervisorRequest
): AsyncGenerator<StreamEvent> {
  const startTime = Date.now();

  // Determine execution mode
  let mode = request.mode || 'auto';
  if (mode === 'auto') {
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    mode = lastUserMessage ? selectExecutionMode(lastUserMessage.content) : 'single';
  }

  console.log(`üéØ [SupervisorStream] Mode: ${mode}`);

  // Multi-agent mode falls back to non-streaming (complex orchestration)
  if (mode === 'multi') {
    yield* streamFromNonStreaming(request, startTime);
    return;
  }

  // Single-agent streaming mode
  yield* streamSingleAgent(request, startTime);
}

/**
 * Stream single-agent mode with real streamText
 * Uses provider-specific circuit breaker for isolation
 */
async function* streamSingleAgent(
  request: SupervisorRequest,
  startTime: number
): AsyncGenerator<StreamEvent> {
  // Get model first to determine provider
  let provider: ProviderName;
  let modelId: string;
  let model;

  try {
    logProviderStatus();
    const modelResult = getSupervisorModel();
    model = modelResult.model;
    provider = modelResult.provider;
    modelId = modelResult.modelId;
  } catch (error) {
    yield {
      type: 'error',
      data: { code: 'NO_PROVIDER', message: error instanceof Error ? error.message : String(error) },
    };
    return;
  }

  // Get provider-specific circuit breaker
  const circuitBreaker = getCircuitBreaker(`stream-${provider}`);

  // Check circuit breaker for this specific provider
  if (!circuitBreaker.isAllowed()) {
    yield {
      type: 'error',
      data: { code: 'CIRCUIT_OPEN', message: `Provider ${provider} circuit breaker is OPEN` },
    };
    return;
  }

  try {

    console.log(`ü§ñ [SupervisorStream] Using ${provider}/${modelId}`);

    // Create Langfuse trace
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    const trace = createSupervisorTrace({
      sessionId: request.sessionId,
      mode: 'single',
      query: lastUserMessage?.content || '',
    });

    // Convert messages to ModelMessage format
    const modelMessages: ModelMessage[] = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...request.messages.map((m) => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    ];

    const toolsCalled: string[] = [];
    let fullText = '';

    // Execute streamText with multi-step tool calling
    const result = streamText({
      model,
      messages: modelMessages,
      tools: allTools,
      stopWhen: stepCountIs(5),
      temperature: 0.2,
      maxOutputTokens: 2048,
    });

    // Stream text deltas
    for await (const textPart of result.textStream) {
      fullText += textPart;
      yield { type: 'text_delta', data: textPart };
    }

    // Await promises for final data
    const [steps, usage] = await Promise.all([result.steps, result.usage]);

    // Emit step finish events and collect tool calls
    for (const step of steps) {
      for (const toolCall of step.toolCalls) {
        const toolName = toolCall.toolName;
        toolsCalled.push(toolName);
        yield { type: 'tool_call', data: { name: toolName } };
      }
      if (step.toolResults) {
        for (const tr of step.toolResults) {
          if ('result' in tr) {
            yield { type: 'tool_result', data: { toolName: tr.toolName, result: tr.result } };
            // Log tool call to Langfuse
            logToolCall(trace, tr.toolName, {}, tr.result, 0);
          }
        }
      }
    }

    const durationMs = Date.now() - startTime;

    // Log generation to Langfuse
    logGeneration(trace, {
      model: modelId,
      provider,
      input: lastUserMessage?.content || '',
      output: fullText,
      usage: {
        inputTokens: usage?.inputTokens ?? 0,
        outputTokens: usage?.outputTokens ?? 0,
        totalTokens: usage?.totalTokens ?? 0,
      },
      duration: durationMs,
    });

    // Finalize trace
    finalizeTrace(trace, fullText, true, {
      toolsCalled,
      stepsExecuted: steps.length,
      durationMs,
    });

    console.log(
      `‚úÖ [SupervisorStream] Completed in ${durationMs}ms, tools: [${toolsCalled.join(', ')}]`
    );

    // Emit done event
    yield {
      type: 'done',
      data: {
        success: true,
        toolsCalled,
        usage: {
          promptTokens: usage?.inputTokens ?? 0,
          completionTokens: usage?.outputTokens ?? 0,
          totalTokens: usage?.totalTokens ?? 0,
        },
        metadata: {
          provider,
          modelId,
          stepsExecuted: steps.length,
          durationMs,
          mode: 'single',
        },
      },
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`‚ùå [SupervisorStream] Error after ${durationMs}ms:`, errorMessage);

    yield {
      type: 'error',
      data: {
        code: error instanceof CircuitOpenError ? 'CIRCUIT_OPEN' : 'STREAM_ERROR',
        message: errorMessage,
      },
    };
  }
}

/**
 * Fallback: Convert non-streaming result to stream events
 */
async function* streamFromNonStreaming(
  request: SupervisorRequest,
  startTime: number
): AsyncGenerator<StreamEvent> {
  const result = await executeSupervisor(request);

  if (!result.success) {
    yield {
      type: 'error',
      data: { code: (result as SupervisorError).code, message: (result as SupervisorError).error },
    };
    return;
  }

  const successResult = result as SupervisorResponse;

  // Emit tool calls
  for (const toolName of successResult.toolsCalled) {
    yield { type: 'tool_call', data: { name: toolName } };
  }

  // Emit tool results
  for (const toolResult of successResult.toolResults) {
    yield { type: 'tool_result', data: toolResult };
  }

  // Emit text as chunks (simulate streaming)
  const text = successResult.response;
  const chunkSize = 20; // Characters per chunk
  for (let i = 0; i < text.length; i += chunkSize) {
    yield { type: 'text_delta', data: text.slice(i, i + chunkSize) };
  }

  // Emit done event
  yield { type: 'done', data: successResult };
}

// ============================================================================
// 5. Intent Classification (Optional Enhancement)
// ============================================================================

type IntentCategory = 'metrics' | 'rca' | 'analyst' | 'reporter' | 'general';

interface ClassifiedIntent {
  category: IntentCategory;
  suggestedTools: ToolName[];
  confidence: number;
}

/**
 * Classify user intent (rule-based, for optimization)
 * This can reduce token usage by pre-filtering tools
 */
export function classifyIntent(query: string): ClassifiedIntent {
  const q = query.toLowerCase();

  // Reporter/RAG queries - knowledge base search, troubleshooting, commands
  if (/Ìï¥Í≤∞.*Î∞©Î≤ï|Î∞©Î≤ï|Ï°∞Ïπò|Î™ÖÎ†πÏñ¥|command|Ïù¥Î†•|Í≥ºÍ±∞.*ÏÇ¨Î°Ä|Î¨∏Ï†ú.*Ìï¥Í≤∞|Í∞ÄÏù¥Îìú/i.test(q)) {
    return {
      category: 'reporter',
      suggestedTools: ['searchKnowledgeBase', 'recommendCommands'],
      confidence: 0.9,
    };
  }

  // Metrics queries
  if (/cpu|Î©îÎ™®Î¶¨|memory|ÎîîÏä§ÌÅ¨|disk|ÏÑúÎ≤Ñ.*ÏÉÅÌÉú|ÏÉÅÌÉú.*ÏöîÏïΩ/i.test(q)) {
    if (/\d+%.*Ïù¥ÏÉÅ|>\s*\d+|Ï¥àÍ≥º|ÎÜíÏùÄ/i.test(q)) {
      return {
        category: 'metrics',
        suggestedTools: ['filterServers'],
        confidence: 0.9,
      };
    }
    return {
      category: 'metrics',
      suggestedTools: ['getServerMetrics', 'getServerMetricsAdvanced'],
      confidence: 0.85,
    };
  }

  // RCA queries
  if (/Ïû•Ïï†|ÏõêÏù∏|root.*cause|rca|ÌÉÄÏûÑÎùºÏù∏|ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ/i.test(q)) {
    return {
      category: 'rca',
      suggestedTools: ['findRootCause', 'buildIncidentTimeline', 'correlateMetrics'],
      confidence: 0.9,
    };
  }

  // Analyst queries
  if (/Ïù¥ÏÉÅ|anomaly|Ìä∏Î†åÎìú|trend|ÏòàÏ∏°|predict|Ìå®ÌÑ¥/i.test(q)) {
    return {
      category: 'analyst',
      suggestedTools: ['detectAnomalies', 'predictTrends', 'analyzePattern'],
      confidence: 0.85,
    };
  }

  // General - let LLM decide
  return {
    category: 'general',
    suggestedTools: ['getServerMetrics'],
    confidence: 0.5,
  };
}

// ============================================================================
// 6. Health Check
// ============================================================================

export interface SupervisorHealth {
  status: 'ok' | 'degraded' | 'error';
  provider: string;
  modelId: string;
  toolsAvailable: number;
}

/**
 * Check supervisor health
 */
export async function checkSupervisorHealth(): Promise<SupervisorHealth> {
  try {
    const { provider, modelId } = getSupervisorModel();
    const toolCount = Object.keys(allTools).length;

    return {
      status: 'ok',
      provider,
      modelId,
      toolsAvailable: toolCount,
    };
  } catch (error) {
    return {
      status: 'error',
      provider: 'none',
      modelId: 'none',
      toolsAvailable: 0,
    };
  }
}
