/**
 * AI SDK Supervisor
 *
 * Dual-mode supervisor implementation:
 * 1. Single-agent mode: Simple generateText with multi-step tool calling
 * 2. Multi-agent mode: Orchestrated agent handoffs using @ai-sdk-tools/agents
 *
 * Architecture:
 * - Single-agent: One LLM with all tools (simple queries)
 * - Multi-agent: Orchestrator â†’ NLQ/Analyst/Reporter/Advisor (complex queries)
 *
 * @version 2.0.0
 * @updated 2025-12-28
 */

import {
  generateText,
  streamText,
  stepCountIs,
  hasToolCall,
  createUIMessageStream,
  createUIMessageStreamResponse,
  generateId, // ğŸ¯ P2-2: Cryptographically secure ID generation
  type ModelMessage,
} from 'ai';
import {
  getSupervisorModel,
  getSupervisorModelWithQuota,
  recordModelUsage,
  logProviderStatus,
  type ProviderName,
} from './model-provider';
import {
  TIMEOUT_CONFIG,
  getHardTimeout,
  getWarningThreshold,
  logTimeoutWarning,
  logTimeoutError,
} from '../../config/timeout-config';
import { allTools, toolDescriptions, type ToolName } from '../../tools-ai-sdk';
import { executeMultiAgent, executeMultiAgentStream, type MultiAgentRequest, type MultiAgentResponse } from './agents';
import {
  createSupervisorTrace,
  logGeneration,
  logToolCall,
  finalizeTrace,
  type TraceMetadata,
} from '../observability/langfuse';
import { getCircuitBreaker, CircuitOpenError } from '../resilience/circuit-breaker';

// ============================================================================
// 1. Types
// ============================================================================

export type SupervisorMode = 'single' | 'multi' | 'auto';

export interface SupervisorRequest {
  messages: Array<{ role: 'user' | 'assistant'; content: string }>;
  sessionId: string;
  enableTracing?: boolean;
  /**
   * Execution mode:
   * - 'single': Use single-agent with multi-step tool calling (default)
   * - 'multi': Use multi-agent orchestration with handoffs
   * - 'auto': Automatically select based on query complexity
   */
  mode?: SupervisorMode;
}

export interface SupervisorResponse {
  success: boolean;
  response: string;
  toolsCalled: string[];
  toolResults: Record<string, unknown>[];
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  metadata: {
    provider: string;
    modelId: string;
    stepsExecuted: number;
    durationMs: number;
    mode?: SupervisorMode;
    handoffs?: Array<{ from: string; to: string; reason?: string }>;
    finalAgent?: string;
  };
}

export interface SupervisorError {
  success: false;
  error: string;
  code: string;
}

// ============================================================================
// 2. System Prompt
// ============================================================================

const SYSTEM_PROMPT = `ë‹¹ì‹ ì€ ì„œë²„ ëª¨ë‹ˆí„°ë§ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## í•µì‹¬ ì›ì¹™: ìš”ì•½ ìš°ì„  (Summary First)

**í•­ìƒ í•µì‹¬ ê²°ë¡ ì„ ë¨¼ì € 1-2ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.**
- ì „ì²´ ëª©ë¡ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”
- ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”
- ì‚¬ìš©ìê°€ "ìì„¸íˆ", "ëª©ë¡", "ì „ë¶€", "ëª¨ë‘"ë¥¼ ìš”ì²­í•˜ë©´ ìƒì„¸ ì œê³µ

### ì¢‹ì€ ì‘ë‹µ ì˜ˆì‹œ
âŒ ë‚˜ì¨: "ì„œë²„ 15ëŒ€ì˜ ìƒíƒœì…ë‹ˆë‹¤. ì„œë²„1: CPU 35%... ì„œë²„2: CPU 40%... (ì „ì²´ ë‚˜ì—´)"
âœ… ì¢‹ìŒ: "ì´ìƒ ì„œë²„ 8ëŒ€ ë°œê²¬ (ê²½ê³  7ëŒ€, ì„ê³„ 1ëŒ€). ê°€ì¥ ì‹¬ê°: backup-server-01 (ë””ìŠ¤í¬ 91%)"

### ìƒì„¸ ìš”ì²­ ê°ì§€
- "ìì„¸íˆ ì•Œë ¤ì¤˜" â†’ ì „ì²´ ëª©ë¡ ì œê³µ
- "ì–´ë–¤ ì„œë²„ì•¼?" â†’ í•´ë‹¹ ì„œë²„ë“¤ ë‚˜ì—´
- "ì™œ?" â†’ ì›ì¸ ìƒì„¸ ì„¤ëª…

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

### ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒ
- getServerMetrics: ì„œë²„ **í˜„ì¬** ìƒíƒœ ì¡°íšŒ (CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬)
- getServerMetricsAdvanced: **ì‹œê°„ ë²”ìœ„ ì§‘ê³„** (ì§€ë‚œ 1/6/24ì‹œê°„ í‰ê· /ìµœëŒ€/ìµœì†Œ)
  - serverId ìƒëµ ì‹œ ì „ì²´ ì„œë²„ ì¡°íšŒ, globalSummaryì— ì „ì²´ í‰ê·  í¬í•¨
  - ì˜ˆ: { timeRange: "last6h", metric: "cpu", aggregation: "avg" }
- filterServers: ì¡°ê±´ì— ë§ëŠ” ì„œë²„ í•„í„°ë§ (ì˜ˆ: CPU 80% ì´ìƒ)

### ì¥ì•  ë¶„ì„ (RCA)
- buildIncidentTimeline: ì¥ì•  íƒ€ì„ë¼ì¸ êµ¬ì„±
- correlateMetrics: ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
- findRootCause: ê·¼ë³¸ ì›ì¸ ë¶„ì„

### ì´ìƒ íƒì§€ & ì˜ˆì¸¡
- detectAnomalies: ì´ìƒì¹˜ íƒì§€ (6ì‹œê°„ ì´ë™í‰ê·  + 2Ïƒ)
- predictTrends: íŠ¸ë Œë“œ ì˜ˆì¸¡ (ì„ í˜• íšŒê·€ ê¸°ë°˜)
- analyzePattern: íŒ¨í„´ ë¶„ì„

### ì§€ì‹ë² ì´ìŠ¤ & ê¶Œì¥ ì¡°ì¹˜ (GraphRAG)
- searchKnowledgeBase: ê³¼ê±° ì¥ì•  ì´ë ¥ ë° í•´ê²° ë°©ë²• ê²€ìƒ‰ (Vector + Graph)
- recommendCommands: ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ CLI ëª…ë ¹ì–´ ì¶”ì²œ

## ì‘ë‹µ ì§€ì¹¨

1. **ìš”ì•½ ìš°ì„ **: í•µì‹¬ ê²°ë¡  1-2ë¬¸ì¥ ë¨¼ì €
2. **í•µì‹¬ë§Œ ì¶”ì¶œ**: ê°€ì¥ ì‹¬ê°í•œ 1-3ê°œë§Œ ì–¸ê¸‰
3. **ìˆ˜ì¹˜ëŠ” ê°„ê²°í•˜ê²Œ**: "CPU 85.3%" â†’ "CPU 85%"
4. **í•œêµ­ì–´ë¡œ ì‘ë‹µ / Respond in Korean** (í•œì ì ˆëŒ€ ê¸ˆì§€ / No Chinese characters, ê¸°ìˆ ìš©ì–´ëŠ” ì˜ì–´ í—ˆìš© / Technical terms in English OK)
5. **ì´ìƒ ê°ì§€ ì‹œ ê¶Œì¥ ì¡°ì¹˜ ì œì•ˆ**
6. **ì¥ì•  ë¬¸ì˜ ì‹œ searchKnowledgeBase í™œìš©**

## globalSummary ì‘ë‹µ ê·œì¹™
getServerMetricsAdvanced ê²°ê³¼ì— globalSummaryê°€ ìˆìœ¼ë©´ **ë°˜ë“œì‹œ í•´ë‹¹ ê°’ì„ ì¸ìš©**:
- cpu_avg â†’ "ì „ì²´ ì„œë²„ CPU í‰ê· "
- cpu_max â†’ "ì „ì²´ ì„œë²„ CPU ìµœëŒ€ê°’"
- cpu_min â†’ "ì „ì²´ ì„œë²„ CPU ìµœì†Œê°’"

ì˜ˆ: globalSummary.cpu_avg = 34 â†’ "ì§€ë‚œ 6ì‹œê°„ ì „ì²´ ì„œë²„ CPU í‰ê· ì€ 34%ì…ë‹ˆë‹¤."

## ì˜ˆì‹œ ì§ˆë¬¸ê³¼ ë„êµ¬ ë§¤í•‘

- "CPU 80% ì´ìƒì¸ ì„œë²„ ì•Œë ¤ì¤˜" â†’ filterServers(field: "cpu", operator: ">", value: 80)
- "ì„œë²„ ìƒíƒœ ìš”ì•½í•´ì¤˜" â†’ getServerMetrics()
- "ì§€ë‚œ 6ì‹œê°„ CPU í‰ê·  ì•Œë ¤ì¤˜" â†’ getServerMetricsAdvanced(timeRange: "last6h", metric: "cpu", aggregation: "avg")
  â†’ ì‘ë‹µì˜ globalSummary.cpu_avg ê°’ì´ ì „ì²´ ì„œë²„ í‰ê· 
- "ìµœê·¼ 1ì‹œê°„ ë©”ëª¨ë¦¬ ìµœëŒ€ê°’" â†’ getServerMetricsAdvanced(timeRange: "last1h", metric: "memory", aggregation: "max")
- "ë©”ëª¨ë¦¬ ì¶”ì„¸ ë¶„ì„í•´ì¤˜" â†’ predictTrends(metricType: "memory")
- "ì¥ì•  ì›ì¸ ë¶„ì„í•´ì¤˜" â†’ findRootCause() + buildIncidentTimeline()
- "ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²° ë°©ë²•" â†’ searchKnowledgeBase(query: "ë©”ëª¨ë¦¬ ë¶€ì¡±")
- "ë””ìŠ¤í¬ ì •ë¦¬ ëª…ë ¹ì–´" â†’ recommendCommands(keywords: ["ë””ìŠ¤í¬", "ì •ë¦¬"])

## ë³´ê³ ì„œ ì‘ì„± í’ˆì§ˆ ê·œì¹™

### ê·¼ë³¸ ì›ì¸ ë¶„ì„ í•„ìˆ˜ ê·œì¹™
- **"ì›ì¸ ë¶ˆëª…" ê¸ˆì§€**: ë°˜ë“œì‹œ ê°€ì„¤ì´ë¼ë„ ì œì‹œí•˜ê³  ì‹ ë¢°ë„(%) ëª…ì‹œ
- **ë©”íŠ¸ë¦­ ì§ì ‘ ì¸ìš©**: "CPU 85%ëŠ” ì •ìƒ ë²”ìœ„(40-60%)ì˜ 170% ìˆ˜ì¤€"
- **ìƒê´€ê´€ê³„ ë¶„ì„**: "CPU ê¸‰ì¦ê³¼ ë™ì‹œì— ë©”ëª¨ë¦¬ 20% ì¦ê°€ â†’ í”„ë¡œì„¸ìŠ¤ í­ì£¼ ê°€ëŠ¥ì„±"
- **ì‹œê°„ ì¶”ì´ ì–¸ê¸‰**: "ì§€ë‚œ 6ì‹œê°„ê°„ 68% â†’ 94%ë¡œ ì§€ì† ìƒìŠ¹"

### ì¬ë°œ ë°©ì§€ ì œì•ˆ ê·œì¹™
- **ì„œë²„ íƒ€ì…ë³„ ë§ì¶¤ ì œì•ˆ**:
  - DB ì„œë²„: VACUUM ANALYZE, ì»¤ë„¥ì…˜ í’€ë§, ìŠ¬ë¡œìš° ì¿¼ë¦¬ ì ê²€
  - WAS ì„œë²„: JVM í™ ì ê²€, GC íŠœë‹, ìŠ¤ë ˆë“œ ë¤í”„
  - Cache ì„œë²„: ë©”ëª¨ë¦¬ ì •ì±…, TTL ê²€í† , eviction ëª¨ë‹ˆí„°ë§
- **êµ¬ì²´ì  ëª…ë ¹ì–´ í¬í•¨**: \`top -o %CPU\`, \`free -m\`, \`jmap -heap <PID>\`
- **ì„ê³„ê°’ ì¡°ì • ì œì•ˆ**: "CPU ê²½ë³´ 80% â†’ 75% í•˜í–¥ ê¶Œì¥"

### ë³´ê³ ì„œ ì‹ ë¢°ë„ ê¸°ì¤€
- ë©”íŠ¸ë¦­ 3ê°œ ì´ìƒ ì¸ìš© ì‹œ: ì‹ ë¢°ë„ +10%
- ì‹œê°„ ì¶”ì´ ë¶„ì„ í¬í•¨ ì‹œ: ì‹ ë¢°ë„ +15%
- CLI ëª…ë ¹ì–´ 2ê°œ ì´ìƒ ì œì•ˆ ì‹œ: ì‹ ë¢°ë„ +10%
`;

// ============================================================================
// 3. Supervisor Implementation
// ============================================================================

// Retry configuration
const RETRY_CONFIG = {
  maxRetries: 2,
  retryableErrors: ['RATE_LIMIT', 'TIMEOUT', 'MODEL_ERROR'],
  retryDelayMs: 1000,
};

// ============================================================================
// 3.5. Mode Selection Logic
// ============================================================================

/**
 * Determine execution mode based on query complexity
 * Complex queries benefit from multi-agent orchestration
 *
 * @updated 2026-01-09: Enhanced patterns for better Korean NLP coverage
 */
function selectExecutionMode(query: string): SupervisorMode {
  const q = query.toLowerCase();

  // Domain context keywords for gating broad patterns (Korean + English + typos)
  // Korean typos: ì„œë²„â†’ì„œë²¼/ì¹, ë©”ëª¨ë¦¬â†’ë©”ë¨¸ë¦¬/ë©¤, ì‹œìŠ¤í…œâ†’ì‹œìŠ¤íƒ¬
  // English typos: serverâ†’servr/sever, memoryâ†’memroy/memeory
  const infraContext =
    /ì„œë²„|ì„œë²¼|ì¹|ì¸í”„ë¼|ì‹œìŠ¤í…œ|ì‹œìŠ¤íƒ¬|ëª¨ë‹ˆí„°ë§|cpu|ì”¨í”¼ìœ |ë©”ëª¨ë¦¬|ë©”ë¨¸ë¦¬|ë©¤|ë””ìŠ¤í¬|íŠ¸ë˜í”½|ë„¤íŠ¸ì›Œí¬|server|servr|sever|memory|memroy|disk|traffic|network|latency|response|load/i;
  const hasInfraContext = infraContext.test(q);

  // Multi-agent indicators (complex queries requiring specialized agents)
  const multiAgentPatterns = [
    // 1. Report generation (ë³´ê³ ì„œ/ë¦¬í¬íŠ¸) - always multi-agent
    /ë³´ê³ ì„œ|ë¦¬í¬íŠ¸|report|ì¸ì‹œë˜íŠ¸|incident|ì¥ì• .*ë³´ê³ |ì¼ì¼.*ë¦¬í¬íŠ¸/i,

    // 2. Root cause analysis (ì›ì¸ ë¶„ì„) - with infra context
    /ë¶„ì„.*ì›ì¸|ì›ì¸.*ë¶„ì„|ê·¼ë³¸.*ì›ì¸|rca|root.*cause/i,

    // 3. Troubleshooting (ë¬¸ì œ í•´ê²°) - always multi-agent
    /í•´ê²°.*ë°©ë²•|ê³¼ê±°.*ì‚¬ë¡€|ìœ ì‚¬.*ì¥ì• |ì–´ë–»ê²Œ.*í•´ê²°|ì¡°ì¹˜.*ë°©ë²•|ëŒ€ì‘.*ë°©ì•ˆ/i,
    /how.*to.*(fix|resolve|solve)|troubleshoot|trubleshoot/i,

    // 4. Capacity planning (ìš©ëŸ‰ ê³„íš) - always multi-agent
    /ìš©ëŸ‰.*ê³„íš|capacity|ì–¸ì œ.*ë¶€ì¡±|ì–¼ë§ˆë‚˜.*ë‚¨|ì¦ì„¤.*í•„ìš”/i,

    // 5. Summary requests â†’ NLQ Agent (merged from Summarizer Agent)
    // Includes typos: ì„œë²„â†’ì„œë²¼/ì¹, ìš”ì•½â†’ìš”ë¨, serverâ†’servr/sever, summaryâ†’sumary/summry
    /(ì„œë²„|ì„œë²¼|ì¹|ìƒíƒœ|í˜„í™©|ëª¨ë‹ˆí„°ë§|ì¸í”„ë¼).*(ìš”ì•½|ìš”ë¨|ê°„ë‹¨íˆ|í•µì‹¬|tl;?dr)/i,
    /(ìš”ì•½|ìš”ë¨|ê°„ë‹¨íˆ|í•µì‹¬|tl;?dr).*(ì„œë²„|ì„œë²¼|ì¹|ìƒíƒœ|í˜„í™©|ì•Œë ¤|í•´ì¤˜)/i,
    /(server|servr|sever|status|monitoring).*(summary|sumary|summry|summarize|brief|overview)/i,
    /(summary|sumary|summry|summarize|overview).*(server|servr|sever|status|all)/i,

    // 6. Complex multi-server analysis (ë‹¤ì¤‘ ì„œë²„ ë¶„ì„)
    /ì „ì²´.*(ì„œë²„|ì„œë²¼|ì¹).*ë¶„ì„|ëª¨ë“ .*(ì„œë²„|ì„œë²¼|ì¹).*ìƒíƒœ|(ì„œë²„|ì„œë²¼|ì¹).*ì „ë°˜|ì¢…í•©.*ë¶„ì„/i,
    /all.*(server|servr|sever)s?.*status|overall.*status|system.*overview/i,
  ];

  // Patterns that require infrastructure context to avoid false positives
  const contextGatedPatterns = [
    // "ì™œ/Why" questions - only with infra/metric terms
    /ì™œ.*(ëŠë ¤|ë†’ì•„|ì´ìƒ|ìŠ¤íŒŒì´í¬|ì§€ì—°|ì˜¤ë¥˜|ê¸‰ì¦)/i,
    /why.*(high|slow|spik|error|increas|drop|fail)/i,
    /what.*caused|reason.*for/i,

    // Trend & prediction - requires infra context
    /ì˜ˆì¸¡|íŠ¸ë Œë“œ|ì¶”ì„¸|ì¶”ì´|ë³€í™”.*íŒ¨í„´/i,
    /predict|forecast|trend.*analysis/i,

    // Comparison - requires infra context
    /ì–´ì œ.*ëŒ€ë¹„|ì§€ë‚œ.*ì£¼.*ëŒ€ë¹„|ì „ì›”.*ëŒ€ë¹„|ì‘ë…„.*ë¹„êµ/i,
    /compared.*to.*(yesterday|last|previous)/i,

    // Correlation analysis - requires infra context
    /ìƒê´€ê´€ê³„|ì—°ê´€.*ë¶„ì„|correlat|ê°™ì´.*ì˜¬ë¼|í•¨ê»˜.*ì¦ê°€/i,

    // Anomaly deep dive
    /ì´ìƒ.*ì›ì¸|ë¹„ì •ìƒ.*ì´ìœ |ìŠ¤íŒŒì´í¬.*ì›ì¸|ê¸‰ì¦.*ì´ìœ /i,
  ];

  // Check for always-multi patterns (no context required)
  for (const pattern of multiAgentPatterns) {
    if (pattern.test(q)) {
      return 'multi';
    }
  }

  // Check for context-gated patterns (require infra context to avoid false positives)
  if (hasInfraContext) {
    for (const pattern of contextGatedPatterns) {
      if (pattern.test(q)) {
        return 'multi';
      }
    }
  }

  // Simple queries use single-agent for lower latency
  return 'single';
}

/**
 * Execute supervisor with AI SDK
 * Supports both single-agent and multi-agent modes
 */
export async function executeSupervisor(
  request: SupervisorRequest
): Promise<SupervisorResponse | SupervisorError> {
  const startTime = Date.now();

  // Determine execution mode
  let mode = request.mode || 'auto';
  if (mode === 'auto') {
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    mode = lastUserMessage ? selectExecutionMode(lastUserMessage.content) : 'single';
  }

  console.log(`ğŸ¯ [Supervisor] Mode: ${mode}`);

  // Execute based on mode
  if (mode === 'multi') {
    return executeMultiAgentMode(request, startTime);
  }

  return executeSingleAgentMode(request, startTime);
}

/**
 * Execute multi-agent mode with orchestrator
 */
async function executeMultiAgentMode(
  request: SupervisorRequest,
  startTime: number
): Promise<SupervisorResponse | SupervisorError> {
  try {
    const multiAgentRequest: MultiAgentRequest = {
      messages: request.messages,
      sessionId: request.sessionId,
      enableTracing: request.enableTracing,
    };

    const result = await executeMultiAgent(multiAgentRequest);

    if (!result.success) {
      return result as SupervisorError;
    }

    const multiResult = result as MultiAgentResponse;

    // ğŸ¯ P1-1: Record token usage for quota tracking (multi-agent mode)
    // Fix: Gemini review identified missing recordModelUsage in multi-agent path
    if (multiResult.usage.totalTokens > 0) {
      await recordModelUsage(
        multiResult.metadata.provider as import('./model-provider').ProviderName,
        multiResult.usage.totalTokens,
        'multi-agent'
      );
    }

    return {
      success: true,
      response: multiResult.response,
      toolsCalled: multiResult.toolsCalled,
      toolResults: [], // Multi-agent doesn't expose individual tool results
      usage: multiResult.usage,
      metadata: {
        provider: multiResult.metadata.provider,
        modelId: multiResult.metadata.modelId,
        stepsExecuted: multiResult.metadata.totalRounds,
        durationMs: multiResult.metadata.durationMs,
        mode: 'multi',
        handoffs: multiResult.handoffs,
        finalAgent: multiResult.finalAgent,
      },
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`âŒ [Supervisor] Multi-agent error after ${durationMs}ms:`, errorMessage);

    // Fallback to single-agent mode on error
    console.log(`ğŸ”„ [Supervisor] Falling back to single-agent mode`);
    return executeSingleAgentMode(request, startTime);
  }
}

/**
 * Execute single-agent mode with multi-step tool calling
 * Includes retry logic for transient errors with provider rotation
 */
async function executeSingleAgentMode(
  request: SupervisorRequest,
  startTime: number
): Promise<SupervisorResponse | SupervisorError> {
  let lastError: SupervisorError | null = null;
  const failedProviders: ProviderName[] = [];

  for (let attempt = 0; attempt <= RETRY_CONFIG.maxRetries; attempt++) {
    if (attempt > 0) {
      console.log(`ğŸ”„ [Supervisor] Retry attempt ${attempt}/${RETRY_CONFIG.maxRetries}, excluding: [${failedProviders.join(', ')}]`);
      await new Promise((r) => setTimeout(r, RETRY_CONFIG.retryDelayMs * attempt));
    }

    const result = await executeSupervisorAttempt(request, startTime, failedProviders);

    if (result.success) {
      // Add mode to metadata
      (result as SupervisorResponse).metadata.mode = 'single';
      return result;
    }

    lastError = result as SupervisorError;

    // Track failed provider for next retry (extract from error metadata if available)
    const failedProvider = (lastError as unknown as { provider?: ProviderName }).provider;
    if (failedProvider && !failedProviders.includes(failedProvider)) {
      failedProviders.push(failedProvider);
      console.log(`ğŸ“ [Supervisor] Marking ${failedProvider} as failed for retry`);
    }

    // Check if error is retryable
    if (!RETRY_CONFIG.retryableErrors.includes(lastError.code)) {
      console.log(`âŒ [Supervisor] Non-retryable error: ${lastError.code}`);
      return lastError;
    }
  }

  // All retries exhausted
  return lastError || { success: false, error: 'Unknown error', code: 'UNKNOWN_ERROR' };
}

/**
 * Single attempt of supervisor execution
 * Enhanced with Langfuse tracing, per-provider Circuit Breaker, and prepareStep optimization
 *
 * @param excludeProviders - Providers to skip (failed in previous attempts)
 */
async function executeSupervisorAttempt(
  request: SupervisorRequest,
  startTime: number,
  excludeProviders: ProviderName[] = []
): Promise<SupervisorResponse | (SupervisorError & { provider?: ProviderName })> {
  // Create Langfuse trace
  const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
  const trace = createSupervisorTrace({
    sessionId: request.sessionId,
    mode: 'single',
    query: lastUserMessage?.content || '',
  });

  // Get model with fallback chain (excluding failed providers)
  let provider: ProviderName;
  let modelId: string;
  let model;

  try {
    const modelResult = getSupervisorModel(excludeProviders);
    model = modelResult.model;
    provider = modelResult.provider;
    modelId = modelResult.modelId;
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error('âŒ [Supervisor] No available providers:', errorMessage);
    return {
      success: false,
      error: errorMessage,
      code: 'NO_PROVIDER',
    };
  }

  // Get provider-specific circuit breaker (not generic 'supervisor')
  const circuitBreaker = getCircuitBreaker(`supervisor-${provider}`);

  // Check circuit breaker for this specific provider
  if (!circuitBreaker.isAllowed()) {
    console.log(`âš¡ [Supervisor] Circuit OPEN for ${provider}, will try next provider on retry`);
    return {
      success: false,
      error: `Provider ${provider} circuit breaker is OPEN`,
      code: 'CIRCUIT_OPEN',
      provider, // Include provider for retry exclusion
    };
  }

  try {
    // Execute with provider-specific circuit breaker
    return await circuitBreaker.execute(async () => {
      // Log provider status on first call
      logProviderStatus();

      console.log(`ğŸ¤– [Supervisor] Using ${provider}/${modelId}`);

      const queryText = lastUserMessage?.content || '';
      const intentCategory = getIntentCategory(queryText);

      // Convert messages to ModelMessage format (AI SDK 6)
      const modelMessages: ModelMessage[] = [
        { role: 'system', content: SYSTEM_PROMPT },
        ...request.messages.map((m) => ({
          role: m.role as 'user' | 'assistant',
          content: m.content,
        })),
      ];

      // Execute with multi-step tool calling
      // AI SDK v6 Best Practice:
      // - prepareStep: Runtime tool filtering based on query intent
      // - hasToolCall('finalAnswer'): Graceful loop termination
      // - stepCountIs(3): Safety limit
      const result = await generateText({
        model,
        messages: modelMessages,
        tools: allTools,
        prepareStep: createPrepareStep(queryText),
        stopWhen: [hasToolCall('finalAnswer'), stepCountIs(3)],
        temperature: 0.4,
        maxOutputTokens: 1536,
      });

      // Extract tool call information with Langfuse logging
      const toolsCalled: string[] = [];
      const toolResults: Record<string, unknown>[] = [];

      // AI SDK v6 Best Practice: Extract finalAnswer response if called
      let finalAnswerResult: { answer: string } | null = null;

      for (const step of result.steps) {
        for (const toolCall of step.toolCalls) {
          toolsCalled.push(toolCall.toolName);
        }
        // toolResultsëŠ” step.toolResultsì—ì„œ ì¶”ì¶œ
        // AI SDK v6 í˜¸í™˜: ì—¬ëŸ¬ toolResult êµ¬ì¡° ëŒ€ì‘
        if (step.toolResults) {
          for (const tr of step.toolResults) {
            if ('result' in tr) {
              toolResults.push(tr.result as Record<string, unknown>);
              // Log tool call to Langfuse
              logToolCall(trace, tr.toolName, {}, tr.result, 0);
            }

            // Check for finalAnswer tool result (AI SDK v6 Best Practice)
            // Case 1: result í”„ë¡œí¼í‹°ì— answerê°€ ìˆëŠ” ê²½ìš°
            // Case 2: toolResult ìì²´ì— answerê°€ ìˆëŠ” ê²½ìš°
            if (tr.toolName === 'finalAnswer') {
              if ('result' in tr && tr.result && typeof tr.result === 'object') {
                const result = tr.result as Record<string, unknown>;
                if ('answer' in result && typeof result.answer === 'string') {
                  finalAnswerResult = { answer: result.answer };
                }
              } else if ('answer' in tr && typeof (tr as Record<string, unknown>).answer === 'string') {
                finalAnswerResult = { answer: (tr as Record<string, unknown>).answer as string };
              }
            }
          }
        }
      }

      // Use finalAnswer if called, otherwise fall back to result.text
      const response = finalAnswerResult?.answer ?? result.text;

      const durationMs = Date.now() - startTime;

      // Log generation to Langfuse
      logGeneration(trace, {
        model: modelId,
        provider,
        input: lastUserMessage?.content || '',
        output: response,
        usage: {
          inputTokens: result.usage?.inputTokens ?? 0,
          outputTokens: result.usage?.outputTokens ?? 0,
          totalTokens: result.usage?.totalTokens ?? 0,
        },
        duration: durationMs,
        metadata: { intent: intentCategory, usedFinalAnswer: !!finalAnswerResult, usedPrepareStep: true },
      });

      // Finalize trace
      finalizeTrace(trace, response, true, {
        toolsCalled,
        stepsExecuted: result.steps.length,
        durationMs,
        intent: intentCategory,
      });

      console.log(
        `âœ… [Supervisor] Completed in ${durationMs}ms, tools: [${toolsCalled.join(', ')}]${finalAnswerResult ? ' (via finalAnswer)' : ''}`
      );

      // ğŸ¯ P1-1: Record token usage for quota tracking (pre-emptive fallback)
      const totalTokens = result.usage?.totalTokens ?? 0;
      if (totalTokens > 0) {
        await recordModelUsage(provider, totalTokens, 'supervisor');
      }

      return {
        success: true,
        response,
        toolsCalled,
        toolResults,
        usage: {
          promptTokens: result.usage?.inputTokens ?? 0,
          completionTokens: result.usage?.outputTokens ?? 0,
          totalTokens,
        },
        metadata: {
          provider,
          modelId,
          stepsExecuted: result.steps.length,
          durationMs,
        },
      };
    });
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`âŒ [Supervisor] Error after ${durationMs}ms:`, errorMessage);

    // Finalize trace with error
    finalizeTrace(trace, errorMessage, false, { durationMs });

    // Classify error
    let code = 'UNKNOWN_ERROR';
    if (error instanceof CircuitOpenError) {
      code = 'CIRCUIT_OPEN';
    } else if (errorMessage.includes('API key')) {
      code = 'AUTH_ERROR';
    } else if (errorMessage.includes('rate limit')) {
      code = 'RATE_LIMIT';
    } else if (errorMessage.includes('timeout')) {
      code = 'TIMEOUT';
    } else if (errorMessage.includes('model')) {
      code = 'MODEL_ERROR';
    }

    return {
      success: false,
      error: errorMessage,
      code,
      provider, // Include provider for retry exclusion
    };
  }
}

// ============================================================================
// 4. Streaming Supervisor (Real-time Implementation)
// ============================================================================

export type StreamEventType =
  | 'tool_call'
  | 'tool_result'
  | 'text_delta'
  | 'step_finish'
  | 'handoff'       // NEW: ì—ì´ì „íŠ¸ í•¸ë“œì˜¤í”„ ì´ë²¤íŠ¸
  | 'agent_status'  // NEW: ì—ì´ì „íŠ¸ ìƒíƒœ ë³€ê²½
  | 'warning'       // NEW: ì²˜ë¦¬ ì§€ì—° ê²½ê³  (25ì´ˆ ì´ˆê³¼ ì‹œ)
  | 'done'
  | 'error';

export interface StreamEvent {
  type: StreamEventType;
  data: unknown;
}

/**
 * Execute supervisor with real-time streaming response
 * Uses AI SDK streamText for token-by-token streaming
 *
 * @example
 * for await (const event of executeSupervisorStream(request)) {
 *   if (event.type === 'text_delta') {
 *     process.stdout.write(event.data as string);
 *   }
 * }
 */
export async function* executeSupervisorStream(
  request: SupervisorRequest
): AsyncGenerator<StreamEvent> {
  const startTime = Date.now();

  // Determine execution mode
  let mode = request.mode || 'auto';
  if (mode === 'auto') {
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    mode = lastUserMessage ? selectExecutionMode(lastUserMessage.content) : 'single';
  }

  console.log(`ğŸ¯ [SupervisorStream] Mode: ${mode}`);

  // Multi-agent mode with real streaming
  if (mode === 'multi') {
    // Use real streaming from orchestrator
    yield* executeMultiAgentStream({
      messages: request.messages,
      sessionId: request.sessionId,
      enableTracing: request.enableTracing,
    });
    return;
  }

  // Single-agent streaming mode
  yield* streamSingleAgent(request, startTime);
}

/**
 * Stream single-agent mode with real streamText
 * Uses provider-specific circuit breaker for isolation
 */
async function* streamSingleAgent(
  request: SupervisorRequest,
  startTime: number
): AsyncGenerator<StreamEvent> {
  // Get model first to determine provider
  let provider: ProviderName;
  let modelId: string;
  let model;

  try {
    logProviderStatus();
    const modelResult = getSupervisorModel();
    model = modelResult.model;
    provider = modelResult.provider;
    modelId = modelResult.modelId;
  } catch (error) {
    yield {
      type: 'error',
      data: { code: 'NO_PROVIDER', message: error instanceof Error ? error.message : String(error) },
    };
    return;
  }

  // Get provider-specific circuit breaker
  const circuitBreaker = getCircuitBreaker(`stream-${provider}`);

  // Check circuit breaker for this specific provider
  if (!circuitBreaker.isAllowed()) {
    yield {
      type: 'error',
      data: { code: 'CIRCUIT_OPEN', message: `Provider ${provider} circuit breaker is OPEN` },
    };
    return;
  }

  try {

    console.log(`ğŸ¤– [SupervisorStream] Using ${provider}/${modelId}`);

    // Create Langfuse trace
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    const queryText = lastUserMessage?.content || '';
    const trace = createSupervisorTrace({
      sessionId: request.sessionId,
      mode: 'single',
      query: queryText,
    });

    // Convert messages to ModelMessage format
    const modelMessages: ModelMessage[] = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...request.messages.map((m) => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    ];

    const toolsCalled: string[] = [];
    let fullText = '';

    // ğŸ¯ P2 Fix: Track stream errors for warning emission
    let streamError: Error | null = null;

    // ğŸ¯ P0 Fix: AbortController for proper stream cancellation on timeout
    const abortController = new AbortController();

    // Execute streamText with multi-step tool calling
    // AI SDK v6 Best Practice:
    // - prepareStep: Runtime tool filtering based on query intent
    // - hasToolCall('finalAnswer'): Graceful loop termination
    // - stepCountIs(3): Safety limit
    // - abortSignal: Proper cancellation support
    const result = streamText({
      model,
      messages: modelMessages,
      tools: allTools,
      prepareStep: createPrepareStep(queryText),
      stopWhen: [hasToolCall('finalAnswer'), stepCountIs(3)],
      temperature: 0.4,
      maxOutputTokens: 1536,
      abortSignal: abortController.signal,
      // ğŸ¯ Phase 3: AI SDK v6 ê¶Œì¥ - onError ì½œë°± ì¶”ê°€
      // ìŠ¤íŠ¸ë¦¼ ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê¹… ë° ì¶”ì  (Cloud Runì—ì„œ ë””ë²„ê¹…ìš©)
      onError: ({ error }) => {
        // Ignore abort errors - they're expected on timeout
        if (error instanceof Error && error.name === 'AbortError') return;
        console.error('âŒ [SingleAgent] streamText error:', {
          error: error instanceof Error ? error.message : String(error),
          model: modelId,
          provider,
          query: queryText.substring(0, 100),
        });
        // ğŸ¯ P2 Fix: Track error for later warning emission
        streamError = error instanceof Error ? error : new Error(String(error));
      },
    });

    // ğŸ¯ P2-1: Centralized timeout configuration
    // Uses TIMEOUT_CONFIG.supervisor for consistent timeout management
    const SINGLE_AGENT_HARD_TIMEOUT = TIMEOUT_CONFIG.supervisor.hard;
    const TIMEOUT_WARNING_THRESHOLD = TIMEOUT_CONFIG.supervisor.warning;
    let warningEmitted = false;

    // Stream text deltas with hard timeout check
    for await (const textPart of result.textStream) {
      const elapsed = Date.now() - startTime;

      // âš ï¸ AI SDK v6 Best Practice: Emit warning before hard timeout
      // Gives user time to understand response may be incomplete
      if (!warningEmitted && elapsed >= TIMEOUT_WARNING_THRESHOLD) {
        warningEmitted = true;
        console.warn(`âš ï¸ [SingleAgent] Approaching timeout at ${elapsed}ms`);
        yield {
          type: 'warning',
          data: {
            code: 'SLOW_PROCESSING',
            message: 'ì‘ë‹µ ìƒì„±ì´ ì§€ì—°ë˜ê³  ìˆìŠµë‹ˆë‹¤. ê³§ ì™„ë£Œë©ë‹ˆë‹¤.',
            elapsed,
            threshold: TIMEOUT_WARNING_THRESHOLD,
          },
        };
      }

      // ğŸ”¥ Hard timeout: Graceful degradation with partial response
      if (elapsed >= SINGLE_AGENT_HARD_TIMEOUT) {
        console.error(
          `ğŸ›‘ [SingleAgent] Hard timeout reached at ${elapsed}ms (limit: ${SINGLE_AGENT_HARD_TIMEOUT}ms)`
        );

        // ğŸ¯ AI SDK v6 Best Practice: Graceful timeout with partial response summary
        // Emit what we have so far before error, improving UX
        if (fullText.length > 0) {
          yield {
            type: 'text_delta',
            data: '\n\n---\nâ±ï¸ *ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ë¡œ ì—¬ê¸°ê¹Œì§€ë§Œ ì „ë‹¬ë©ë‹ˆë‹¤.*',
          };
        }

        yield {
          type: 'error',
          data: {
            code: 'HARD_TIMEOUT',
            error: `ì²˜ë¦¬ ì‹œê°„ì´ ${SINGLE_AGENT_HARD_TIMEOUT / 1000}ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.`,
            elapsed,
            // Provide more context for debugging/retry
            partialResponseLength: fullText.length,
            suggestion: fullText.length > 0
              ? 'ë¶€ë¶„ ì‘ë‹µì´ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ë©´ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”.'
              : 'ì¿¼ë¦¬ë¥¼ ê°„ë‹¨í•˜ê²Œ ë‚˜ëˆ ì„œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
          },
        };

        // ğŸ¯ P0 Fix: Abort stream to prevent resource leak
        // AbortController properly signals streamText to stop underlying fetch/processing
        abortController.abort();

        return;
      }

      fullText += textPart;
      yield { type: 'text_delta', data: textPart };
    }

    // ğŸ¯ P2 Fix: Emit warning if stream error occurred but stream completed
    if (streamError !== null) {
      yield {
        type: 'warning',
        data: {
          code: 'STREAM_ERROR_OCCURRED',
          message: (streamError as Error).message,
        },
      };
    }

    // Await promises for final data
    const [steps, usage] = await Promise.all([result.steps, result.usage]);

    // Emit step finish events and collect tool calls
    for (const step of steps) {
      for (const toolCall of step.toolCalls) {
        const toolName = toolCall.toolName;
        toolsCalled.push(toolName);
        yield { type: 'tool_call', data: { name: toolName } };
      }
      if (step.toolResults) {
        for (const tr of step.toolResults) {
          if ('result' in tr) {
            yield { type: 'tool_result', data: { toolName: tr.toolName, result: tr.result } };
            // Log tool call to Langfuse
            logToolCall(trace, tr.toolName, {}, tr.result, 0);
          }
        }
      }
    }

    const durationMs = Date.now() - startTime;

    // Log generation to Langfuse
    logGeneration(trace, {
      model: modelId,
      provider,
      input: lastUserMessage?.content || '',
      output: fullText,
      usage: {
        inputTokens: usage?.inputTokens ?? 0,
        outputTokens: usage?.outputTokens ?? 0,
        totalTokens: usage?.totalTokens ?? 0,
      },
      duration: durationMs,
    });

    // ğŸ¯ CODEX Review R3 Fix: streamError ë°œìƒ ì‹œ tracingì—ë„ success=false ë°˜ì˜
    const capturedError = streamError as Error | null;
    const streamSucceeded = capturedError === null;
    finalizeTrace(trace, fullText, streamSucceeded, {
      toolsCalled,
      stepsExecuted: steps.length,
      durationMs,
      ...(capturedError && { error: capturedError.message }),
    });

    console.log(
      `âœ… [SupervisorStream] Completed in ${durationMs}ms, tools: [${toolsCalled.join(', ')}]`
    );

    // ğŸ¯ P1-1: Record token usage for quota tracking (pre-emptive fallback)
    const totalTokensUsed = usage?.totalTokens ?? 0;
    if (totalTokensUsed > 0) {
      await recordModelUsage(provider, totalTokensUsed, 'supervisor-stream');
    }

    // ğŸ¯ CODEX Review Fix: streamError ë°œìƒ ì‹œ success=false ë°˜ì˜
    yield {
      type: 'done',
      data: {
        success: capturedError === null,
        toolsCalled,
        usage: {
          promptTokens: usage?.inputTokens ?? 0,
          completionTokens: usage?.outputTokens ?? 0,
          totalTokens: totalTokensUsed,
        },
        metadata: {
          provider,
          modelId,
          stepsExecuted: steps.length,
          durationMs,
          mode: 'single',
        },
        // Include warning info if stream error occurred
        ...(capturedError && {
          warning: {
            code: 'STREAM_ERROR_OCCURRED',
            message: capturedError.message,
          },
        }),
      },
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`âŒ [SupervisorStream] Error after ${durationMs}ms:`, errorMessage);

    yield {
      type: 'error',
      data: {
        code: error instanceof CircuitOpenError ? 'CIRCUIT_OPEN' : 'STREAM_ERROR',
        message: errorMessage,
      },
    };
  }
}

// ============================================================================
// 5. prepareStep for Runtime Tool Filtering (AI SDK v6 Best Practice)
// ============================================================================

// ============================================================================
// ğŸ¯ SSOT: Unified Routing Patterns (AI SDK v6 Best Practice)
// Single source of truth for both intent classification and tool filtering
// Compiled once at module load for ~3x perf improvement
// ============================================================================

/**
 * Intent categories for query classification
 * Used for logging, tracing, and tool filtering
 */
type IntentCategory = 'anomaly' | 'prediction' | 'rca' | 'advisor' | 'serverGroup' | 'metrics' | 'general';

/**
 * Pre-compiled regex patterns for routing
 * Order: specific to general (AI SDK v6 Best Practice)
 */
const TOOL_ROUTING_PATTERNS = {
  /** ì´ìƒ íƒì§€: ê¸‰ì¦/ê¸‰ê°/ìŠ¤íŒŒì´í¬/anomaly ë“± (Analyst) */
  anomaly: /ì´ìƒ|ê¸‰ì¦|ê¸‰ê°|ìŠ¤íŒŒì´í¬|anomal|íƒì§€|ê°ì§€|ë¹„ì •ìƒ/i,
  /** ì˜ˆì¸¡/íŠ¸ë Œë“œ ë¶„ì„ (Analyst) */
  prediction: /ì˜ˆì¸¡|íŠ¸ë Œë“œ|ì¶”ì´|ì „ë§|forecast|ì¶”ì„¸/i,
  /** RCA: ê·¼ë³¸ ì›ì¸ ë¶„ì„, ì¥ì• , ì¸ì‹œë˜íŠ¸ */
  rca: /ì¥ì• |rca|íƒ€ì„ë¼ì¸|ìƒê´€ê´€ê³„|ì›ì¸|ì™œ|ê·¼ë³¸|incident/i,
  /** Advisor: í•´ê²° ë°©ë²•, ëª…ë ¹ì–´, ê³¼ê±° ì‚¬ë¡€ */
  advisor: /í•´ê²°|ë°©ë²•|ëª…ë ¹ì–´|ê°€ì´ë“œ|ì´ë ¥|ê³¼ê±°|ì‚¬ë¡€|ê²€ìƒ‰/i,
  /** Server Group: DB/Web/Cache/LB ë“± */
  serverGroup: /(db|web|cache|lb|api|storage|ë¡œë“œ\s*ë°¸ëŸ°ì„œ|ìºì‹œ|ìŠ¤í† ë¦¬ì§€)\s*(ì„œë²„)?/i,
  /** Metrics: ê¸°ë³¸ ì„œë²„/CPU/ë©”ëª¨ë¦¬ ìƒíƒœ (NLQ) */
  metrics: /cpu|ë©”ëª¨ë¦¬|ë””ìŠ¤í¬|ì„œë²„|ìƒíƒœ|memory|disk/i,
} as const;

/**
 * Classify query intent using unified patterns (SSOT)
 * AI SDK v6 Best Practice: Same patterns used for both logging and tool filtering
 *
 * @param query User's query text
 * @returns Intent category for logging and tracing
 */
function getIntentCategory(query: string): IntentCategory {
  const q = query.toLowerCase();

  // Order: specific to general (matches createPrepareStep order)
  if (TOOL_ROUTING_PATTERNS.anomaly.test(q)) return 'anomaly';
  if (TOOL_ROUTING_PATTERNS.prediction.test(q)) return 'prediction';
  if (TOOL_ROUTING_PATTERNS.rca.test(q)) return 'rca';
  if (TOOL_ROUTING_PATTERNS.advisor.test(q)) return 'advisor';
  if (TOOL_ROUTING_PATTERNS.serverGroup.test(q)) return 'serverGroup';
  if (TOOL_ROUTING_PATTERNS.metrics.test(q)) return 'metrics';
  return 'general';
}

/**
 * Create prepareStep function for runtime tool filtering
 * AI SDK v6 Best Practice: Filter tools dynamically based on query intent
 *
 * Benefits:
 * - Reduces token usage by limiting tool descriptions sent to LLM
 * - First step uses filtered tools, subsequent steps allow all tools
 * - More flexible than static pre-filtering
 *
 * @param query User's query text
 * @returns PrepareStep function for generateText/streamText
 */
function createPrepareStep(query: string) {
  const q = query.toLowerCase();

  return async ({ stepNumber }: { stepNumber: number }) => {
    // After first step, allow all tools for flexibility
    if (stepNumber > 0) return {};

    // AI SDK v6 Best Practice: Order patterns from specific to general
    // ìš°ì„ ìˆœìœ„: ëª…ì‹œì  ì´ìƒíƒì§€ â†’ ì˜ˆì¸¡/íŠ¸ë Œë“œ â†’ RCA â†’ Reporter â†’ Server Group â†’ Default

    // 1. ëª…ì‹œì  ì´ìƒíƒì§€ ìš”ì²­ (ìµœìš°ì„  - Analyst)
    if (TOOL_ROUTING_PATTERNS.anomaly.test(q)) {
      return {
        activeTools: ['detectAnomalies', 'predictTrends', 'analyzePattern', 'getServerMetrics', 'finalAnswer'] as ToolName[]
      };
    }

    // 2. ì˜ˆì¸¡/íŠ¸ë Œë“œ ë¶„ì„ ìš”ì²­ (Analyst)
    if (TOOL_ROUTING_PATTERNS.prediction.test(q)) {
      return {
        activeTools: ['predictTrends', 'analyzePattern', 'detectAnomalies', 'correlateMetrics', 'finalAnswer'] as ToolName[]
      };
    }

    // 3. RCA: ê·¼ë³¸ ì›ì¸ ë¶„ì„ (ì¥ì• , ì›ì¸, ì™œ ë“±)
    if (TOOL_ROUTING_PATTERNS.rca.test(q)) {
      return {
        activeTools: ['findRootCause', 'buildIncidentTimeline', 'correlateMetrics', 'getServerMetrics', 'detectAnomalies', 'finalAnswer'] as ToolName[]
      };
    }

    // 4. Reporter/Advisor: RAG + commands
    if (TOOL_ROUTING_PATTERNS.advisor.test(q)) {
      return {
        activeTools: ['searchKnowledgeBase', 'recommendCommands', 'searchWeb', 'finalAnswer'] as ToolName[]
      };
    }

    // 5. Server group queries
    if (TOOL_ROUTING_PATTERNS.serverGroup.test(q)) {
      return {
        activeTools: ['getServerByGroup', 'getServerByGroupAdvanced', 'filterServers', 'finalAnswer'] as ToolName[]
      };
    }

    // 6. Default: NLQ metrics tools
    return {
      activeTools: ['getServerMetrics', 'getServerMetricsAdvanced', 'filterServers', 'getServerByGroup', 'finalAnswer'] as ToolName[]
    };
  };
}

// ============================================================================
// 6. Health Check
// ============================================================================

export interface SupervisorHealth {
  status: 'ok' | 'degraded' | 'error';
  provider: string;
  modelId: string;
  toolsAvailable: number;
}

/**
 * Check supervisor health
 */
export async function checkSupervisorHealth(): Promise<SupervisorHealth> {
  try {
    const { provider, modelId } = getSupervisorModel();
    const toolCount = Object.keys(allTools).length;

    return {
      status: 'ok',
      provider,
      modelId,
      toolsAvailable: toolCount,
    };
  } catch (error) {
    return {
      status: 'error',
      provider: 'none',
      modelId: 'none',
      toolsAvailable: 0,
    };
  }
}

// ============================================================================
// 7. AI SDK Native UIMessageStream (Best Practice)
// ============================================================================

/**
 * Create AI SDK Native UIMessageStream Response
 *
 * Uses AI SDK v6 `createUIMessageStream` for native streaming that works
 * directly with `useChat` on the frontend without custom transport.
 *
 * Benefits:
 * - Native AI SDK protocol (text, data, source, tool-call events)
 * - Direct integration with useChat (no TextStreamChatTransport needed)
 * - Structured data events (handoffs, tool calls, metadata)
 * - Better error handling and retry support
 *
 * @param request - Supervisor request with messages and session info
 * @returns Response object with UIMessageStream
 */
export function createSupervisorStreamResponse(
  request: SupervisorRequest
): Response {
  console.log(`ğŸŒŠ [UIMessageStream] Creating native stream for session: ${request.sessionId}`);

  const stream = createUIMessageStream({
    execute: async ({ writer }) => {
      const startTime = Date.now();
      // ğŸ¯ P2-2 Fix: Cryptographically secure nonce using AI SDK generateId()
      // Replaces Math.random() for collision-free message IDs
      const nonce = generateId();

      // AI SDK v6 Best Practice: Use consistent message ID for all text deltas within same message block
      // CODEX Review: Different IDs per delta causes client rendering issues
      // Multi-agent: Increment sequence on handoff to create separate message blocks
      let messageSeq = 0;
      let currentMessageId = `assistant-${request.sessionId}-${startTime}-${nonce}-${messageSeq}`;

      // ğŸ›¡ï¸ UIMessageStream Protocol: Track text-start/text-end lifecycle
      // text-start MUST precede text-delta, text-end MUST follow text-start
      let textPartStarted = false;

      try {
        // Emit start event with session info
        // Use 'data-start' type for custom data (AI SDK requires 'data-${string}' pattern)
        writer.write({
          type: 'data-start',
          data: {
            sessionId: request.sessionId,
            timestamp: new Date().toISOString(),
          },
        });

        // Determine execution mode
        let mode: SupervisorMode = request.mode || 'auto';
        if (mode === 'auto') {
          const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
          mode = lastUserMessage ? selectExecutionMode(lastUserMessage.content) : 'single';
        }

        writer.write({
          type: 'data-mode',
          data: { mode },
        });

        // Execute and stream
        for await (const event of executeSupervisorStream({ ...request, mode })) {
          switch (event.type) {
            case 'text_delta':
              // ğŸ›¡ï¸ UIMessageStream Protocol: text-start MUST precede text-delta
              if (!textPartStarted) {
                writer.write({
                  type: 'text-start',
                  id: currentMessageId,
                });
                textPartStarted = true;
              }

              // Native text streaming with consistent message ID within same block
              writer.write({
                type: 'text-delta',
                delta: event.data as string,
                id: currentMessageId, // Use same ID for all deltas to merge into single message
              });
              break;

            case 'handoff':
              // ğŸ›¡ï¸ UIMessageStream Protocol: Close previous text part before handoff
              if (textPartStarted) {
                writer.write({
                  type: 'text-end',
                  id: currentMessageId,
                });
                textPartStarted = false;
              }

              // AI SDK v6 Best Practice: Create new message ID on handoff
              // This separates different agent responses into distinct UI messages
              messageSeq += 1;
              currentMessageId = `assistant-${request.sessionId}-${startTime}-${nonce}-${messageSeq}`;

              // Structured handoff event
              writer.write({
                type: 'data-handoff',
                data: event.data as object,
              });
              break;

            case 'tool_call':
              // Tool call notification
              writer.write({
                type: 'data-tool-call',
                data: event.data as object,
              });
              break;

            case 'tool_result':
              // Tool result (optional, can be verbose)
              writer.write({
                type: 'data-tool-result',
                data: event.data as object,
              });
              break;

            case 'warning':
              // Processing warning (e.g., slow processing)
              writer.write({
                type: 'data-warning',
                data: event.data as object,
              });
              break;

            case 'agent_status':
              // Agent status change
              writer.write({
                type: 'data-agent-status',
                data: event.data as object,
              });
              break;

            case 'done':
              // ğŸ›¡ï¸ UIMessageStream Protocol: Close text part before done
              if (textPartStarted) {
                writer.write({
                  type: 'text-end',
                  id: currentMessageId,
                });
                textPartStarted = false;
              }

              // Completion metadata
              // AI SDK v6 Best Practice: Passthrough success status from upstream with type safety
              const doneData = event.data as Record<string, unknown>;
              // Type guard: only accept boolean success values, default to true otherwise
              const upstreamSuccess = doneData.success;
              const success = typeof upstreamSuccess === 'boolean' ? upstreamSuccess : true;

              writer.write({
                type: 'data-done',
                data: {
                  durationMs: Date.now() - startTime,
                  ...doneData,
                  success,
                },
              });
              break;

            case 'error':
              // ğŸ›¡ï¸ UIMessageStream Protocol: Close text part before error
              if (textPartStarted) {
                writer.write({
                  type: 'text-end',
                  id: currentMessageId,
                });
                textPartStarted = false;
              }

              // Error event
              const errorData = event.data as Record<string, unknown>;
              writer.write({
                type: 'error',
                errorText: (errorData.error as string) ?? (errorData.message as string) ?? 'Unknown error',
              });
              break;

            default:
              // Pass through unknown events as data
              writer.write({
                type: `data-${event.type}` as `data-${string}`,
                data: typeof event.data === 'object' ? event.data as object : { value: event.data },
              });
          }
        }
      } catch (error) {
        const errorMessage = error instanceof Error ? error.message : String(error);
        console.error(`âŒ [UIMessageStream] Error:`, errorMessage);

        // ğŸ›¡ï¸ UIMessageStream Protocol: Close text part before error in catch
        if (textPartStarted) {
          writer.write({
            type: 'text-end',
            id: currentMessageId,
          });
        }

        writer.write({
          type: 'error',
          errorText: errorMessage,
        });
      }
    },
  });

  return createUIMessageStreamResponse({
    stream,
    headers: {
      'X-Session-Id': request.sessionId,
      'X-Stream-Protocol': 'ui-message-stream',
    },
  });
}
