/**
 * AI SDK Supervisor
 *
 * Dual-mode supervisor implementation:
 * 1. Single-agent mode: Simple generateText with multi-step tool calling
 * 2. Multi-agent mode: Orchestrated agent handoffs using @ai-sdk-tools/agents
 *
 * Architecture:
 * - Single-agent: One LLM with all tools (simple queries)
 * - Multi-agent: Orchestrator â†’ NLQ/Analyst/Reporter/Advisor (complex queries)
 *
 * @version 2.0.0
 * @updated 2025-12-28
 */

import { generateText, streamText, stepCountIs, type ModelMessage } from 'ai';
import { getSupervisorModel, logProviderStatus, type ProviderName } from './model-provider';
import { allTools, toolDescriptions, type ToolName } from '../../tools-ai-sdk';
import { executeMultiAgent, executeMultiAgentStream, type MultiAgentRequest, type MultiAgentResponse } from './agents';
import {
  createSupervisorTrace,
  logGeneration,
  logToolCall,
  finalizeTrace,
  type TraceMetadata,
} from '../observability/langfuse';
import { getCircuitBreaker, CircuitOpenError } from '../resilience/circuit-breaker';

// ============================================================================
// 1. Types
// ============================================================================

export type SupervisorMode = 'single' | 'multi' | 'auto';

export interface SupervisorRequest {
  messages: Array<{ role: 'user' | 'assistant'; content: string }>;
  sessionId: string;
  enableTracing?: boolean;
  /**
   * Execution mode:
   * - 'single': Use single-agent with multi-step tool calling (default)
   * - 'multi': Use multi-agent orchestration with handoffs
   * - 'auto': Automatically select based on query complexity
   */
  mode?: SupervisorMode;
}

export interface SupervisorResponse {
  success: boolean;
  response: string;
  toolsCalled: string[];
  toolResults: Record<string, unknown>[];
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  metadata: {
    provider: string;
    modelId: string;
    stepsExecuted: number;
    durationMs: number;
    mode?: SupervisorMode;
    handoffs?: Array<{ from: string; to: string; reason?: string }>;
    finalAgent?: string;
  };
}

export interface SupervisorError {
  success: false;
  error: string;
  code: string;
}

// ============================================================================
// 2. System Prompt
// ============================================================================

const SYSTEM_PROMPT = `ë‹¹ì‹ ì€ ì„œë²„ ëª¨ë‹ˆí„°ë§ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

## í•µì‹¬ ì›ì¹™: ìš”ì•½ ìš°ì„  (Summary First)

**í•­ìƒ í•µì‹¬ ê²°ë¡ ì„ ë¨¼ì € 1-2ë¬¸ì¥ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.**
- ì „ì²´ ëª©ë¡ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”
- ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì„¸ìš”
- ì‚¬ìš©ìê°€ "ìì„¸íˆ", "ëª©ë¡", "ì „ë¶€", "ëª¨ë‘"ë¥¼ ìš”ì²­í•˜ë©´ ìƒì„¸ ì œê³µ

### ì¢‹ì€ ì‘ë‹µ ì˜ˆì‹œ
âŒ ë‚˜ì¨: "ì„œë²„ 15ëŒ€ì˜ ìƒíƒœì…ë‹ˆë‹¤. ì„œë²„1: CPU 35%... ì„œë²„2: CPU 40%... (ì „ì²´ ë‚˜ì—´)"
âœ… ì¢‹ìŒ: "ì´ìƒ ì„œë²„ 8ëŒ€ ë°œê²¬ (ê²½ê³  7ëŒ€, ì„ê³„ 1ëŒ€). ê°€ì¥ ì‹¬ê°: backup-server-01 (ë””ìŠ¤í¬ 91%)"

### ìƒì„¸ ìš”ì²­ ê°ì§€
- "ìì„¸íˆ ì•Œë ¤ì¤˜" â†’ ì „ì²´ ëª©ë¡ ì œê³µ
- "ì–´ë–¤ ì„œë²„ì•¼?" â†’ í•´ë‹¹ ì„œë²„ë“¤ ë‚˜ì—´
- "ì™œ?" â†’ ì›ì¸ ìƒì„¸ ì„¤ëª…

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

### ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒ
- getServerMetrics: ì„œë²„ **í˜„ì¬** ìƒíƒœ ì¡°íšŒ (CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬)
- getServerMetricsAdvanced: **ì‹œê°„ ë²”ìœ„ ì§‘ê³„** (ì§€ë‚œ 1/6/24ì‹œê°„ í‰ê· /ìµœëŒ€/ìµœì†Œ)
  - serverId ìƒëµ ì‹œ ì „ì²´ ì„œë²„ ì¡°íšŒ, globalSummaryì— ì „ì²´ í‰ê·  í¬í•¨
  - ì˜ˆ: { timeRange: "last6h", metric: "cpu", aggregation: "avg" }
- filterServers: ì¡°ê±´ì— ë§ëŠ” ì„œë²„ í•„í„°ë§ (ì˜ˆ: CPU 80% ì´ìƒ)

### ì¥ì•  ë¶„ì„ (RCA)
- buildIncidentTimeline: ì¥ì•  íƒ€ì„ë¼ì¸ êµ¬ì„±
- correlateMetrics: ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
- findRootCause: ê·¼ë³¸ ì›ì¸ ë¶„ì„

### ì´ìƒ íƒì§€ & ì˜ˆì¸¡
- detectAnomalies: ì´ìƒì¹˜ íƒì§€ (6ì‹œê°„ ì´ë™í‰ê·  + 2Ïƒ)
- predictTrends: íŠ¸ë Œë“œ ì˜ˆì¸¡ (ì„ í˜• íšŒê·€ ê¸°ë°˜)
- analyzePattern: íŒ¨í„´ ë¶„ì„

### ì§€ì‹ë² ì´ìŠ¤ & ê¶Œì¥ ì¡°ì¹˜ (GraphRAG)
- searchKnowledgeBase: ê³¼ê±° ì¥ì•  ì´ë ¥ ë° í•´ê²° ë°©ë²• ê²€ìƒ‰ (Vector + Graph)
- recommendCommands: ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ CLI ëª…ë ¹ì–´ ì¶”ì²œ

## ì‘ë‹µ ì§€ì¹¨

1. **ìš”ì•½ ìš°ì„ **: í•µì‹¬ ê²°ë¡  1-2ë¬¸ì¥ ë¨¼ì €
2. **í•µì‹¬ë§Œ ì¶”ì¶œ**: ê°€ì¥ ì‹¬ê°í•œ 1-3ê°œë§Œ ì–¸ê¸‰
3. **ìˆ˜ì¹˜ëŠ” ê°„ê²°í•˜ê²Œ**: "CPU 85.3%" â†’ "CPU 85%"
4. **í•œêµ­ì–´ë¡œ ì‘ë‹µ / Respond in Korean** (í•œì ì ˆëŒ€ ê¸ˆì§€ / No Chinese characters, ê¸°ìˆ ìš©ì–´ëŠ” ì˜ì–´ í—ˆìš© / Technical terms in English OK)
5. **ì´ìƒ ê°ì§€ ì‹œ ê¶Œì¥ ì¡°ì¹˜ ì œì•ˆ**
6. **ì¥ì•  ë¬¸ì˜ ì‹œ searchKnowledgeBase í™œìš©**

## globalSummary ì‘ë‹µ ê·œì¹™
getServerMetricsAdvanced ê²°ê³¼ì— globalSummaryê°€ ìˆìœ¼ë©´ **ë°˜ë“œì‹œ í•´ë‹¹ ê°’ì„ ì¸ìš©**:
- cpu_avg â†’ "ì „ì²´ ì„œë²„ CPU í‰ê· "
- cpu_max â†’ "ì „ì²´ ì„œë²„ CPU ìµœëŒ€ê°’"
- cpu_min â†’ "ì „ì²´ ì„œë²„ CPU ìµœì†Œê°’"

ì˜ˆ: globalSummary.cpu_avg = 34 â†’ "ì§€ë‚œ 6ì‹œê°„ ì „ì²´ ì„œë²„ CPU í‰ê· ì€ 34%ì…ë‹ˆë‹¤."

## ì˜ˆì‹œ ì§ˆë¬¸ê³¼ ë„êµ¬ ë§¤í•‘

- "CPU 80% ì´ìƒì¸ ì„œë²„ ì•Œë ¤ì¤˜" â†’ filterServers(field: "cpu", operator: ">", value: 80)
- "ì„œë²„ ìƒíƒœ ìš”ì•½í•´ì¤˜" â†’ getServerMetrics()
- "ì§€ë‚œ 6ì‹œê°„ CPU í‰ê·  ì•Œë ¤ì¤˜" â†’ getServerMetricsAdvanced(timeRange: "last6h", metric: "cpu", aggregation: "avg")
  â†’ ì‘ë‹µì˜ globalSummary.cpu_avg ê°’ì´ ì „ì²´ ì„œë²„ í‰ê· 
- "ìµœê·¼ 1ì‹œê°„ ë©”ëª¨ë¦¬ ìµœëŒ€ê°’" â†’ getServerMetricsAdvanced(timeRange: "last1h", metric: "memory", aggregation: "max")
- "ë©”ëª¨ë¦¬ ì¶”ì„¸ ë¶„ì„í•´ì¤˜" â†’ predictTrends(metricType: "memory")
- "ì¥ì•  ì›ì¸ ë¶„ì„í•´ì¤˜" â†’ findRootCause() + buildIncidentTimeline()
- "ë©”ëª¨ë¦¬ ë¶€ì¡± í•´ê²° ë°©ë²•" â†’ searchKnowledgeBase(query: "ë©”ëª¨ë¦¬ ë¶€ì¡±")
- "ë””ìŠ¤í¬ ì •ë¦¬ ëª…ë ¹ì–´" â†’ recommendCommands(keywords: ["ë””ìŠ¤í¬", "ì •ë¦¬"])
`;

// ============================================================================
// 3. Supervisor Implementation
// ============================================================================

// Retry configuration
const RETRY_CONFIG = {
  maxRetries: 2,
  retryableErrors: ['RATE_LIMIT', 'TIMEOUT', 'MODEL_ERROR'],
  retryDelayMs: 1000,
};

// ============================================================================
// 3.5. Mode Selection Logic
// ============================================================================

/**
 * Determine execution mode based on query complexity
 * Complex queries benefit from multi-agent orchestration
 *
 * @updated 2026-01-09: Enhanced patterns for better Korean NLP coverage
 */
function selectExecutionMode(query: string): SupervisorMode {
  const q = query.toLowerCase();

  // Domain context keywords for gating broad patterns (Korean + English + typos)
  // Korean typos: ì„œë²„â†’ì„œë²¼/ì¹, ë©”ëª¨ë¦¬â†’ë©”ë¨¸ë¦¬/ë©¤, ì‹œìŠ¤í…œâ†’ì‹œìŠ¤íƒ¬
  // English typos: serverâ†’servr/sever, memoryâ†’memroy/memeory
  const infraContext =
    /ì„œë²„|ì„œë²¼|ì¹|ì¸í”„ë¼|ì‹œìŠ¤í…œ|ì‹œìŠ¤íƒ¬|ëª¨ë‹ˆí„°ë§|cpu|ì”¨í”¼ìœ |ë©”ëª¨ë¦¬|ë©”ë¨¸ë¦¬|ë©¤|ë””ìŠ¤í¬|íŠ¸ë˜í”½|ë„¤íŠ¸ì›Œí¬|server|servr|sever|memory|memroy|disk|traffic|network|latency|response|load/i;
  const hasInfraContext = infraContext.test(q);

  // Multi-agent indicators (complex queries requiring specialized agents)
  const multiAgentPatterns = [
    // 1. Report generation (ë³´ê³ ì„œ/ë¦¬í¬íŠ¸) - always multi-agent
    /ë³´ê³ ì„œ|ë¦¬í¬íŠ¸|report|ì¸ì‹œë˜íŠ¸|incident|ì¥ì• .*ë³´ê³ |ì¼ì¼.*ë¦¬í¬íŠ¸/i,

    // 2. Root cause analysis (ì›ì¸ ë¶„ì„) - with infra context
    /ë¶„ì„.*ì›ì¸|ì›ì¸.*ë¶„ì„|ê·¼ë³¸.*ì›ì¸|rca|root.*cause/i,

    // 3. Troubleshooting (ë¬¸ì œ í•´ê²°) - always multi-agent
    /í•´ê²°.*ë°©ë²•|ê³¼ê±°.*ì‚¬ë¡€|ìœ ì‚¬.*ì¥ì• |ì–´ë–»ê²Œ.*í•´ê²°|ì¡°ì¹˜.*ë°©ë²•|ëŒ€ì‘.*ë°©ì•ˆ/i,
    /how.*to.*(fix|resolve|solve)|troubleshoot|trubleshoot/i,

    // 4. Capacity planning (ìš©ëŸ‰ ê³„íš) - always multi-agent
    /ìš©ëŸ‰.*ê³„íš|capacity|ì–¸ì œ.*ë¶€ì¡±|ì–¼ë§ˆë‚˜.*ë‚¨|ì¦ì„¤.*í•„ìš”/i,

    // 5. Summary requests â†’ NLQ Agent (merged from Summarizer Agent)
    // Includes typos: ì„œë²„â†’ì„œë²¼/ì¹, ìš”ì•½â†’ìš”ë¨, serverâ†’servr/sever, summaryâ†’sumary/summry
    /(ì„œë²„|ì„œë²¼|ì¹|ìƒíƒœ|í˜„í™©|ëª¨ë‹ˆí„°ë§|ì¸í”„ë¼).*(ìš”ì•½|ìš”ë¨|ê°„ë‹¨íˆ|í•µì‹¬|tl;?dr)/i,
    /(ìš”ì•½|ìš”ë¨|ê°„ë‹¨íˆ|í•µì‹¬|tl;?dr).*(ì„œë²„|ì„œë²¼|ì¹|ìƒíƒœ|í˜„í™©|ì•Œë ¤|í•´ì¤˜)/i,
    /(server|servr|sever|status|monitoring).*(summary|sumary|summry|summarize|brief|overview)/i,
    /(summary|sumary|summry|summarize|overview).*(server|servr|sever|status|all)/i,

    // 6. Complex multi-server analysis (ë‹¤ì¤‘ ì„œë²„ ë¶„ì„)
    /ì „ì²´.*(ì„œë²„|ì„œë²¼|ì¹).*ë¶„ì„|ëª¨ë“ .*(ì„œë²„|ì„œë²¼|ì¹).*ìƒíƒœ|(ì„œë²„|ì„œë²¼|ì¹).*ì „ë°˜|ì¢…í•©.*ë¶„ì„/i,
    /all.*(server|servr|sever)s?.*status|overall.*status|system.*overview/i,
  ];

  // Patterns that require infrastructure context to avoid false positives
  const contextGatedPatterns = [
    // "ì™œ/Why" questions - only with infra/metric terms
    /ì™œ.*(ëŠë ¤|ë†’ì•„|ì´ìƒ|ìŠ¤íŒŒì´í¬|ì§€ì—°|ì˜¤ë¥˜|ê¸‰ì¦)/i,
    /why.*(high|slow|spik|error|increas|drop|fail)/i,
    /what.*caused|reason.*for/i,

    // Trend & prediction - requires infra context
    /ì˜ˆì¸¡|íŠ¸ë Œë“œ|ì¶”ì„¸|ì¶”ì´|ë³€í™”.*íŒ¨í„´/i,
    /predict|forecast|trend.*analysis/i,

    // Comparison - requires infra context
    /ì–´ì œ.*ëŒ€ë¹„|ì§€ë‚œ.*ì£¼.*ëŒ€ë¹„|ì „ì›”.*ëŒ€ë¹„|ì‘ë…„.*ë¹„êµ/i,
    /compared.*to.*(yesterday|last|previous)/i,

    // Correlation analysis - requires infra context
    /ìƒê´€ê´€ê³„|ì—°ê´€.*ë¶„ì„|correlat|ê°™ì´.*ì˜¬ë¼|í•¨ê»˜.*ì¦ê°€/i,

    // Anomaly deep dive
    /ì´ìƒ.*ì›ì¸|ë¹„ì •ìƒ.*ì´ìœ |ìŠ¤íŒŒì´í¬.*ì›ì¸|ê¸‰ì¦.*ì´ìœ /i,
  ];

  // Check for always-multi patterns (no context required)
  for (const pattern of multiAgentPatterns) {
    if (pattern.test(q)) {
      return 'multi';
    }
  }

  // Check for context-gated patterns (require infra context to avoid false positives)
  if (hasInfraContext) {
    for (const pattern of contextGatedPatterns) {
      if (pattern.test(q)) {
        return 'multi';
      }
    }
  }

  // Simple queries use single-agent for lower latency
  return 'single';
}

/**
 * Execute supervisor with AI SDK
 * Supports both single-agent and multi-agent modes
 */
export async function executeSupervisor(
  request: SupervisorRequest
): Promise<SupervisorResponse | SupervisorError> {
  const startTime = Date.now();

  // Determine execution mode
  let mode = request.mode || 'auto';
  if (mode === 'auto') {
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    mode = lastUserMessage ? selectExecutionMode(lastUserMessage.content) : 'single';
  }

  console.log(`ğŸ¯ [Supervisor] Mode: ${mode}`);

  // Execute based on mode
  if (mode === 'multi') {
    return executeMultiAgentMode(request, startTime);
  }

  return executeSingleAgentMode(request, startTime);
}

/**
 * Execute multi-agent mode with orchestrator
 */
async function executeMultiAgentMode(
  request: SupervisorRequest,
  startTime: number
): Promise<SupervisorResponse | SupervisorError> {
  try {
    const multiAgentRequest: MultiAgentRequest = {
      messages: request.messages,
      sessionId: request.sessionId,
      enableTracing: request.enableTracing,
    };

    const result = await executeMultiAgent(multiAgentRequest);

    if (!result.success) {
      return result as SupervisorError;
    }

    const multiResult = result as MultiAgentResponse;

    return {
      success: true,
      response: multiResult.response,
      toolsCalled: multiResult.toolsCalled,
      toolResults: [], // Multi-agent doesn't expose individual tool results
      usage: multiResult.usage,
      metadata: {
        provider: multiResult.metadata.provider,
        modelId: multiResult.metadata.modelId,
        stepsExecuted: multiResult.metadata.totalRounds,
        durationMs: multiResult.metadata.durationMs,
        mode: 'multi',
        handoffs: multiResult.handoffs,
        finalAgent: multiResult.finalAgent,
      },
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`âŒ [Supervisor] Multi-agent error after ${durationMs}ms:`, errorMessage);

    // Fallback to single-agent mode on error
    console.log(`ğŸ”„ [Supervisor] Falling back to single-agent mode`);
    return executeSingleAgentMode(request, startTime);
  }
}

/**
 * Execute single-agent mode with multi-step tool calling
 * Includes retry logic for transient errors with provider rotation
 */
async function executeSingleAgentMode(
  request: SupervisorRequest,
  startTime: number
): Promise<SupervisorResponse | SupervisorError> {
  let lastError: SupervisorError | null = null;
  const failedProviders: ProviderName[] = [];

  for (let attempt = 0; attempt <= RETRY_CONFIG.maxRetries; attempt++) {
    if (attempt > 0) {
      console.log(`ğŸ”„ [Supervisor] Retry attempt ${attempt}/${RETRY_CONFIG.maxRetries}, excluding: [${failedProviders.join(', ')}]`);
      await new Promise((r) => setTimeout(r, RETRY_CONFIG.retryDelayMs * attempt));
    }

    const result = await executeSupervisorAttempt(request, startTime, failedProviders);

    if (result.success) {
      // Add mode to metadata
      (result as SupervisorResponse).metadata.mode = 'single';
      return result;
    }

    lastError = result as SupervisorError;

    // Track failed provider for next retry (extract from error metadata if available)
    const failedProvider = (lastError as unknown as { provider?: ProviderName }).provider;
    if (failedProvider && !failedProviders.includes(failedProvider)) {
      failedProviders.push(failedProvider);
      console.log(`ğŸ“ [Supervisor] Marking ${failedProvider} as failed for retry`);
    }

    // Check if error is retryable
    if (!RETRY_CONFIG.retryableErrors.includes(lastError.code)) {
      console.log(`âŒ [Supervisor] Non-retryable error: ${lastError.code}`);
      return lastError;
    }
  }

  // All retries exhausted
  return lastError || { success: false, error: 'Unknown error', code: 'UNKNOWN_ERROR' };
}

/**
 * Single attempt of supervisor execution
 * Enhanced with Langfuse tracing, per-provider Circuit Breaker, and prepareStep optimization
 *
 * @param excludeProviders - Providers to skip (failed in previous attempts)
 */
async function executeSupervisorAttempt(
  request: SupervisorRequest,
  startTime: number,
  excludeProviders: ProviderName[] = []
): Promise<SupervisorResponse | (SupervisorError & { provider?: ProviderName })> {
  // Create Langfuse trace
  const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
  const trace = createSupervisorTrace({
    sessionId: request.sessionId,
    mode: 'single',
    query: lastUserMessage?.content || '',
  });

  // Get model with fallback chain (excluding failed providers)
  let provider: ProviderName;
  let modelId: string;
  let model;

  try {
    const modelResult = getSupervisorModel(excludeProviders);
    model = modelResult.model;
    provider = modelResult.provider;
    modelId = modelResult.modelId;
  } catch (error) {
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error('âŒ [Supervisor] No available providers:', errorMessage);
    return {
      success: false,
      error: errorMessage,
      code: 'NO_PROVIDER',
    };
  }

  // Get provider-specific circuit breaker (not generic 'supervisor')
  const circuitBreaker = getCircuitBreaker(`supervisor-${provider}`);

  // Check circuit breaker for this specific provider
  if (!circuitBreaker.isAllowed()) {
    console.log(`âš¡ [Supervisor] Circuit OPEN for ${provider}, will try next provider on retry`);
    return {
      success: false,
      error: `Provider ${provider} circuit breaker is OPEN`,
      code: 'CIRCUIT_OPEN',
      provider, // Include provider for retry exclusion
    };
  }

  try {
    // Execute with provider-specific circuit breaker
    return await circuitBreaker.execute(async () => {
      // Log provider status on first call
      logProviderStatus();

      console.log(`ğŸ¤– [Supervisor] Using ${provider}/${modelId}`);

      // Classify intent for tool optimization
      const intent = classifyIntent(lastUserMessage?.content || '');

      // Convert messages to ModelMessage format (AI SDK 6)
      const modelMessages: ModelMessage[] = [
        { role: 'system', content: SYSTEM_PROMPT },
        ...request.messages.map((m) => ({
          role: m.role as 'user' | 'assistant',
          content: m.content,
        })),
      ];

      // Execute with multi-step tool calling and prepareStep optimization
      // Reduced step count and tokens for faster responses (Vercel 60s limit)
      const result = await generateText({
        model,
        messages: modelMessages,
        tools: allTools,
        stopWhen: stepCountIs(3), // Reduced from 5 to 3 for faster responses
        temperature: 0.2,
        maxOutputTokens: 1536, // Reduced from 2048 for faster responses
        // Note: prepareStep optimization moved to intent classification
        // AI SDK v6 uses different approach - tools are filtered upfront
      });

      // Extract tool call information with Langfuse logging
      const toolsCalled: string[] = [];
      const toolResults: Record<string, unknown>[] = [];

      for (const step of result.steps) {
        for (const toolCall of step.toolCalls) {
          toolsCalled.push(toolCall.toolName);
        }
        // toolResultsëŠ” step.toolResultsì—ì„œ ì¶”ì¶œ
        if (step.toolResults) {
          for (const tr of step.toolResults) {
            if ('result' in tr) {
              toolResults.push(tr.result as Record<string, unknown>);
              // Log tool call to Langfuse
              logToolCall(trace, tr.toolName, {}, tr.result, 0);
            }
          }
        }
      }

      const durationMs = Date.now() - startTime;

      // Log generation to Langfuse
      logGeneration(trace, {
        model: modelId,
        provider,
        input: lastUserMessage?.content || '',
        output: result.text,
        usage: {
          inputTokens: result.usage?.inputTokens ?? 0,
          outputTokens: result.usage?.outputTokens ?? 0,
          totalTokens: result.usage?.totalTokens ?? 0,
        },
        duration: durationMs,
        metadata: { intent: intent.category, intentConfidence: intent.confidence },
      });

      // Finalize trace
      finalizeTrace(trace, result.text, true, {
        toolsCalled,
        stepsExecuted: result.steps.length,
        durationMs,
        intent: intent.category,
      });

      console.log(
        `âœ… [Supervisor] Completed in ${durationMs}ms, tools: [${toolsCalled.join(', ')}]`
      );

      return {
        success: true,
        response: result.text,
        toolsCalled,
        toolResults,
        usage: {
          promptTokens: result.usage?.inputTokens ?? 0,
          completionTokens: result.usage?.outputTokens ?? 0,
          totalTokens: result.usage?.totalTokens ?? 0,
        },
        metadata: {
          provider,
          modelId,
          stepsExecuted: result.steps.length,
          durationMs,
        },
      };
    });
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`âŒ [Supervisor] Error after ${durationMs}ms:`, errorMessage);

    // Finalize trace with error
    finalizeTrace(trace, errorMessage, false, { durationMs });

    // Classify error
    let code = 'UNKNOWN_ERROR';
    if (error instanceof CircuitOpenError) {
      code = 'CIRCUIT_OPEN';
    } else if (errorMessage.includes('API key')) {
      code = 'AUTH_ERROR';
    } else if (errorMessage.includes('rate limit')) {
      code = 'RATE_LIMIT';
    } else if (errorMessage.includes('timeout')) {
      code = 'TIMEOUT';
    } else if (errorMessage.includes('model')) {
      code = 'MODEL_ERROR';
    }

    return {
      success: false,
      error: errorMessage,
      code,
      provider, // Include provider for retry exclusion
    };
  }
}

// ============================================================================
// 4. Streaming Supervisor (Real-time Implementation)
// ============================================================================

export type StreamEventType =
  | 'tool_call'
  | 'tool_result'
  | 'text_delta'
  | 'step_finish'
  | 'handoff'       // NEW: ì—ì´ì „íŠ¸ í•¸ë“œì˜¤í”„ ì´ë²¤íŠ¸
  | 'agent_status'  // NEW: ì—ì´ì „íŠ¸ ìƒíƒœ ë³€ê²½
  | 'done'
  | 'error';

export interface StreamEvent {
  type: StreamEventType;
  data: unknown;
}

/**
 * Execute supervisor with real-time streaming response
 * Uses AI SDK streamText for token-by-token streaming
 *
 * @example
 * for await (const event of executeSupervisorStream(request)) {
 *   if (event.type === 'text_delta') {
 *     process.stdout.write(event.data as string);
 *   }
 * }
 */
export async function* executeSupervisorStream(
  request: SupervisorRequest
): AsyncGenerator<StreamEvent> {
  const startTime = Date.now();

  // Determine execution mode
  let mode = request.mode || 'auto';
  if (mode === 'auto') {
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    mode = lastUserMessage ? selectExecutionMode(lastUserMessage.content) : 'single';
  }

  console.log(`ğŸ¯ [SupervisorStream] Mode: ${mode}`);

  // Multi-agent mode with real streaming
  if (mode === 'multi') {
    // Use real streaming from orchestrator
    yield* executeMultiAgentStream({
      messages: request.messages,
      sessionId: request.sessionId,
      enableTracing: request.enableTracing,
    });
    return;
  }

  // Single-agent streaming mode
  yield* streamSingleAgent(request, startTime);
}

/**
 * Stream single-agent mode with real streamText
 * Uses provider-specific circuit breaker for isolation
 */
async function* streamSingleAgent(
  request: SupervisorRequest,
  startTime: number
): AsyncGenerator<StreamEvent> {
  // Get model first to determine provider
  let provider: ProviderName;
  let modelId: string;
  let model;

  try {
    logProviderStatus();
    const modelResult = getSupervisorModel();
    model = modelResult.model;
    provider = modelResult.provider;
    modelId = modelResult.modelId;
  } catch (error) {
    yield {
      type: 'error',
      data: { code: 'NO_PROVIDER', message: error instanceof Error ? error.message : String(error) },
    };
    return;
  }

  // Get provider-specific circuit breaker
  const circuitBreaker = getCircuitBreaker(`stream-${provider}`);

  // Check circuit breaker for this specific provider
  if (!circuitBreaker.isAllowed()) {
    yield {
      type: 'error',
      data: { code: 'CIRCUIT_OPEN', message: `Provider ${provider} circuit breaker is OPEN` },
    };
    return;
  }

  try {

    console.log(`ğŸ¤– [SupervisorStream] Using ${provider}/${modelId}`);

    // Create Langfuse trace
    const lastUserMessage = request.messages.filter((m) => m.role === 'user').pop();
    const trace = createSupervisorTrace({
      sessionId: request.sessionId,
      mode: 'single',
      query: lastUserMessage?.content || '',
    });

    // Convert messages to ModelMessage format
    const modelMessages: ModelMessage[] = [
      { role: 'system', content: SYSTEM_PROMPT },
      ...request.messages.map((m) => ({
        role: m.role as 'user' | 'assistant',
        content: m.content,
      })),
    ];

    const toolsCalled: string[] = [];
    let fullText = '';

    // Execute streamText with multi-step tool calling
    // Reduced step count and tokens for faster responses (Vercel 60s limit)
    const result = streamText({
      model,
      messages: modelMessages,
      tools: allTools,
      stopWhen: stepCountIs(3), // Reduced from 5 to 3 for faster responses
      temperature: 0.2,
      maxOutputTokens: 1536, // Reduced from 2048 for faster responses
    });

    // Stream text deltas
    for await (const textPart of result.textStream) {
      fullText += textPart;
      yield { type: 'text_delta', data: textPart };
    }

    // Await promises for final data
    const [steps, usage] = await Promise.all([result.steps, result.usage]);

    // Emit step finish events and collect tool calls
    for (const step of steps) {
      for (const toolCall of step.toolCalls) {
        const toolName = toolCall.toolName;
        toolsCalled.push(toolName);
        yield { type: 'tool_call', data: { name: toolName } };
      }
      if (step.toolResults) {
        for (const tr of step.toolResults) {
          if ('result' in tr) {
            yield { type: 'tool_result', data: { toolName: tr.toolName, result: tr.result } };
            // Log tool call to Langfuse
            logToolCall(trace, tr.toolName, {}, tr.result, 0);
          }
        }
      }
    }

    const durationMs = Date.now() - startTime;

    // Log generation to Langfuse
    logGeneration(trace, {
      model: modelId,
      provider,
      input: lastUserMessage?.content || '',
      output: fullText,
      usage: {
        inputTokens: usage?.inputTokens ?? 0,
        outputTokens: usage?.outputTokens ?? 0,
        totalTokens: usage?.totalTokens ?? 0,
      },
      duration: durationMs,
    });

    // Finalize trace
    finalizeTrace(trace, fullText, true, {
      toolsCalled,
      stepsExecuted: steps.length,
      durationMs,
    });

    console.log(
      `âœ… [SupervisorStream] Completed in ${durationMs}ms, tools: [${toolsCalled.join(', ')}]`
    );

    // Emit done event
    yield {
      type: 'done',
      data: {
        success: true,
        toolsCalled,
        usage: {
          promptTokens: usage?.inputTokens ?? 0,
          completionTokens: usage?.outputTokens ?? 0,
          totalTokens: usage?.totalTokens ?? 0,
        },
        metadata: {
          provider,
          modelId,
          stepsExecuted: steps.length,
          durationMs,
          mode: 'single',
        },
      },
    };
  } catch (error) {
    const durationMs = Date.now() - startTime;
    const errorMessage = error instanceof Error ? error.message : String(error);

    console.error(`âŒ [SupervisorStream] Error after ${durationMs}ms:`, errorMessage);

    yield {
      type: 'error',
      data: {
        code: error instanceof CircuitOpenError ? 'CIRCUIT_OPEN' : 'STREAM_ERROR',
        message: errorMessage,
      },
    };
  }
}

/**
 * Fallback: Convert non-streaming result to stream events
 */
async function* streamFromNonStreaming(
  request: SupervisorRequest,
  startTime: number
): AsyncGenerator<StreamEvent> {
  const result = await executeSupervisor(request);

  if (!result.success) {
    yield {
      type: 'error',
      data: { code: (result as SupervisorError).code, message: (result as SupervisorError).error },
    };
    return;
  }

  const successResult = result as SupervisorResponse;

  // Emit tool calls
  for (const toolName of successResult.toolsCalled) {
    yield { type: 'tool_call', data: { name: toolName } };
  }

  // Emit tool results
  for (const toolResult of successResult.toolResults) {
    yield { type: 'tool_result', data: toolResult };
  }

  // Emit text as chunks (simulate streaming)
  const text = successResult.response;
  const chunkSize = 20; // Characters per chunk
  for (let i = 0; i < text.length; i += chunkSize) {
    yield { type: 'text_delta', data: text.slice(i, i + chunkSize) };
  }

  // Emit done event
  yield { type: 'done', data: successResult };
}

// ============================================================================
// 5. Intent Classification (Optional Enhancement)
// ============================================================================

type IntentCategory = 'metrics' | 'rca' | 'analyst' | 'reporter' | 'general';

interface ClassifiedIntent {
  category: IntentCategory;
  suggestedTools: ToolName[];
  confidence: number;
}

/**
 * Classify user intent (rule-based, for optimization)
 * This can reduce token usage by pre-filtering tools
 */
export function classifyIntent(query: string): ClassifiedIntent {
  const q = query.toLowerCase();

  // Reporter/RAG queries - knowledge base search, troubleshooting, commands
  if (/í•´ê²°.*ë°©ë²•|ë°©ë²•|ì¡°ì¹˜|ëª…ë ¹ì–´|command|ì´ë ¥|ê³¼ê±°.*ì‚¬ë¡€|ë¬¸ì œ.*í•´ê²°|ê°€ì´ë“œ/i.test(q)) {
    return {
      category: 'reporter',
      suggestedTools: ['searchKnowledgeBase', 'recommendCommands'],
      confidence: 0.9,
    };
  }

  // Server group/type queries (DB, ë¡œë“œë°¸ëŸ°ì„œ, ì›¹, ìºì‹œ ë“±)
  if (/(db|database|ë°ì´í„°ë² ì´ìŠ¤|ë””ë¹„)\s*(ì„œë²„|í˜„í™©|ìƒíƒœ|ëª©ë¡)/i.test(q) ||
      /(lb|loadbalancer|ë¡œë“œ\s*ë°¸ëŸ°ì„œ|ë¡œë“œë°¸ëŸ°ì„œ)\s*(ì„œë²„|í˜„í™©|ìƒíƒœ|ëª©ë¡)?/i.test(q) ||
      /(web|ì›¹)\s*(ì„œë²„|í˜„í™©|ìƒíƒœ|ëª©ë¡)/i.test(q) ||
      /(cache|ìºì‹œ|redis|ë ˆë””ìŠ¤)\s*(ì„œë²„|í˜„í™©|ìƒíƒœ|ëª©ë¡)/i.test(q) ||
      /(storage|ìŠ¤í† ë¦¬ì§€|ì €ì¥ì†Œ)\s*(ì„œë²„|í˜„í™©|ìƒíƒœ|ëª©ë¡)/i.test(q) ||
      /(api|app|application|ì• í”Œë¦¬ì¼€ì´ì…˜|ì•±)\s*(ì„œë²„|í˜„í™©|ìƒíƒœ|ëª©ë¡)/i.test(q)) {
    return {
      category: 'metrics',
      suggestedTools: ['getServerByGroup'],
      confidence: 0.95,
    };
  }

  // Metrics queries
  if (/cpu|ë©”ëª¨ë¦¬|memory|ë””ìŠ¤í¬|disk|ì„œë²„.*ìƒíƒœ|ìƒíƒœ.*ìš”ì•½/i.test(q)) {
    if (/\d+%.*ì´ìƒ|>\s*\d+|ì´ˆê³¼|ë†’ì€/i.test(q)) {
      return {
        category: 'metrics',
        suggestedTools: ['filterServers'],
        confidence: 0.9,
      };
    }
    return {
      category: 'metrics',
      suggestedTools: ['getServerMetrics', 'getServerMetricsAdvanced'],
      confidence: 0.85,
    };
  }

  // RCA queries
  if (/ì¥ì• |ì›ì¸|root.*cause|rca|íƒ€ì„ë¼ì¸|ìƒê´€ê´€ê³„/i.test(q)) {
    return {
      category: 'rca',
      suggestedTools: ['findRootCause', 'buildIncidentTimeline', 'correlateMetrics'],
      confidence: 0.9,
    };
  }

  // Analyst queries
  if (/ì´ìƒ|anomaly|íŠ¸ë Œë“œ|trend|ì˜ˆì¸¡|predict|íŒ¨í„´/i.test(q)) {
    return {
      category: 'analyst',
      suggestedTools: ['detectAnomalies', 'predictTrends', 'analyzePattern'],
      confidence: 0.85,
    };
  }

  // General - let LLM decide
  return {
    category: 'general',
    suggestedTools: ['getServerMetrics'],
    confidence: 0.5,
  };
}

// ============================================================================
// 6. Health Check
// ============================================================================

export interface SupervisorHealth {
  status: 'ok' | 'degraded' | 'error';
  provider: string;
  modelId: string;
  toolsAvailable: number;
}

/**
 * Check supervisor health
 */
export async function checkSupervisorHealth(): Promise<SupervisorHealth> {
  try {
    const { provider, modelId } = getSupervisorModel();
    const toolCount = Object.keys(allTools).length;

    return {
      status: 'ok',
      provider,
      modelId,
      toolsAvailable: toolCount,
    };
  } catch (error) {
    return {
      status: 'error',
      provider: 'none',
      modelId: 'none',
      toolsAvailable: 0,
    };
  }
}
