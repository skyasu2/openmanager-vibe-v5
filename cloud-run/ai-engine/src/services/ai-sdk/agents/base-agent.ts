/**
 * BaseAgent - Abstract base class for all agents
 *
 * Encapsulates the common agent execution pattern using AI SDK v6's
 * generateText with stopWhen conditions for graceful termination.
 *
 * Key Features:
 * - Unified execution interface via run() method
 * - AI SDK v6 native: stopWhen with hasToolCall('finalAnswer') + stepCountIs(N)
 * - Provider fallback chain support
 * - Step-by-step monitoring via onStepFinish
 * - Timeout protection with configurable limits
 *
 * @version 1.0.0
 * @created 2026-01-27
 */

import {
  generateText,
  streamText,
  hasToolCall,
  stepCountIs,
  type LanguageModel,
  type Tool,
  type TextPart,
  type ImagePart,
  type FilePart,
  type UserContent,
} from 'ai';
import { sanitizeChineseCharacters } from '../../../lib/text-sanitizer';
import type { AgentConfig, ModelResult } from './config';

// ============================================================================
// Types
// ============================================================================

/**
 * Image attachment for multimodal messages
 * Supports Base64, Data URL, HTTP(S) URL formats
 *
 * @see https://ai-sdk.dev/docs/ai-sdk-core/prompts#image-parts
 */
export interface ImageAttachment {
  /** Image data: Base64 string, Data URL, or HTTP(S) URL */
  data: string;
  /** MIME type (e.g., 'image/png', 'image/jpeg') */
  mimeType: string;
  /** Optional filename for display */
  name?: string;
}

/**
 * File attachment for multimodal messages
 * Supports PDF, audio, and other file types
 *
 * @see https://ai-sdk.dev/docs/ai-sdk-core/prompts#file-parts
 */
export interface FileAttachment {
  /** File data: Base64 string or HTTP(S) URL */
  data: string;
  /** MIME type (e.g., 'application/pdf', 'text/plain') */
  mimeType: string;
  /** Optional filename */
  name?: string;
}

/**
 * Result returned by agent execution
 */
export interface AgentResult {
  /** Generated text response */
  text: string;
  /** Whether execution was successful */
  success: boolean;
  /** Tools called during execution */
  toolsCalled: string[];
  /** Token usage statistics */
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  /** Execution metadata */
  metadata: {
    provider: string;
    modelId: string;
    durationMs: number;
    steps: number;
    finishReason?: string;
  };
  /** Error message if failed */
  error?: string;
}

/**
 * Configuration options for agent execution
 */
export interface AgentRunOptions {
  /** Maximum execution time in milliseconds */
  timeoutMs?: number;
  /** Maximum number of steps (LLM calls) */
  maxSteps?: number;
  /** Temperature for response generation */
  temperature?: number;
  /** Maximum output tokens */
  maxOutputTokens?: number;
  /** Enable web search tools */
  webSearchEnabled?: boolean;
  /** Session ID for context tracking */
  sessionId?: string;
  /**
   * Image attachments for multimodal queries (Vision Agent)
   * Images are passed directly to the model via message content
   * @see https://ai-sdk.dev/docs/ai-sdk-core/prompts#image-parts
   */
  images?: ImageAttachment[];
  /**
   * File attachments for multimodal queries (PDF, audio, etc.)
   * @see https://ai-sdk.dev/docs/ai-sdk-core/prompts#file-parts
   */
  files?: FileAttachment[];
}

/**
 * Stream event types for streaming execution
 */
export interface AgentStreamEvent {
  type: 'text_delta' | 'tool_call' | 'step_finish' | 'done' | 'error' | 'warning';
  data: unknown;
}

// ============================================================================
// Default Configuration
// ============================================================================

const DEFAULT_OPTIONS: Required<Omit<AgentRunOptions, 'sessionId' | 'images' | 'files'>> = {
  timeoutMs: 45_000,
  maxSteps: 5,
  temperature: 0.4,
  maxOutputTokens: 2048,
  webSearchEnabled: true,
};

// ============================================================================
// BaseAgent Abstract Class
// ============================================================================

/**
 * Abstract base class for all agents
 *
 * Subclasses must implement:
 * - getName(): Agent display name
 * - getConfig(): Get AgentConfig from AGENT_CONFIGS
 *
 * Provides:
 * - run(): Execute agent with query and return result
 * - stream(): Execute agent with streaming response
 * - isAvailable(): Check if agent has valid model
 */
export abstract class BaseAgent {
  /**
   * Get the agent's display name
   */
  abstract getName(): string;

  /**
   * Get the agent's configuration from AGENT_CONFIGS
   */
  abstract getConfig(): AgentConfig | null;

  /**
   * Check if agent is available (has valid model)
   */
  isAvailable(): boolean {
    const config = this.getConfig();
    if (!config) return false;
    return config.getModel() !== null;
  }

  /**
   * Get model result from config
   */
  protected getModel(): ModelResult | null {
    const config = this.getConfig();
    if (!config) return null;
    return config.getModel();
  }

  /**
   * Filter tools based on options
   */
  protected filterTools(
    tools: Record<string, Tool>,
    options: AgentRunOptions
  ): Record<string, Tool> {
    if (options.webSearchEnabled !== false) {
      return tools;
    }

    // Remove searchWeb tool when disabled
    const filtered = { ...tools };
    if ('searchWeb' in filtered) {
      delete filtered.searchWeb;
      console.log(`üö´ [${this.getName()}] searchWeb disabled`);
    }
    return filtered;
  }

  /**
   * Build multimodal user message content
   *
   * AI SDK v6 Best Practice: Include images/files directly in message content
   * @see https://ai-sdk.dev/docs/ai-sdk-core/prompts#image-parts
   *
   * @param query - Text query
   * @param options - Options with images/files
   * @returns Content array or string (for text-only)
   */
  protected buildUserContent(
    query: string,
    options: AgentRunOptions
  ): UserContent {
    const hasImages = options.images && options.images.length > 0;
    const hasFiles = options.files && options.files.length > 0;

    // Text-only: return simple string (most common case)
    if (!hasImages && !hasFiles) {
      return query;
    }

    // Multimodal: build content array with AI SDK-compatible types
    const content: Array<TextPart | ImagePart | FilePart> = [
      { type: 'text', text: query } as TextPart,
    ];

    // Add images (Vision Agent)
    if (hasImages) {
      for (const img of options.images!) {
        content.push({
          type: 'image',
          image: img.data,
          mimeType: img.mimeType,
        } as ImagePart);
      }
      console.log(`üì∑ [${this.getName()}] Added ${options.images!.length} image(s) to message`);
    }

    // Add files (PDF, audio, etc.)
    // Note: AI SDK FilePart uses 'mediaType' not 'mimeType'
    if (hasFiles) {
      for (const file of options.files!) {
        content.push({
          type: 'file',
          data: file.data,
          mediaType: file.mimeType, // AI SDK uses 'mediaType'
        } as FilePart);
      }
      console.log(`üìé [${this.getName()}] Added ${options.files!.length} file(s) to message`);
    }

    return content;
  }

  /**
   * Execute agent with query and return complete result
   *
   * Uses AI SDK v6 generateText with stopWhen conditions:
   * - hasToolCall('finalAnswer'): Graceful termination when agent calls finalAnswer
   * - stepCountIs(maxSteps): Safety limit to prevent infinite loops
   *
   * @param query - User query to process
   * @param options - Execution options
   * @returns AgentResult with response and metadata
   */
  async run(query: string, options: AgentRunOptions = {}): Promise<AgentResult> {
    const startTime = Date.now();
    const opts = { ...DEFAULT_OPTIONS, ...options };
    const agentName = this.getName();

    console.log(`ü§ñ [${agentName}] Starting execution...`);

    // Validate configuration
    const config = this.getConfig();
    if (!config) {
      return {
        text: '',
        success: false,
        toolsCalled: [],
        usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
        metadata: {
          provider: 'none',
          modelId: 'none',
          durationMs: Date.now() - startTime,
          steps: 0,
        },
        error: `Agent ${agentName} config not found`,
      };
    }

    const modelResult = config.getModel();
    if (!modelResult) {
      return {
        text: '',
        success: false,
        toolsCalled: [],
        usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
        metadata: {
          provider: 'none',
          modelId: 'none',
          durationMs: Date.now() - startTime,
          steps: 0,
        },
        error: `No model available for ${agentName}`,
      };
    }

    const { model, provider, modelId } = modelResult;
    const filteredTools = this.filterTools(config.tools as Record<string, Tool>, opts);

    console.log(`üéØ [${agentName}] Using ${provider}/${modelId}`);

    try {
      // Build multimodal user content (text + images + files)
      const userContent = this.buildUserContent(query, opts);

      // Execute with AI SDK v6 native pattern
      const result = await generateText({
        model,
        system: config.instructions,
        messages: [{ role: 'user', content: userContent }],
        tools: filteredTools,
        maxRetries: 1,
        // üéØ Fix: Apply timeout configuration (AI SDK v6.0.50)
        // Defensive: use default if undefined to avoid SDK validation errors
        timeout: { totalMs: opts.timeoutMs ?? DEFAULT_OPTIONS.timeoutMs },
        // AI SDK v6 Best Practice: Graceful termination conditions
        stopWhen: [hasToolCall('finalAnswer'), stepCountIs(opts.maxSteps)],
        temperature: opts.temperature,
        maxOutputTokens: opts.maxOutputTokens,
        // Step-by-step monitoring
        onStepFinish: ({ finishReason, toolCalls }) => {
          const toolNames = toolCalls?.map(tc => tc.toolName) || [];
          console.log(`üìç [${agentName}] Step: reason=${finishReason}, tools=[${toolNames.join(',')}]`);
        },
      });

      // Extract tool calls and check for finalAnswer
      const toolsCalled: string[] = [];
      let finalAnswerResult: { answer: string } | null = null;
      let finishReason = 'stop';

      for (const step of result.steps) {
        if (step.finishReason) {
          finishReason = step.finishReason;
        }
        for (const toolCall of step.toolCalls) {
          toolsCalled.push(toolCall.toolName);
        }
        // Extract finalAnswer result if called
        if (step.toolResults) {
          for (const tr of step.toolResults) {
            if ('result' in tr && tr.toolName === 'finalAnswer' && tr.result && typeof tr.result === 'object') {
              // üéØ Fix: Type guard for consistency with stream() method (Gemini review feedback)
              const result = tr.result as Record<string, unknown>;
              if ('answer' in result && typeof result.answer === 'string') {
                finalAnswerResult = { answer: result.answer };
              }
            }
          }
        }
      }

      // Use finalAnswer if called, otherwise fall back to result.text
      const responseText = finalAnswerResult?.answer ?? result.text;
      const sanitizedText = sanitizeChineseCharacters(responseText);

      const durationMs = Date.now() - startTime;
      console.log(`‚úÖ [${agentName}] Completed in ${durationMs}ms, tools: [${toolsCalled.join(', ')}]`);

      return {
        text: sanitizedText,
        success: true,
        toolsCalled,
        usage: {
          promptTokens: result.usage?.inputTokens ?? 0,
          completionTokens: result.usage?.outputTokens ?? 0,
          totalTokens: result.usage?.totalTokens ?? 0,
        },
        metadata: {
          provider,
          modelId,
          durationMs,
          steps: result.steps.length,
          finishReason,
        },
      };
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      const durationMs = Date.now() - startTime;

      console.error(`‚ùå [${agentName}] Error after ${durationMs}ms:`, errorMessage);

      return {
        text: '',
        success: false,
        toolsCalled: [],
        usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 },
        metadata: {
          provider,
          modelId,
          durationMs,
          steps: 0,
        },
        error: errorMessage,
      };
    }
  }

  /**
   * Execute agent with streaming response
   *
   * Yields AgentStreamEvent chunks in real-time for progressive UI updates.
   *
   * @param query - User query to process
   * @param options - Execution options
   * @yields AgentStreamEvent - Real-time streaming events
   */
  async *stream(query: string, options: AgentRunOptions = {}): AsyncGenerator<AgentStreamEvent> {
    const startTime = Date.now();
    const opts = { ...DEFAULT_OPTIONS, ...options };
    const agentName = this.getName();

    console.log(`ü§ñ [${agentName}] Starting stream...`);

    // Validate configuration
    const config = this.getConfig();
    if (!config) {
      yield { type: 'error', data: { code: 'CONFIG_NOT_FOUND', error: `Agent ${agentName} config not found` } };
      return;
    }

    const modelResult = config.getModel();
    if (!modelResult) {
      yield { type: 'error', data: { code: 'MODEL_UNAVAILABLE', error: `No model available for ${agentName}` } };
      return;
    }

    const { model, provider, modelId } = modelResult;
    const filteredTools = this.filterTools(config.tools as Record<string, Tool>, opts);

    console.log(`üéØ [${agentName}] Streaming with ${provider}/${modelId}`);

    try {
      // Build multimodal user content (text + images + files)
      const userContent = this.buildUserContent(query, opts);

      const streamResult = streamText({
        model,
        system: config.instructions,
        messages: [{ role: 'user', content: userContent }],
        tools: filteredTools,
        // üéØ Fix: Apply timeout configuration (AI SDK v6.0.50)
        // Defensive: use default if undefined to avoid SDK validation errors
        timeout: { totalMs: opts.timeoutMs ?? DEFAULT_OPTIONS.timeoutMs, chunkMs: 30_000 },
        stopWhen: [hasToolCall('finalAnswer'), stepCountIs(opts.maxSteps)],
        temperature: opts.temperature,
        maxOutputTokens: opts.maxOutputTokens,
        onStepFinish: ({ finishReason, toolCalls }) => {
          const toolNames = toolCalls?.map(tc => tc.toolName) || [];
          console.log(`üìç [${agentName}] Step: reason=${finishReason}, tools=[${toolNames.join(',')}]`);
        },
      });

      const toolsCalled: string[] = [];
      let hasTextContent = false;

      // Stream text deltas
      for await (const textChunk of streamResult.textStream) {
        const sanitized = sanitizeChineseCharacters(textChunk);
        // üéØ Fix: Only set hasTextContent for non-whitespace content (Codex review feedback)
        // This ensures finalAnswer fallback works when stream is empty/whitespace-only
        if (sanitized && sanitized.trim().length > 0) {
          hasTextContent = true;
          yield { type: 'text_delta', data: sanitized };
        }
      }

      // Gather metadata after streaming completes
      const [steps, usage] = await Promise.all([streamResult.steps, streamResult.usage]);

      // Extract tool calls and finalAnswer result
      let finalAnswerText: string | null = null;
      if (steps) {
        for (const step of steps) {
          if (step.toolCalls) {
            for (const tc of step.toolCalls) {
              toolsCalled.push(tc.toolName);
              yield { type: 'tool_call', data: { name: tc.toolName } };
            }
          }
          // üéØ Fix: Extract finalAnswer from toolResults (Codex review feedback)
          if (step.toolResults) {
            for (const tr of step.toolResults) {
              if ('result' in tr && tr.toolName === 'finalAnswer' && tr.result && typeof tr.result === 'object') {
                const result = tr.result as Record<string, unknown>;
                // üéØ Fix: Type guard to ensure answer is a string (Codex review feedback)
                if ('answer' in result && typeof result.answer === 'string') {
                  finalAnswerText = result.answer;
                }
              }
            }
          }
        }
      }

      // üéØ Fix: If no text was streamed but finalAnswer exists, emit it
      if (!hasTextContent && finalAnswerText) {
        const sanitized = sanitizeChineseCharacters(finalAnswerText);
        if (sanitized) {
          yield { type: 'text_delta', data: sanitized };
        }
      }

      const durationMs = Date.now() - startTime;
      console.log(`‚úÖ [${agentName}] Stream completed in ${durationMs}ms`);

      yield {
        type: 'done',
        data: {
          success: true,
          finalAgent: agentName,
          toolsCalled,
          usage: {
            promptTokens: usage?.inputTokens ?? 0,
            completionTokens: usage?.outputTokens ?? 0,
          },
          metadata: { provider, modelId, durationMs },
        },
      };
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      const durationMs = Date.now() - startTime;

      console.error(`‚ùå [${agentName}] Stream error after ${durationMs}ms:`, errorMessage);
      yield { type: 'error', data: { code: 'STREAM_ERROR', error: errorMessage } };
    }
  }
}
