/**
 * AI SDK Model Provider
 *
 * Vercel AI SDK 6 based model provider with quad-provider architecture:
 * - Primary: Cerebras (llama-3.3-70b, 24M tokens/day)
 * - Fallback: Groq (llama-3.3-70b-versatile, 100K tokens/day)
 * - Verifier: Mistral (mistral-small-2506, 24B params)
 * - Vision: Gemini Flash-Lite (1M context, Vision, Search Grounding)
 *
 * @version 3.0.0
 * @updated 2026-01-12 - Removed OpenRouter (free tier tool calling unreliable)
 * @updated 2026-01-27 - Added Gemini Flash-Lite for Vision Agent
 */

import { createCerebras } from '@ai-sdk/cerebras';
import { createMistral } from '@ai-sdk/mistral';
import { createGroq } from '@ai-sdk/groq';
import { createGoogleGenerativeAI } from '@ai-sdk/google';
import type { LanguageModel } from 'ai';

// Use centralized config getters (supports AI_PROVIDERS_CONFIG JSON format)
import {
  getCerebrasApiKey,
  getMistralApiKey,
  getGroqApiKey,
  getGeminiApiKey,
} from '../../lib/config-parser';

// ============================================================================
// 1. Types
// ============================================================================

export type ProviderName = 'cerebras' | 'groq' | 'mistral' | 'gemini';

export interface ProviderStatus {
  cerebras: boolean;
  groq: boolean;
  mistral: boolean;
  gemini: boolean;
}

// ============================================================================
// 2. Runtime Provider Toggle (for testing)
// ============================================================================

/**
 * Runtime toggle state for providers (default: all enabled)
 * Use toggleProvider() to enable/disable at runtime for testing
 */
const providerToggleState: Record<ProviderName, boolean> = {
  cerebras: true,
  groq: true,
  mistral: true,
  gemini: true,
};

/**
 * Cached provider status (invalidated when toggling providers)
 * @optimization Reduces redundant API key checks during agent initialization
 */
let cachedProviderStatus: ProviderStatus | null = null;

/**
 * Toggle a provider on/off at runtime
 * @note Invalidates provider status cache to reflect changes
 */
export function toggleProvider(provider: ProviderName, enabled: boolean): void {
  providerToggleState[provider] = enabled;
  // Invalidate cache when provider toggle changes
  cachedProviderStatus = null;
  console.log(`üîß [Provider] ${provider} ${enabled ? 'ENABLED' : 'DISABLED'}`);
}

/**
 * Get current toggle state for all providers
 */
export function getProviderToggleState(): Record<ProviderName, boolean> {
  return { ...providerToggleState };
}

/**
 * Check if provider is enabled (both has API key AND toggle is on)
 */
function isProviderEnabled(provider: ProviderName): boolean {
  return providerToggleState[provider];
}

/**
 * Check which providers are available (API key exists AND toggle enabled)
 * @optimization Caches result for startup performance (agent-configs.ts calls this 5+ times)
 */
export function checkProviderStatus(): ProviderStatus {
  if (cachedProviderStatus) {
    return cachedProviderStatus;
  }

  cachedProviderStatus = {
    cerebras: !!getCerebrasApiKey() && isProviderEnabled('cerebras'),
    groq: !!getGroqApiKey() && isProviderEnabled('groq'),
    mistral: !!getMistralApiKey() && isProviderEnabled('mistral'),
    gemini: !!getGeminiApiKey() && isProviderEnabled('gemini'),
  };

  return cachedProviderStatus;
}

/**
 * Invalidate provider status cache (call when toggling providers)
 */
export function invalidateProviderStatusCache(): void {
  cachedProviderStatus = null;
}

export interface ProviderConfig {
  apiKey: string | undefined;
  baseURL?: string;
}

export interface ModelConfig {
  temperature?: number;
  maxTokens?: number;
}

// ============================================================================
// 3. Provider Factories
// ============================================================================

/**
 * Create Cerebras provider instance
 */
function createCerebrasProvider() {
  const apiKey = getCerebrasApiKey();
  if (!apiKey) {
    throw new Error('CEREBRAS_API_KEY not configured');
  }

  return createCerebras({
    apiKey,
  });
}

/**
 * Create Groq provider instance
 */
function createGroqProvider() {
  const apiKey = getGroqApiKey();
  if (!apiKey) {
    throw new Error('GROQ_API_KEY not configured');
  }

  return createGroq({
    apiKey,
  });
}

/**
 * Create Mistral provider instance
 */
function createMistralProvider() {
  const apiKey = getMistralApiKey();
  if (!apiKey) {
    throw new Error('MISTRAL_API_KEY not configured');
  }

  return createMistral({
    apiKey,
  });
}

/**
 * Create Gemini provider instance
 * Uses Google Generative AI (Gemini 2.5 Flash-Lite)
 *
 * @added 2026-01-27 Vision Agent
 */
function createGeminiProvider() {
  const apiKey = getGeminiApiKey();
  if (!apiKey) {
    throw new Error('GEMINI_API_KEY not configured');
  }

  return createGoogleGenerativeAI({
    apiKey,
  });
}

// ============================================================================
// 4. Model Factory Functions
// ============================================================================

/**
 * AI SDK ÌÉÄÏûÖ Ìò∏ÌôòÏÑ± Ìó¨Ìçº
 *
 * Provider SDKÎì§Ïù¥ LanguageModelV3Î•º Î∞òÌôòÌïòÏßÄÎßå generateText()Îäî LanguageModelV2Î•º Í∏∞ÎåÄÌï®.
 * Îü∞ÌÉÄÏûÑÏóêÏÑúÎäî Ìò∏ÌôòÎêòÎØÄÎ°ú ÌÉÄÏûÖ Ï∫êÏä§ÌåÖÏúºÎ°ú Ìï¥Í≤∞.
 *
 * üéØ P1 Fix: Îü∞ÌÉÄÏûÑ Í≤ÄÏ¶ù Ï∂îÍ∞ÄÎ°ú Provider SDK Î≥ÄÍ≤Ω Ïãú Ï°∞Í∏∞ ÏóêÎü¨ Í∞êÏßÄ
 *
 * @see https://github.com/vercel/ai/issues - AI SDK Î≤ÑÏ†Ñ Ìò∏ÌôòÏÑ± Ïù¥Ïäà
 */
function asLanguageModel(model: unknown): LanguageModel {
  // üéØ CODEX Review Fix: Ìï®ÏàòÌòï Î™®Îç∏ÎèÑ ÌóàÏö© (callable + ÏÜçÏÑ± Ï°∞Ìï© Í∞ÄÎä•)
  if (!model || (typeof model !== 'object' && typeof model !== 'function')) {
    throw new TypeError('[ModelProvider] Model must be an object or function');
  }

  // Check for essential LanguageModel interface methods
  const m = model as Record<string, unknown>;
  const hasDoGenerate = typeof m.doGenerate === 'function';
  const hasDoStream = typeof m.doStream === 'function';

  if (!hasDoGenerate && !hasDoStream) {
    throw new TypeError(
      '[ModelProvider] Model does not implement LanguageModel interface (missing doGenerate/doStream)'
    );
  }

  return model as LanguageModel;
}

/**
 * Get Cerebras model via OpenAI-compatible API
 * @param modelId - 'llama-3.3-70b' (default) or 'llama-3.1-8b'
 */
export function getCerebrasModel(
  modelId: string = 'llama-3.3-70b'
): LanguageModel {
  const cerebras = createCerebrasProvider();
  return asLanguageModel(cerebras(modelId));
}

/**
 * Get Groq model
 * @param modelId - 'llama-3.3-70b-versatile' (default) or 'llama-3.1-8b-instant'
 */
export function getGroqModel(
  modelId: string = 'llama-3.3-70b-versatile'
): LanguageModel {
  const groq = createGroqProvider();
  return asLanguageModel(groq(modelId));
}

/**
 * Get Mistral model
 * @param modelId - 'mistral-small-2506' (default)
 */
export function getMistralModel(
  modelId: string = 'mistral-small-2506'
): LanguageModel {
  const mistral = createMistralProvider();
  return asLanguageModel(mistral(modelId));
}

/**
 * Get Gemini Flash-Lite model (Vision Agent)
 *
 * Features:
 * - 1M token context window
 * - Vision (Image/PDF/Video/Audio)
 * - Google Search Grounding
 * - URL Context
 *
 * Free Tier (2026-01):
 * - 1,000 RPD, 15 RPM, 250K TPM
 *
 * @param modelId - 'gemini-2.5-flash-lite' (default) or 'gemini-2.5-flash-preview-04-17'
 * @added 2026-01-27
 */
export function getGeminiFlashLiteModel(
  modelId: string = 'gemini-2.5-flash-lite'
): LanguageModel {
  const gemini = createGeminiProvider();
  return asLanguageModel(gemini(modelId));
}

// ============================================================================
// 5. Supervisor Model with Fallback Chain
// ============================================================================

/**
 * Get primary model for Supervisor (Single-Agent Mode)
 * Fallback chain: Cerebras ‚Üí Mistral ‚Üí Groq
 *
 * @param excludeProviders - Providers to skip (e.g., recently failed providers on retry)
 */
export function getSupervisorModel(excludeProviders: ProviderName[] = []): {
  model: LanguageModel;
  provider: ProviderName;
  modelId: string;
} {
  const status = checkProviderStatus();
  const excluded = new Set(excludeProviders);

  if (excluded.size > 0) {
    console.log(`üîÑ [Supervisor] Excluding providers: [${excludeProviders.join(', ')}]`);
  }

  // Try Cerebras first (24M tokens/day, fastest)
  if (status.cerebras && !excluded.has('cerebras')) {
    try {
      return {
        model: getCerebrasModel('llama-3.3-70b'),
        provider: 'cerebras',
        modelId: 'llama-3.3-70b',
      };
    } catch (error) {
      console.warn('‚ö†Ô∏è [Supervisor] Cerebras initialization failed:', error);
    }
  }

  // Fallback 1: Mistral
  if (status.mistral && !excluded.has('mistral')) {
    return {
      model: getMistralModel('mistral-small-2506'),
      provider: 'mistral',
      modelId: 'mistral-small-2506',
    };
  }

  // Fallback 2: Groq
  if (status.groq && !excluded.has('groq')) {
    console.log('üîÑ [Supervisor] Using Groq fallback');
    return {
      model: getGroqModel('llama-3.3-70b-versatile'),
      provider: 'groq',
      modelId: 'llama-3.3-70b-versatile',
    };
  }

  throw new Error('No LLM provider configured. Set CEREBRAS_API_KEY, MISTRAL_API_KEY, or GROQ_API_KEY.');
}

/**
 * Get verifier model with 3-way fallback
 * Mistral ‚Üí Cerebras ‚Üí Groq
 * Ensures operation even if 2 of 3 providers are down
 */
export function getVerifierModel(): {
  model: LanguageModel;
  provider: ProviderName;
  modelId: string;
} {
  const status = checkProviderStatus();

  // Primary: Mistral (best for verification)
  if (status.mistral) {
    try {
      return {
        model: getMistralModel('mistral-small-2506'),
        provider: 'mistral',
        modelId: 'mistral-small-2506',
      };
    } catch (error) {
      console.warn('‚ö†Ô∏è [Verifier] Mistral initialization failed, trying Cerebras:', error);
    }
  }

  // Fallback 1: Cerebras
  if (status.cerebras) {
    try {
      console.log('üîÑ [Verifier] Using Cerebras fallback');
      return {
        model: getCerebrasModel('llama-3.3-70b'),
        provider: 'cerebras',
        modelId: 'llama-3.3-70b',
      };
    } catch (error) {
      console.warn('‚ö†Ô∏è [Verifier] Cerebras initialization failed, trying Groq:', error);
    }
  }

  // Fallback 2: Groq (last resort)
  if (status.groq) {
    console.log('üîÑ [Verifier] Using Groq fallback (last resort)');
    return {
      model: getGroqModel('llama-3.3-70b-versatile'),
      provider: 'groq',
      modelId: 'llama-3.3-70b-versatile',
    };
  }

  throw new Error('No provider available for verifier model (all 3 providers down).');
}

/**
 * Get advisor model with 3-way fallback
 * Mistral ‚Üí Groq ‚Üí Cerebras
 * Ensures operation even if 2 of 3 providers are down
 */
export function getAdvisorModel(): {
  model: LanguageModel;
  provider: ProviderName;
  modelId: string;
} {
  const status = checkProviderStatus();

  // Primary: Mistral (best for RAG + reasoning)
  if (status.mistral) {
    try {
      return {
        model: getMistralModel('mistral-small-2506'),
        provider: 'mistral',
        modelId: 'mistral-small-2506',
      };
    } catch (error) {
      console.warn('‚ö†Ô∏è [Advisor] Mistral initialization failed, trying Groq:', error);
    }
  }

  // Fallback 1: Groq
  if (status.groq) {
    try {
      console.log('üîÑ [Advisor] Using Groq fallback');
      return {
        model: getGroqModel('llama-3.3-70b-versatile'),
        provider: 'groq',
        modelId: 'llama-3.3-70b-versatile',
      };
    } catch (error) {
      console.warn('‚ö†Ô∏è [Advisor] Groq initialization failed, trying Cerebras:', error);
    }
  }

  // Fallback 2: Cerebras (last resort)
  if (status.cerebras) {
    console.log('üîÑ [Advisor] Using Cerebras fallback (last resort)');
    return {
      model: getCerebrasModel('llama-3.3-70b'),
      provider: 'cerebras',
      modelId: 'llama-3.3-70b',
    };
  }

  throw new Error('No provider available for advisor model (all 3 providers down).');
}

/**
 * Get Vision Agent model (Gemini Flash-Lite Only - Graceful Degradation)
 *
 * NO FALLBACK: Vision Agent uses Gemini exclusively due to unique features:
 * - 1M token context (vs 128K for others)
 * - Vision/PDF/Video/Audio multimodal
 * - Google Search Grounding
 * - URL Context
 *
 * When Gemini is unavailable:
 * - Returns null
 * - Vision Agent will be disabled
 * - Text-based agents continue to function normally
 *
 * @returns Model info or null (graceful degradation)
 * @added 2026-01-27
 */
export function getVisionAgentModel(): {
  model: LanguageModel;
  provider: 'gemini';
  modelId: string;
} | null {
  const status = checkProviderStatus();

  if (!status.gemini) {
    console.warn('‚ö†Ô∏è [Vision Agent] Gemini unavailable - Vision features disabled');
    return null;
  }

  try {
    return {
      model: getGeminiFlashLiteModel('gemini-2.5-flash-lite'),
      provider: 'gemini',
      modelId: 'gemini-2.5-flash-lite',
    };
  } catch (error) {
    console.error('‚ùå [Vision Agent] Gemini initialization failed:', error);
    return null;
  }
}

/**
 * Check if Vision Agent is available
 * @returns true if Gemini provider is configured and enabled
 */
export function isVisionAgentAvailable(): boolean {
  const status = checkProviderStatus();
  return status.gemini;
}

// ============================================================================
// 6. Health Check
// ============================================================================

export interface ProviderHealth {
  provider: ProviderName;
  status: 'ok' | 'error';
  latencyMs?: number;
  error?: string;
}

/**
 * Test provider connectivity (lightweight check)
 */
export async function testProviderHealth(
  provider: ProviderName
): Promise<ProviderHealth> {
  const startTime = Date.now();

  try {
    switch (provider) {
      case 'cerebras':
        getCerebrasModel(); // Just check if provider can be created
        break;
      case 'groq':
        getGroqModel();
        break;
      case 'mistral':
        getMistralModel();
        break;
      case 'gemini':
        getGeminiFlashLiteModel();
        break;
    }

    return {
      provider,
      status: 'ok',
      latencyMs: Date.now() - startTime,
    };
  } catch (error) {
    return {
      provider,
      status: 'error',
      error: error instanceof Error ? error.message : String(error),
    };
  }
}

/**
 * Check all providers health
 */
export async function checkAllProvidersHealth(): Promise<ProviderHealth[]> {
  const status = checkProviderStatus();
  const results: ProviderHealth[] = [];

  if (status.cerebras) {
    results.push(await testProviderHealth('cerebras'));
  }
  if (status.groq) {
    results.push(await testProviderHealth('groq'));
  }
  if (status.mistral) {
    results.push(await testProviderHealth('mistral'));
  }
  if (status.gemini) {
    results.push(await testProviderHealth('gemini'));
  }

  return results;
}

/**
 * Log provider status
 */
export function logProviderStatus(): void {
  const status = checkProviderStatus();
  console.log('üîë AI SDK Provider Status:', {
    Cerebras: status.cerebras ? '‚úÖ' : '‚ùå',
    Groq: status.groq ? '‚úÖ' : '‚ùå',
    Mistral: status.mistral ? '‚úÖ' : '‚ùå',
    Gemini: status.gemini ? '‚úÖ (Vision)' : '‚ùå',
  });
}

// ============================================================================
// 7. Pre-emptive Fallback with Quota Tracking
// ============================================================================

import {
  selectAvailableProvider,
  recordProviderUsage,
  getQuotaSummary,
  type ProviderName as QuotaProviderName,
} from '../resilience/quota-tracker';

/**
 * Get Supervisor model with Pre-emptive Fallback (Quota-aware)
 *
 * 80% ÏûÑÍ≥ÑÍ∞í ÎèÑÎã¨ Ïãú ÏÇ¨Ï†Ñ Ï†ÑÌôòÌïòÏó¨ Rate Limit ÏóêÎü¨ Î∞©ÏßÄ
 *
 * @param excludeProviders - Ï†úÏô∏Ìï† Provider Î™©Î°ù
 * @returns Î™®Îç∏ Ï†ïÎ≥¥ + Pre-emptive Fallback Ïó¨Î∂Ä
 */
export async function getSupervisorModelWithQuota(
  excludeProviders: ProviderName[] = []
): Promise<{
  model: LanguageModel;
  provider: ProviderName;
  modelId: string;
  isPreemptiveFallback: boolean;
}> {
  const status = checkProviderStatus();
  const excluded = new Set(excludeProviders);

  // Provider Ïö∞ÏÑ†ÏàúÏúÑ
  const preferredOrder: QuotaProviderName[] = ['cerebras', 'mistral', 'groq'];
  const availableOrder = preferredOrder.filter(
    (p) => status[p] && !excluded.has(p)
  );

  // Quota-aware Provider ÏÑ†ÌÉù
  const selection = await selectAvailableProvider(availableOrder);

  if (selection) {
    const { provider, isPreemptiveFallback } = selection;

    if (isPreemptiveFallback) {
      console.log(`‚ö†Ô∏è [Supervisor] Pre-emptive fallback to ${provider}`);
    }

    // ProviderÎ≥Ñ Î™®Îç∏ Î∞òÌôò
    switch (provider) {
      case 'cerebras':
        return {
          model: getCerebrasModel('llama-3.3-70b'),
          provider: 'cerebras',
          modelId: 'llama-3.3-70b',
          isPreemptiveFallback,
        };
      case 'mistral':
        return {
          model: getMistralModel('mistral-small-2506'),
          provider: 'mistral',
          modelId: 'mistral-small-2506',
          isPreemptiveFallback,
        };
      case 'groq':
        return {
          model: getGroqModel('llama-3.3-70b-versatile'),
          provider: 'groq',
          modelId: 'llama-3.3-70b-versatile',
          isPreemptiveFallback,
        };
    }
  }

  // Quota Ï¥àÍ≥º Ïãú Í∏∞Ï°¥ Î°úÏßÅÏúºÎ°ú Ìè¥Î∞±
  console.warn('‚ö†Ô∏è [Supervisor] All providers at quota limit, using fallback logic');
  const fallback = getSupervisorModel(excludeProviders);
  return { ...fallback, isPreemptiveFallback: false };
}

/**
 * Record token usage after API call
 *
 * üéØ AI SDK v6 Best Practice: Track all provider usage for pre-emptive fallback
 * All providers including Groq are tracked for accurate quota management
 *
 * @param provider - Provider name
 * @param tokensUsed - Number of tokens consumed
 * @param context - Optional context for logging (e.g., 'nlq', 'fallback', 'supervisor')
 */
export async function recordModelUsage(
  provider: ProviderName,
  tokensUsed: number,
  context: string = 'general'
): Promise<void> {
  // Track all providers for quota management
  // Groq has lower limits (100K/day) so tracking is especially important for fallback scenarios
  await recordProviderUsage(provider as QuotaProviderName, tokensUsed);

  // Enhanced logging with context
  if (provider === 'groq') {
    console.log(`[QuotaTracker] Groq (${context}): ${tokensUsed} tokens - low quota provider, monitor closely`);
  }
}

/**
 * Get quota summary for all providers
 */
export { getQuotaSummary };
