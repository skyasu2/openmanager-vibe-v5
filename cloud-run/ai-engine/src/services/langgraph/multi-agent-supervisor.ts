/**
 * Multi-Agent Supervisor using @langchain/langgraph-supervisor
 * Cloud Run Standalone Implementation
 */

import { AIMessage, BaseMessage, HumanMessage, SystemMessage } from '@langchain/core/messages';
import { createReactAgent } from '@langchain/langgraph/prebuilt';
import { createSupervisor } from '@langchain/langgraph-supervisor';
import {
  analyzePatternTool,
  detectAnomaliesTool,
  predictTrendsTool,
} from '../../agents/analyst-agent';
// Import tools from Agents
import {
  getServerMetricsTool,
  getServerLogsTool,
  getServerMetricsAdvancedTool,
} from '../../agents/nlq-agent';
// NLQ SubGraph for complex queries (Phase 2)
import { executeNlqSubGraph } from './nlq-subgraph';
import {
  recommendCommandsTool,
  searchKnowledgeBaseTool,
} from '../../agents/reporter-agent';
// Verifier Agent for post-processing validation
import { comprehensiveVerifyTool } from '../../agents/verifier-agent';
import type { VerificationResult } from '../../lib/state-definition';

import {
  createSessionConfig,
  getAutoCheckpointer,
} from '../../lib/checkpointer';
// Context Compression Integration (Phase 3)
import {
  needsCompression,
  getCompressionStats,
  getSummarizer,
  isCompressionDisabled,
} from '../../lib/context-compression';
import { RateLimitError } from '../../lib/errors';
import { approvalStore } from '../approval/approval-store';
import {
  getAnalystModel,
  getNLQModel,
  getReporterModel,
  getSupervisorModel,
  createMistralModel,
  MISTRAL_MODELS,
} from '../../lib/model-config';
// LangFuse Integration (Phase 2)
import { createSessionHandler } from '../../lib/langfuse-handler';

// ============================================================================
// 0. Groq Message Compatibility Helper
// ============================================================================

/**
 * Creates a stateModifier function that:
 * 1. Adds a system prompt
 * 2. Filters out tool-call messages that Groq cannot process
 * 3. Ensures all message content is string-based
 *
 * This is necessary because the Supervisor generates tool-call messages
 * with array-based content that Groq's API rejects.
 */
function createGroqCompatibleStateModifier(systemPrompt: string) {
  return (state: { messages: BaseMessage[] }): BaseMessage[] => {
    const filteredMessages: BaseMessage[] = [];

    // Add system prompt first
    filteredMessages.push(new SystemMessage(systemPrompt));

    // Filter and transform messages for Groq compatibility
    for (const msg of state.messages) {
      // Skip tool-call messages (they have complex content structures)
      // Also skip internal handoff messages from supervisor
      const content = msg.content;

      // Check if content is a string (Groq-compatible)
      if (typeof content === 'string') {
        // Skip internal supervisor handoff messages
        if (content.includes('Successfully transferred') ||
            content.includes('Transferring back to')) {
          continue;
        }
        filteredMessages.push(msg);
      }
      // If content is an array, try to extract text parts only
      else if (Array.isArray(content)) {
        const textParts = content
          .filter((part): part is { type: 'text'; text: string } =>
            typeof part === 'object' && part !== null && part.type === 'text'
          )
          .map(part => part.text)
          .join('\n');

        if (textParts.length > 0) {
          // Create a new message with string content
          if (msg._getType() === 'human') {
            filteredMessages.push(new HumanMessage(textParts));
          } else if (msg._getType() === 'ai') {
            filteredMessages.push(new AIMessage(textParts));
          }
        }
      }
    }

    return filteredMessages;
  };
}

// ============================================================================
// 1. Worker Agent Creation
// ============================================================================

/**
 * Create NLQ Agent - Server metrics queries
 * Phase 2 Enhancement: Added getServerMetricsAdvancedTool for complex queries
 */
function createNLQAgent() {
  const systemPrompt = `NLQ Agent - ì„œë²„ ë©”íŠ¸ë¦­/ë¡œê·¸ ì¡°íšŒ ì „ë¬¸

## ë„êµ¬ ì‚¬ìš© ê·œì¹™
1. **ì „ì²´ ì„œë²„ ì¡°íšŒ**: "ì„œë²„ ìƒíƒœ", "ì „ì²´ í˜„í™©" ë“± â†’ serverId ìƒëµ (í•„ìˆ˜!)
2. **íŠ¹ì • ì„œë²„ ì¡°íšŒ**: "WEB-01 ìƒíƒœ" ë“± â†’ serverId ì§€ì •
3. ë¡œê·¸/ì—ëŸ¬ ì¡°íšŒ â†’ getServerLogs
4. ìƒíƒœ/ë©”íŠ¸ë¦­ ì¡°íšŒ â†’ getServerMetrics
5. **ê³ ê¸‰ ì¿¼ë¦¬** (ì‹œê°„ ë²”ìœ„/í•„í„°/ì§‘ê³„) â†’ getServerMetricsAdvanced
   - "ì§€ë‚œ 6ì‹œê°„ CPU í‰ê· " â†’ timeRange="last6h", aggregation="avg"
   - "CPU 80% ì´ìƒ ì„œë²„ë“¤" â†’ filters=[{field:"cpu", operator:">", value:80}]
   - "ë©”ëª¨ë¦¬ TOP 5" â†’ sortBy="memory", limit=5
6. ì‹œìŠ¤í…œ ìš©ì–´/ê°œë… í™•ì¸ â†’ searchKnowledgeBase (RAG)

## ì „ì²´ ì„œë²„ ì‘ë‹µ í˜•ì‹ (serverId ì—†ì´ ì¡°íšŒ ì‹œ)
ğŸ“Š **ì „ì²´ ì„œë²„ í˜„í™©** (ì´ NëŒ€)
- âœ… ì •ìƒ: NëŒ€
- âš ï¸ ì£¼ì˜: NëŒ€ (ì„œë²„ëª… ë‚˜ì—´)
- ğŸ”´ ìœ„í—˜: NëŒ€ (ì„œë²„ëª… ë‚˜ì—´)

**ì£¼ìš” ì´ìƒ ì„œë²„:**
â€¢ [ì„œë²„ëª…] CPU X% / Memory X% / Disk X% - ìƒíƒœ

## ê³ ê¸‰ ì¿¼ë¦¬ ì‘ë‹µ í˜•ì‹
ğŸ“ˆ **[ì¿¼ë¦¬ ì„¤ëª…]** (ì‹œê°„ ë²”ìœ„/ì¡°ê±´)
- ì„œë²„1: ë©”íŠ¸ë¦­ê°’
- ì„œë²„2: ë©”íŠ¸ë¦­ê°’
[ìš”ì•½: NëŒ€ ì¤‘ NëŒ€ ë§¤ì¹­]

## íŠ¹ì • ì„œë²„ ì‘ë‹µ í˜•ì‹
â€¢ [ì„œë²„ëª…] CPU: X% | Memory: X% | Disk: X%
â€¢ ìƒíƒœ: ì •ìƒ/ì£¼ì˜/ìœ„í—˜
â€¢ íŠ¹ì´ì‚¬í•­: (ìˆìœ¼ë©´ 1ì¤„)

âš ï¸ ì¤‘ìš”: íŠ¹ì • ì„œë²„ë¥¼ ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ ë°˜ë“œì‹œ serverIdë¥¼ ìƒëµí•˜ì—¬ ì „ì²´ ì¡°íšŒ!`;

  return createReactAgent({
    llm: getNLQModel(),
    tools: [
      getServerMetricsTool,
      getServerLogsTool,
      getServerMetricsAdvancedTool, // Phase 2: ê³ ê¸‰ ì¿¼ë¦¬ ë„êµ¬
      searchKnowledgeBaseTool,
    ],
    name: 'nlq_agent',
    stateModifier: createGroqCompatibleStateModifier(systemPrompt),
  });
}

/**
 * Execute NLQ SubGraph directly for complex queries
 * Alternative execution path when Supervisor detects complex NLQ query
 * @param query - Natural language query
 * @returns Formatted response from NLQ SubGraph
 */
export async function executeComplexNlqQuery(query: string): Promise<{
  success: boolean;
  response: string;
  metadata: {
    intent: string;
    timeRange?: string;
    aggregation?: string;
    filterCount: number;
  };
}> {
  const result = await executeNlqSubGraph(query);
  return {
    success: result.success,
    response: result.response,
    metadata: {
      intent: result.intent,
      timeRange: result.params?.timeRange,
      aggregation: result.params?.aggregation,
      filterCount: result.params?.filters?.length || 0,
    },
  };
}

/**
 * Create Analyst Agent - Pattern analysis & anomaly detection
 */
function createAnalystAgent() {
  const systemPrompt = `Analyst Agent - íŒ¨í„´ ë¶„ì„/ì´ìƒ íƒì§€ ì „ë¬¸

## ë„êµ¬
- detectAnomalies: ì´ìƒì¹˜ ê°ì§€
- predictTrends: íŠ¸ë Œë“œ ì˜ˆì¸¡
- analyzePattern: íŒ¨í„´ ë¶„ì„
- searchKnowledgeBase: ê³¼ê±° ì‚¬ë¡€ ë° í•´ê²° ê°€ì´ë“œ ê²€ìƒ‰ (RAG)

## ì‘ë‹µ í˜•ì‹ (í•„ìˆ˜)
**í˜„í™©**: (1ì¤„ ìš”ì•½)
**íŒ¨í„´**: (ë°œê²¬ëœ íŒ¨í„´ í•´ì„)
**ì¡°ì¹˜**: (í•„ìš”ì‹œ ê¶Œì¥ì‚¬í•­)

âš ï¸ í†µê³„ ìˆ˜ì¹˜ë§Œ ë‚˜ì—´ ê¸ˆì§€. ì˜ë¯¸ í•´ì„ ì¤‘ì‹¬ìœ¼ë¡œ 3ì„¹ì…˜ ì´ë‚´.`;

  return createReactAgent({
    llm: getAnalystModel(),
    tools: [detectAnomaliesTool, predictTrendsTool, analyzePatternTool, searchKnowledgeBaseTool],
    name: 'analyst_agent',
    stateModifier: createGroqCompatibleStateModifier(systemPrompt),
  });
}

/**
 * Create Reporter Agent - Incident reports & RAG
 */
function createReporterAgent() {
  const systemPrompt = `Reporter Agent - ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸ ì „ë¬¸

## ë„êµ¬
- searchKnowledgeBase: RAG ê²€ìƒ‰
- recommendCommands: CLI ëª…ë ¹ì–´

## ì‘ë‹µ í˜•ì‹ (ì—„ê²© ì¤€ìˆ˜)
### ğŸ“‹ ìš”ì•½
(1-2ì¤„ í•µì‹¬ë§Œ)

### ğŸ” ì›ì¸
- (ì›ì¸ 1ì¤„ì”©, ìµœëŒ€ 3ê°œ)

### ğŸ’¡ ì¡°ì¹˜
1. (ë‹¨ê³„ë³„, ìµœëŒ€ 3ë‹¨ê³„)

### âŒ¨ï¸ ëª…ë ¹ì–´
\`command\` - ì„¤ëª…

âš ï¸ ì„œë¡ /ì¸ì‚¬ë§ ê¸ˆì§€. í…œí”Œë¦¿ í˜•ì‹ë§Œ ì¶œë ¥.`;

  return createReactAgent({
    llm: getReporterModel(),
    tools: [searchKnowledgeBaseTool, recommendCommandsTool],
    name: 'reporter_agent',
    stateModifier: createGroqCompatibleStateModifier(systemPrompt),
  });
}

// ============================================================================
// 2. Supervisor Creation
// ============================================================================

const SUPERVISOR_PROMPT = `ë‹¹ì‹ ì€ OpenManager VIBEì˜ Multi-Agent Supervisorì…ë‹ˆë‹¤.

## ì—ì´ì „íŠ¸ ë¼ìš°íŒ…
- **nlq_agent**: ì„œë²„ ìƒíƒœ/ë©”íŠ¸ë¦­ ì¡°íšŒ (CPU, Memory, Disk)
- **analyst_agent**: íŒ¨í„´ ë¶„ì„, ì´ìƒ íƒì§€, íŠ¸ë Œë“œ ì˜ˆì¸¡
- **reporter_agent**: ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸, ì¥ì•  ë¶„ì„, RAG ê²€ìƒ‰

## ë¼ìš°íŒ… ê·œì¹™
- "ì„œë²„ ìƒíƒœ", "ì „ì²´ í˜„í™©", "ì„œë²„ í™•ì¸" â†’ nlq_agent (ì „ì²´ ì„œë²„ ì¡°íšŒ í•„ìš”)
- "WEB-01 ìƒíƒœ" ë“± íŠ¹ì • ì„œë²„ ì–¸ê¸‰ â†’ nlq_agent (í•´ë‹¹ ì„œë²„ë§Œ ì¡°íšŒ)
- ë¶„ì„/ì˜ˆì¸¡/íŠ¸ë Œë“œ â†’ analyst_agent
- ì¥ì• /ë¦¬í¬íŠ¸/ì›ì¸ â†’ reporter_agent
- ì¸ì‚¬ë§ â†’ ì§ì ‘ ì‘ë‹µ (1ë¬¸ì¥)

## ì‘ë‹µ ì§€ì¹¨
- ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬ (ì¬ê°€ê³µ ê¸ˆì§€)
- ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§/ì„œë¡  ê¸ˆì§€
- í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ`;

/**
 * Create Multi-Agent Supervisor Workflow
 */
interface SupervisorOptions {
  useFallbackModel?: boolean;
}

export async function createMultiAgentSupervisor(options: SupervisorOptions = {}) {
  const { useFallbackModel = false } = options;
  const checkpointer = await getAutoCheckpointer();

  // Create worker agents
  const nlqAgent = createNLQAgent();
  const analystAgent = createAnalystAgent();
  const reporterAgent = createReporterAgent();

  // Select model: Groq (primary) or Mistral (fallback)
  const supervisorModel = useFallbackModel
    ? createMistralModel(MISTRAL_MODELS.SMALL, { temperature: 0.1, maxOutputTokens: 512 })
    : getSupervisorModel();

  if (useFallbackModel) {
    console.log('ğŸ”„ [Supervisor] Using Mistral fallback model due to Groq rate limit');
  }

  // Create supervisor with automatic handoffs
  const workflow = createSupervisor({
    agents: [nlqAgent, analystAgent, reporterAgent],
    llm: supervisorModel,
    prompt: SUPERVISOR_PROMPT,
    outputMode: 'full_history',
  });

  // Compile with checkpointer for session persistence
  return workflow.compile({
    checkpointer,
  });
}

// ============================================================================
// 3. Verifier Integration (Post-Processing)
// ============================================================================

interface VerificationOptions {
  enableVerification?: boolean;
  context?: string;
}

/**
 * Verify agent response using Verifier Agent
 * Post-processing validation for quality assurance
 */
async function verifyAgentResponse(
  response: string,
  options: VerificationOptions = {}
): Promise<{
  response: string;
  verification: VerificationResult | null;
}> {
  const { enableVerification = true, context } = options;

  if (!enableVerification) {
    return { response, verification: null };
  }

  try {
    const startTime = Date.now();
    const result = await comprehensiveVerifyTool.invoke({
      response,
      context,
    });

    const verification: VerificationResult = {
      isValid: result.isValid,
      confidence: result.confidence,
      originalResponse: result.originalResponse,
      validatedResponse: result.validatedResponse,
      issues: result.issues || [],
      metadata: {
        verifiedAt: new Date().toISOString(),
        rulesApplied: result.metadata?.rulesApplied || [],
        corrections: result.metadata?.corrections || [],
        processingTimeMs: Date.now() - startTime,
      },
    };

    console.log(
      `âœ… [Verifier] Response verified. Confidence: ${(verification.confidence * 100).toFixed(1)}%, Issues: ${verification.issues.length}`
    );

    // Return validated response if corrections were made
    return {
      response: verification.validatedResponse || response,
      verification,
    };
  } catch (error) {
    console.warn('âš ï¸ [Verifier] Verification failed, using original response:', error);
    return { response, verification: null };
  }
}

// ============================================================================
// 4. Context Compression Helper
// ============================================================================

/**
 * Compress conversation history if needed
 * Pre-processing step before supervisor invocation
 *
 * @param messages - Current conversation messages
 * @returns Compressed messages or original if compression not needed
 */
async function compressIfNeeded(
  messages: BaseMessage[]
): Promise<{ messages: BaseMessage[]; wasCompressed: boolean; compressionInfo?: { originalCount: number; newCount: number; ratio: number } }> {
  // Check if compression is globally disabled via environment variable
  if (isCompressionDisabled()) {
    return { messages, wasCompressed: false };
  }

  // Check if compression is needed (threshold: 85% context usage, configurable via COMPRESSION_THRESHOLD)
  if (!needsCompression(messages)) {
    return { messages, wasCompressed: false };
  }

  console.log('[Compression] Context compression triggered', {
    messageCount: messages.length,
  });

  try {
    const summarizer = getSummarizer();
    const result = await summarizer.summarize(messages);

    if (!result.wasCompressed) {
      return { messages, wasCompressed: false };
    }

    // Build new message array: [summary, ...recentMessages]
    const compressedMessages: BaseMessage[] = [
      result.summaryMessage,
      ...result.compressedBuffer.recentMessages,
    ];

    console.log('[Compression] Completed', {
      originalCount: messages.length,
      newCount: compressedMessages.length,
      compressionRatio: result.compressedBuffer.metadata.compressionRatio,
      processingTimeMs: result.processingTimeMs,
    });

    return {
      messages: compressedMessages,
      wasCompressed: true,
      compressionInfo: {
        originalCount: messages.length,
        newCount: compressedMessages.length,
        ratio: result.compressedBuffer.metadata.compressionRatio,
      },
    };
  } catch (error) {
    console.warn('[Compression] Failed, using original messages:', error);
    return { messages, wasCompressed: false };
  }
}

// ============================================================================
// 5. Execution Functions
// ============================================================================

export interface SupervisorExecutionOptions {
  sessionId?: string;
  enableVerification?: boolean;
  verificationContext?: string;
  /** Enable context compression for long conversations (default: true) */
  enableCompression?: boolean;
  /** Enable LangFuse tracing for observability (default: true) */
  enableTracing?: boolean;
  /** User ID for LangFuse tracing */
  userId?: string;
}

/**
 * Execute supervisor workflow (single response)
 * Uses Mistral AI for Supervisor with automatic rate limit handling
 * v5.85.0: Added Verifier Agent post-processing for quality assurance
 * v5.86.0: Added Context Compression for long conversations
 * v5.87.0: Added LangFuse tracing for observability
 * v5.88.0: Migrated from Gemini to Mistral AI
 */
export async function executeSupervisor(
  query: string,
  options: SupervisorExecutionOptions = {}
): Promise<{
  response: string;
  sessionId: string;
  verification?: VerificationResult | null;
  compressionApplied?: boolean;
}> {
  const sessionId = options.sessionId || `session_${Date.now()}`;
  const {
    enableVerification = true,
    verificationContext,
    enableCompression = true,
    enableTracing = true,
    userId,
  } = options;
  const MAX_RETRIES = 3; // Retry on transient errors
  let compressionApplied = false;

  // === LangFuse Tracing (v5.87.0) ===
  // Note: LangFuse integration is initialized for session-level observability
  // The handler is passed to LangGraph config for automatic tracing
  const langfuseHandler = enableTracing
    ? await createSessionHandler({
        sessionId,
        userId,
        metadata: {
          query: query.slice(0, 100), // Truncate for metadata
          enableVerification,
          enableCompression,
        },
      })
    : null;

  // Create config with optional LangFuse callbacks
  const config = createSessionConfig(
    sessionId,
    undefined,
    langfuseHandler ? [langfuseHandler] : undefined
  );

  if (langfuseHandler) {
    console.log(`ğŸ“Š [LangFuse] Tracing initialized for session: ${sessionId}`);
  }

  let useFallbackModel = false;

  for (let attempt = 0; attempt < MAX_RETRIES; attempt++) {
    try {
      // Create fresh supervisor (Groq primary, Mistral fallback on rate limit)
      const app = await createMultiAgentSupervisor({ useFallbackModel });

      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      const result = await app.invoke(
        {
          messages: [new HumanMessage(query)],
        },
        config as any // Type assertion for LangGraph config with LangFuse callbacks
      );

      // Extract final response from messages
      const messages = result.messages || [];

      // === Context Compression (v5.86.0) ===
      // Check if compression is needed after accumulating messages
      if (enableCompression && messages.length > 0) {
        const compressionResult = await compressIfNeeded(messages);
        if (compressionResult.wasCompressed) {
          compressionApplied = true;
          console.log(`ğŸ—œï¸ [Supervisor] Context compressed: ${compressionResult.compressionInfo?.originalCount} â†’ ${compressionResult.compressionInfo?.newCount} messages`);
        }
      }

      const lastMessage = messages[messages.length - 1];
      let response =
        typeof lastMessage?.content === 'string'
          ? lastMessage.content
          : 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';

      // === Verifier Agent Post-Processing ===
      let verification: VerificationResult | null = null;
      if (enableVerification && response && response !== 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.') {
        const verifyResult = await verifyAgentResponse(response, {
          enableVerification: true,
          context: verificationContext || query,
        });
        response = verifyResult.response;
        verification = verifyResult.verification;
      }

      console.log(`âœ… [Supervisor] Completed. Session: ${sessionId}, Compressed: ${compressionApplied}, Traced: ${!!langfuseHandler}`);

      // Flush LangFuse traces asynchronously (non-blocking)
      if (langfuseHandler?.flushAsync) {
        langfuseHandler.flushAsync().catch(err => console.warn('âš ï¸ [LangFuse] Flush failed:', err));
      }

      return { response, sessionId, verification, compressionApplied };
    } catch (error) {
      // Debug: Log caught error details
      const errorMessage = error instanceof Error ? error.message : String(error);
      console.error(`ğŸ”´ [Supervisor] Caught error on attempt ${attempt + 1}:`, {
        errorType: error?.constructor?.name,
        messagePreview: errorMessage.slice(0, 200),
      });

      // Check if this is a rate limit error
      const isRateLimit = RateLimitError.isRateLimitError(error);
      console.log(`ğŸ” [Supervisor] isRateLimitError check: ${isRateLimit}`);

      if (isRateLimit && attempt < MAX_RETRIES - 1) {
        // Switch to Mistral fallback model on rate limit
        if (!useFallbackModel) {
          console.warn(
            `âš ï¸ [Supervisor] Groq rate limit hit on attempt ${attempt + 1}/${MAX_RETRIES}, switching to Mistral fallback...`
          );
          useFallbackModel = true;
          // Minimal backoff before retry with fallback model
          await new Promise(resolve => setTimeout(resolve, 500));
          continue;
        }

        // If already using fallback and still hitting limits, apply backoff
        const backoffMs = Math.pow(2, attempt) * 1000;
        console.warn(
          `âš ï¸ [Supervisor] Rate limit hit on fallback model, attempt ${attempt + 1}/${MAX_RETRIES}, retrying in ${backoffMs}ms...`
        );
        await new Promise(resolve => setTimeout(resolve, backoffMs));
        continue;
      }

      // Re-throw if not a rate limit error or no more retries
      throw error;
    }
  }

  // Should not reach here, but TypeScript requires it
  throw new Error('Supervisor execution failed after max retries');
}

/**
 * Stream supervisor workflow (Alternative Implementation)
 * Uses app.stream() for native LangGraph streaming
 *
 * @note CURRENTLY UNUSED - Kept for future reference/optimization
 *
 * This function was replaced by `createSupervisorStreamResponse` which uses
 * `executeSupervisor()` + simulated SSE streaming for better Groq compatibility.
 * Native LangGraph streaming (streamEvents) doesn't emit text tokens properly for Groq models.
 *
 * Consider reactivating if:
 * 1. Switching to a model with native streaming support (e.g., Mistral, OpenAI)
 * 2. LangGraph improves Groq streaming support
 *
 * @see createSupervisorStreamResponse - Currently active implementation
 */
export async function* streamSupervisor(
  query: string,
  options: SupervisorExecutionOptions = {}
): AsyncGenerator<{
  type: 'token' | 'agent_start' | 'agent_end' | 'final' | 'error';
  content: string;
  metadata?: Record<string, unknown>;
}> {
  const app = await createMultiAgentSupervisor();
  const sessionId = options.sessionId || `session_${Date.now()}`;
  const config = createSessionConfig(sessionId);

  try {
    // Use default stream() instead of streamEvents for Groq compatibility
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const stream = await app.stream(
      { messages: [new HumanMessage(query)] },
      config as any // Type assertion for LangGraph config
    );

    let finalContent = '';
    let lastAgentName = '';

    for await (const chunk of stream) {
      // Chunk has agent name as key: { supervisor: {...}, test_agent: {...} }
      for (const [agentName, agentOutput] of Object.entries(chunk)) {
        // Emit agent start
        if (agentName !== lastAgentName) {
          if (lastAgentName) {
            yield {
              type: 'agent_end',
              content: lastAgentName,
              metadata: {},
            };
          }
          yield {
            type: 'agent_start',
            content: agentName,
            metadata: {},
          };
          lastAgentName = agentName;
        }

        // Extract AI message content from agent output
        const output = agentOutput as {
          messages?: Array<{ content?: string; _getType?: () => string }>;
        };
        if (output?.messages) {
          for (const msg of output.messages) {
            // Check if it's an AI message with content
            const msgType = msg._getType?.();
            if (
              msgType === 'ai' &&
              msg.content &&
              typeof msg.content === 'string' &&
              msg.content.length > 0
            ) {
              // Skip system messages like "Successfully transferred..."
              if (
                !msg.content.includes('Successfully transferred') &&
                !msg.content.includes('Transferring back to')
              ) {
                finalContent = msg.content; // Keep the last meaningful AI response
                yield {
                  type: 'token',
                  content: msg.content,
                  metadata: { agent: agentName },
                };
              }
            }
          }
        }
      }
    }

    // Emit final agent end
    if (lastAgentName) {
      yield {
        type: 'agent_end',
        content: lastAgentName,
        metadata: {},
      };
    }

    // Final response
    yield {
      type: 'final',
      content: finalContent,
      metadata: { sessionId },
    };
  } catch (error) {
    yield {
      type: 'error',
      content: error instanceof Error ? error.message : String(error),
      metadata: { sessionId },
    };
  }
}

/**
 * Detect if response requires human approval
 * Returns approval metadata if needed
 */
function detectApprovalRequired(
  response: string,
  query: string
): {
  required: boolean;
  actionType?: 'incident_report' | 'system_command';
  description?: string;
} {
  // Check for command recommendations in response
  const hasCommands =
    response.includes('âŒ¨ï¸') ||
    response.includes('ì¶”ì²œ ëª…ë ¹ì–´') ||
    response.includes('command') ||
    /`[a-z]+\s+[a-z]+`/i.test(response);

  // Check for incident report
  const isIncident =
    query.includes('ì¥ì• ') ||
    query.includes('ì¸ì‹œë˜íŠ¸') ||
    response.includes('ğŸ“‹ ì¸ì‹œë˜íŠ¸');

  if (isIncident) {
    return {
      required: true,
      actionType: 'incident_report',
      description: 'ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ê²€í†  í›„ ìŠ¹ì¸í•´ì£¼ì„¸ìš”.',
    };
  }

  if (hasCommands) {
    return {
      required: true,
      actionType: 'system_command',
      description: 'ì‹œìŠ¤í…œ ëª…ë ¹ì–´ê°€ ì¶”ì²œë˜ì—ˆìŠµë‹ˆë‹¤. ì‹¤í–‰ ì „ ê²€í† í•´ì£¼ì„¸ìš”.',
    };
  }

  return { required: false };
}

/**
 * Create AI SDK compatible streaming response
 * Uses invoke() for reliability with Groq, then simulates streaming
 * Includes SSE approval events for Human-in-the-Loop
 *
 * v5.84.0: Added keep-alive pings to prevent Vercel/Cloud Run timeout
 */
export async function createSupervisorStreamResponse(
  query: string,
  sessionId?: string
): Promise<ReadableStream<Uint8Array>> {
  const encoder = new TextEncoder();
  const effectiveSessionId = sessionId || `session_${Date.now()}`;

  return new ReadableStream({
    async start(controller) {
      let isClosed = false;
      let keepAliveInterval: ReturnType<typeof setInterval> | null = null;

      const safeEnqueue = (data: Uint8Array): boolean => {
        // Check both our flag AND controller's internal state
        if (isClosed) return false;

        try {
          // Double-check controller state before enqueue
          if (controller.desiredSize === null) {
            isClosed = true;
            return false;
          }
          controller.enqueue(data);
          return true;
        } catch {
          // Controller may have been closed externally (client disconnect)
          isClosed = true;
          return false;
        }
      };

      const safeClose = () => {
        // Clear keep-alive interval first
        if (keepAliveInterval) {
          clearInterval(keepAliveInterval);
          keepAliveInterval = null;
        }

        if (!isClosed) {
          isClosed = true;
          try {
            controller.close();
          } catch {
            // Controller may have been closed externally
          }
        }
      };

      // Start keep-alive ping (every 10 seconds)
      // Uses AI SDK annotation format '8:' for progress updates
      const startKeepAlive = () => {
        let pingCount = 0;
        keepAliveInterval = setInterval(() => {
          pingCount++;
          if (isClosed) {
            if (keepAliveInterval) clearInterval(keepAliveInterval);
            return;
          }

          // Send progress annotation (AI SDK v5 format)
          const progressMessages = [
            'ğŸ”„ AI ì—ì´ì „íŠ¸ê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...',
            'ğŸ“Š ì„œë²„ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...',
            'ğŸ§  íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...',
            'â³ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...',
          ];
          const message = progressMessages[pingCount % progressMessages.length];

          // Use annotation format for keep-alive (doesn't affect main response)
          const keepAliveMessage = `8:${JSON.stringify([
            { type: 'progress', message, timestamp: Date.now() },
          ])}\n`;

          if (!safeEnqueue(encoder.encode(keepAliveMessage))) {
            // Stream was closed, stop keep-alive
            if (keepAliveInterval) clearInterval(keepAliveInterval);
          }
        }, 10000); // 10 second interval
      };

      try {
        // ğŸš€ Anti-Timeout: Immediate First Byte (Vercel 504 ë°©ì§€)
        const thinkingMessage = `0:${JSON.stringify('ğŸ” ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n\n')}\n`;
        if (!safeEnqueue(encoder.encode(thinkingMessage))) {
          console.log('âš ï¸ Stream closed before processing started');
          return;
        }

        // Start keep-alive pings
        startKeepAlive();

        // Use invoke() for more reliable Groq integration
        // v5.85.0: Verification enabled by default
        // v5.86.0: Context Compression enabled by default
        // v5.87.1: Smart verification - skip for simple queries to reduce latency
        const isSimpleQuery = query.length < 30 ||
          /^(ì•ˆë…•|ë°˜ê°€ì›Œ|ê³ ë§ˆì›Œ|ë„ì›€|ë­í•´|í…ŒìŠ¤íŠ¸)/i.test(query);

        const { response, verification, compressionApplied } = await executeSupervisor(query, {
          sessionId: effectiveSessionId,
          enableVerification: !isSimpleQuery, // Skip verifier for simple queries (~10-20s savings)
          verificationContext: query,
          enableCompression: true,
        });

        // Stop keep-alive before sending response
        if (keepAliveInterval) {
          clearInterval(keepAliveInterval);
          keepAliveInterval = null;
        }

        // Check if stream was closed during supervisor execution
        if (isClosed) {
          console.log(
            'âš ï¸ Stream closed during supervisor execution, skipping response'
          );
          return;
        }

        if (response) {
          // AI SDK v5 Data Stream Protocol: text part
          // Format: '0:"text"\n'
          const dataStreamText = `0:${JSON.stringify(response)}\n`;
          if (!safeEnqueue(encoder.encode(dataStreamText))) {
            console.log('âš ï¸ Stream closed, could not send response');
            return;
          }

          // Check if response requires human approval
          const approval = detectApprovalRequired(response, query);

          if (approval.required && approval.actionType) {
            // Register in approval store (async but fire-and-forget is acceptable here)
            await approvalStore.registerPending({
              sessionId: effectiveSessionId,
              actionType: approval.actionType,
              description: approval.description || '',
              payload: { response, query },
              requestedAt: new Date(),
              requestedBy: 'reporter_agent',
            });

            // AI SDK v5 Data Stream Protocol: custom data event
            // Format: '2:[{...}]\n' for data array, or use 'data-*' pattern
            // Using '8:' prefix for custom annotation/metadata
            const approvalEvent = `8:${JSON.stringify([
              {
                type: 'approval_request',
                id: effectiveSessionId,
                actionType: approval.actionType,
                description: approval.description,
              },
            ])}\n`;
            safeEnqueue(encoder.encode(approvalEvent));

            console.log(
              `ğŸ”” [Supervisor] Approval required: ${approval.actionType}`
            );
          }

          // v5.85.0: Emit verification result as annotation
          if (verification) {
            const verificationEvent = `8:${JSON.stringify([
              {
                type: 'verification',
                isValid: verification.isValid,
                confidence: verification.confidence,
                issueCount: verification.issues.length,
                processingTimeMs: verification.metadata.processingTimeMs,
              },
            ])}\n`;
            safeEnqueue(encoder.encode(verificationEvent));

            if (verification.issues.length > 0) {
              console.log(
                `ğŸ” [Verifier] Detected ${verification.issues.length} issue(s), confidence: ${(verification.confidence * 100).toFixed(1)}%`
              );
            }
          }

          // v5.86.0: Emit compression result as annotation
          if (compressionApplied) {
            const compressionEvent = `8:${JSON.stringify([
              {
                type: 'compression',
                applied: true,
                timestamp: Date.now(),
              },
            ])}\n`;
            safeEnqueue(encoder.encode(compressionEvent));
            console.log('ğŸ—œï¸ [Compression] Context was compressed for this session');
          }
        }

        // AI SDK v5 Data Stream Protocol: finish message
        const finishMessage = `d:${JSON.stringify({
          finishReason: 'stop',
          sessionId: effectiveSessionId,
          verified: verification?.isValid ?? true,
          confidence: verification?.confidence ?? 1.0,
        })}\n`;
        safeEnqueue(encoder.encode(finishMessage));
        console.log(
          `ğŸ“¤ Supervisor completed (AI SDK v5 Protocol, Verified: ${verification?.isValid ?? 'N/A'})`
        );

        safeClose();
      } catch (error) {
        // Stop keep-alive on error
        if (keepAliveInterval) {
          clearInterval(keepAliveInterval);
          keepAliveInterval = null;
        }

        // Only log if stream is still open (avoid noise from closed streams)
        if (!isClosed) {
          console.error('âŒ Supervisor error:', error);
          const errorMessage =
            error instanceof Error ? error.message : 'Unknown error';
          const errorStream = `3:${JSON.stringify(errorMessage)}\n`;
          safeEnqueue(encoder.encode(errorStream));
        } else {
          console.log(
            'âš ï¸ Supervisor error occurred after stream closed:',
            error instanceof Error ? error.message : 'Unknown'
          );
        }
        safeClose();
      }
    },
  });
}
