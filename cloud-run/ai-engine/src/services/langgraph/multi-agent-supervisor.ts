/**
 * Multi-Agent Supervisor using @langchain/langgraph-supervisor
 * Cloud Run Standalone Implementation
 */

import { HumanMessage } from '@langchain/core/messages';
import { createReactAgent } from '@langchain/langgraph/prebuilt';
import { createSupervisor } from '@langchain/langgraph-supervisor';
import {
  analyzePatternTool,
  detectAnomaliesTool,
  predictTrendsTool,
} from '../../agents/analyst-agent';
// Import tools from Agents
import { getServerMetricsTool } from '../../agents/nlq-agent';
import {
  recommendCommandsTool,
  searchKnowledgeBaseTool,
} from '../../agents/reporter-agent';
import {
  createSessionConfig,
  getAutoCheckpointer,
} from '../../lib/checkpointer';
import {
  getAnalystModel,
  getNLQModel,
  getReporterModel,
  getSupervisorModel,
} from '../../lib/model-config';

// ============================================================================
// 1. Worker Agent Creation
// ============================================================================

/**
 * Create NLQ Agent - Server metrics queries
 */
function createNLQAgent() {
  return createReactAgent({
    llm: getNLQModel(),
    tools: [getServerMetricsTool],
    name: 'nlq_agent',
    stateModifier: `ë‹¹ì‹ ì€ OpenManager VIBEì˜ NLQ Agentì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ê°€ëŠ¥í•œ ì‘ì—…:
- ì„œë²„ ìƒíƒœ ì¡°íšŒ (CPU, Memory, Disk)
- íŠ¹ì • ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒ
- ì „ì²´ ì„œë²„ ìš”ì•½

ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•œ í›„, ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`,
  });
}

/**
 * Create Analyst Agent - Pattern analysis & anomaly detection
 */
function createAnalystAgent() {
  return createReactAgent({
    llm: getAnalystModel(),
    tools: [detectAnomaliesTool, predictTrendsTool, analyzePatternTool],
    name: 'analyst_agent',
    stateModifier: `ë‹¹ì‹ ì€ OpenManager VIBEì˜ Analyst Agentì…ë‹ˆë‹¤.
ì„œë²„ ì‹œìŠ¤í…œ íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ê°€ëŠ¥í•œ ì‘ì—…:
- ì´ìƒ íƒì§€ (detectAnomalies): í†µê³„ì  ì´ìƒì¹˜ ê°ì§€
- íŠ¸ë Œë“œ ì˜ˆì¸¡ (predictTrends): ì„ í˜• íšŒê·€ ê¸°ë°˜ ì˜ˆì¸¡
- íŒ¨í„´ ë¶„ì„ (analyzePattern): ì§ˆë¬¸ ì˜ë„ íŒŒì•…

ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
1. í˜„ì¬ ìƒíƒœ ìš”ì•½
2. ë°œê²¬ëœ íŒ¨í„´/ì´ìƒì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…
3. ì ì¬ì  ë¬¸ì œì  ë˜ëŠ” ì£¼ì˜ì‚¬í•­
4. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`,
  });
}

/**
 * Create Reporter Agent - Incident reports & RAG
 */
function createReporterAgent() {
  return createReactAgent({
    llm: getReporterModel(),
    tools: [searchKnowledgeBaseTool, recommendCommandsTool],
    name: 'reporter_agent',
    stateModifier: `ë‹¹ì‹ ì€ OpenManager VIBEì˜ Reporter Agentì…ë‹ˆë‹¤.
ì¥ì•  ë¶„ì„ ë° ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ê°€ëŠ¥í•œ ì‘ì—…:
- ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ (searchKnowledgeBase): RAG ê¸°ë°˜ ê³¼ê±° ì¥ì•  ì´ë ¥ ê²€ìƒ‰
- ëª…ë ¹ì–´ ì¶”ì²œ (recommendCommands): CLI ëª…ë ¹ì–´ ì¶”ì²œ

ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸ í˜•ì‹:
### ğŸ“‹ ì¸ì‹œë˜íŠ¸ ìš”ì•½
[ë¬¸ì œ ìƒí™© ìš”ì•½]

### ğŸ” ì›ì¸ ë¶„ì„
[ê°€ëŠ¥í•œ ì›ì¸ë“¤]

### ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜
[ë‹¨ê³„ë³„ í•´ê²° ë°©ì•ˆ]

### âŒ¨ï¸ ì¶”ì²œ ëª…ë ¹ì–´
[ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë“¤]

í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`,
  });
}

// ============================================================================
// 2. Supervisor Creation
// ============================================================================

const SUPERVISOR_PROMPT = `ë‹¹ì‹ ì€ OpenManager VIBEì˜ Multi-Agent Supervisorì…ë‹ˆë‹¤.
ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤.

## ì—ì´ì „íŠ¸ ëª©ë¡
1. **nlq_agent**: ì„œë²„ ìƒíƒœ/ë©”íŠ¸ë¦­ ì¡°íšŒ (CPU, Memory, Disk)
   - ì˜ˆ: "ì„œë²„ ìƒíƒœ", "CPU ì‚¬ìš©ë¥ ", "ë©”ëª¨ë¦¬ í™•ì¸"

2. **analyst_agent**: íŒ¨í„´ ë¶„ì„, ì´ìƒ íƒì§€, íŠ¸ë Œë“œ ì˜ˆì¸¡
   - ì˜ˆ: "ì´ìƒ ê°ì§€", "íŠ¸ë Œë“œ ë¶„ì„", "íŒ¨í„´ í™•ì¸", "ì¢…í•© ë¶„ì„"

3. **reporter_agent**: ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸, ì¥ì•  ë¶„ì„, RAG ê²€ìƒ‰
   - ì˜ˆ: "ì¥ì•  ë¶„ì„", "ì›ì¸ íŒŒì•…", "í•´ê²° ë°©ë²•", "ê³¼ê±° ì´ë ¥"

## ë¼ìš°íŒ… ê·œì¹™
- ë‹¨ìˆœ ì¡°íšŒ â†’ nlq_agent
- ë¶„ì„/ì˜ˆì¸¡ â†’ analyst_agent
- ì¥ì• /ë¦¬í¬íŠ¸ â†’ reporter_agent
- ì¸ì‚¬ë§/ì¼ë°˜ ëŒ€í™” â†’ ì§ì ‘ ì‘ë‹µ

ì ì ˆí•œ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ì—¬ ì‘ì—…ì„ ìœ„ì„í•˜ì„¸ìš”.`;

/**
 * Create Multi-Agent Supervisor Workflow
 */
export async function createMultiAgentSupervisor() {
  const checkpointer = await getAutoCheckpointer();

  // Create worker agents
  const nlqAgent = createNLQAgent();
  const analystAgent = createAnalystAgent();
  const reporterAgent = createReporterAgent();

  // Create supervisor with automatic handoffs
  const workflow = createSupervisor({
    agents: [nlqAgent, analystAgent, reporterAgent],
    llm: getSupervisorModel(),
    prompt: SUPERVISOR_PROMPT,
    outputMode: 'full_history',
  });

  // Compile with checkpointer for session persistence
  return workflow.compile({
    checkpointer,
  });
}

// ============================================================================
// 3. Execution Functions
// ============================================================================

export interface SupervisorExecutionOptions {
  sessionId?: string;
}

/**
 * Execute supervisor workflow (single response)
 */
export async function executeSupervisor(
  query: string,
  options: SupervisorExecutionOptions = {}
): Promise<{
  response: string;
  sessionId: string;
}> {
  const app = await createMultiAgentSupervisor();
  const sessionId = options.sessionId || `session_${Date.now()}`;
  const config = createSessionConfig(sessionId);

  const result = await app.invoke(
    {
      messages: [new HumanMessage(query)],
    },
    config
  );

  // Extract final response from messages
  const messages = result.messages || [];
  const lastMessage = messages[messages.length - 1];
  const response =
    typeof lastMessage?.content === 'string'
      ? lastMessage.content
      : 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';

  console.log(`âœ… [Supervisor] Completed. Session: ${sessionId}`);

  return { response, sessionId };
}

/**
 * Stream supervisor workflow
 */
export async function* streamSupervisor(
  query: string,
  options: SupervisorExecutionOptions = {}
): AsyncGenerator<{
  type: 'token' | 'agent_start' | 'agent_end' | 'final' | 'error';
  content: string;
  metadata?: Record<string, unknown>;
}> {
  const app = await createMultiAgentSupervisor();
  const sessionId = options.sessionId || `session_${Date.now()}`;
  const config = createSessionConfig(sessionId);

  try {
    const stream = await app.streamEvents(
      {
        messages: [new HumanMessage(query)],
      },
      {
        version: 'v2',
        ...config,
      }
    );

    let finalContent = '';

    for await (const event of stream) {
      // LLM token streaming
      if (event.event === 'on_chat_model_stream') {
        const chunk = event.data?.chunk;
        if (chunk?.content && typeof chunk.content === 'string') {
          finalContent += chunk.content;
          yield {
            type: 'token',
            content: chunk.content,
            metadata: { node: event.name },
          };
        }
      }

      // Agent start
      if (event.event === 'on_chain_start' && event.tags?.includes('agent')) {
        yield {
          type: 'agent_start',
          content: event.name || 'unknown_agent',
          metadata: { tags: event.tags },
        };
      }

      // Agent end
      if (event.event === 'on_chain_end' && event.tags?.includes('agent')) {
        yield {
          type: 'agent_end',
          content: event.name || 'unknown_agent',
          metadata: { output: event.data?.output },
        };
      }
    }

    // Final response
    yield {
      type: 'final',
      content: finalContent,
      metadata: { sessionId },
    };
  } catch (error) {
    yield {
      type: 'error',
      content: error instanceof Error ? error.message : String(error),
      metadata: { sessionId },
    };
  }
}

/**
 * Create AI SDK compatible streaming response using toUIMessageStream
 * This integrates LangGraph with Vercel AI SDK v5
 */
export async function createSupervisorStreamResponse(
  query: string,
  sessionId?: string
): Promise<ReadableStream<Uint8Array>> {
  const encoder = new TextEncoder();

  return new ReadableStream({
    async start(controller) {
      try {
        const generator = streamSupervisor(query, { sessionId });

        for await (const chunk of generator) {
          if (chunk.type === 'token') {
            // AI SDK v5 Data Stream Protocol: text part
            // Format: '0:"text"\n'
            const dataStreamText = `0:${JSON.stringify(chunk.content)}\n`;
            controller.enqueue(encoder.encode(dataStreamText));
          } else if (chunk.type === 'final') {
            // AI SDK v5 Data Stream Protocol: finish message
            // Format: 'd:{"finishReason":"stop"}\n'
            const finishMessage = `d:${JSON.stringify({ finishReason: 'stop' })}\n`;
            controller.enqueue(encoder.encode(finishMessage));
            console.log('ğŸ“¤ Supervisor stream completed (AI SDK v5 Protocol)');
          } else if (chunk.type === 'error') {
            // AI SDK v5 Data Stream Protocol: error
            // Format: '3:"error message"\n'
            const errorMessage = `3:${JSON.stringify(chunk.content)}\n`;
            controller.enqueue(encoder.encode(errorMessage));
          }
        }

        controller.close();
      } catch (error) {
        console.error('âŒ Supervisor streaming error:', error);
        const errorMessage =
          error instanceof Error ? error.message : 'Unknown error';
        const errorStream = `3:${JSON.stringify(errorMessage)}\n`;
        controller.enqueue(encoder.encode(errorStream));
        controller.close();
      }
    },
  });
}
