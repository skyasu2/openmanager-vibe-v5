/**
 * ğŸ¤– ì‹¤ì œ AI ì²˜ë¦¬ ì„œë¹„ìŠ¤
 *
 * ê¸°ìˆ  ìŠ¤íƒ:
 * - ê¸°ë³¸: ë¡œì»¬ MCP ì—”ì§„ìœ¼ë¡œ LLM ì—†ì´ ë™ì‘
 * - ì°¨í›„ ê°œë°œ: OpenAI GPT-3.5-turbo ì—°ë™ (ì˜ˆì •)
 * - ì°¨í›„ ê°œë°œ: Google Gemini ì—°ë™ (ì˜ˆì •)
 * - ì°¨í›„ ê°œë°œ: Anthropic Claude ì—°ë™ (ì˜ˆì •)
 * - Redis ìºì‹±
 * - Render Python ì„œë²„ ì—°ë™
 */

import { z } from 'zod';
import { getRedisClient } from '@/lib/redis';

// AI ì‘ë‹µ ìŠ¤í‚¤ë§ˆ
const AIResponseSchema = z.object({
  intent: z.enum(['status_check', 'troubleshooting', 'analysis', 'recommendation', 'prediction']),
  confidence: z.number().min(0).max(1),
  summary: z.string(),
  details: z.array(z.string()),
  actions: z.array(z.string()),
  urgency: z.enum(['low', 'medium', 'high', 'critical'])
});

const SystemAnalysisSchema = z.object({
  status: z.enum(['healthy', 'warning', 'critical']),
  issues: z.array(z.object({
    type: z.string(),
    severity: z.enum(['low', 'medium', 'high', 'critical']),
    description: z.string(),
    solution: z.string()
  })),
  metrics: z.object({
    cpu: z.number(),
    memory: z.number(),
    disk: z.number(),
    network: z.number().optional()
  }),
  recommendations: z.array(z.string())
});

export interface AIProcessingRequest {
  query: string;
  context?: {
    serverMetrics?: any[];
    logEntries?: any[];
    systemState?: any;
  };
  options?: {
    model?: 'gpt-3.5-turbo' | 'claude-3-haiku' | 'gemini-1.5-flash';
    temperature?: number;
    maxTokens?: number;
    useCache?: boolean;
    usePython?: boolean;
  };
}

export interface AIProcessingResponse {
  success: boolean;
  intent: string;
  confidence: number;
  summary: string;
  details: string[];
  actions: string[];
  urgency: string;
  processingTime: number;
  model: string;
  cached: boolean;
  pythonAnalysis?: any;
}

export class RealAIProcessor {
  private static instance: RealAIProcessor | null = null;
  private redis: any;
  private pythonServiceUrl: string;
  private enabledModels: string[] = [];

  private constructor() {
    // Render Python ì„œë²„ URL (ë¬´ë£Œ tier)
    this.pythonServiceUrl = process.env.PYTHON_SERVICE_URL || 'https://openmanager-ai-python.onrender.com';
    
    // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ í™•ì¸
    this.initializeModels();
  }

  public static getInstance(): RealAIProcessor {
    if (!RealAIProcessor.instance) {
      RealAIProcessor.instance = new RealAIProcessor();
    }
    return RealAIProcessor.instance;
  }

  /**
   * ğŸš€ ëª¨ë¸ ì´ˆê¸°í™”
   */
  private async initializeModels(): Promise<void> {
    try {
      this.redis = await getRedisClient();
      
      // API í‚¤ê°€ ìˆëŠ” ëª¨ë¸ë“¤ë§Œ í™œì„±í™”
      if (process.env.OPENAI_API_KEY) {
        this.enabledModels.push('gpt-3.5-turbo');
      }
      if (process.env.ANTHROPIC_API_KEY) {
        this.enabledModels.push('claude-3-haiku');
      }
      if (process.env.GOOGLE_GENERATIVE_AI_API_KEY) {
        this.enabledModels.push('gemini-1.5-flash');
      }

      console.log(`âœ… AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: ${this.enabledModels.join(', ')}`);
    } catch (error) {
      console.warn('âš ï¸ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      // í™˜ê²½ ì„¤ì •ì´ ì—†ì–´ë„ ê¸°ë³¸ ê¸°ëŠ¥ì€ ì œê³µ
    }
  }

  /**
   * ğŸ§  ì‹¤ì œ AI ì¿¼ë¦¬ ì²˜ë¦¬
   */
  public async processQuery(request: AIProcessingRequest): Promise<AIProcessingResponse> {
    const startTime = Date.now();
    
    try {
      // ìºì‹œ í™•ì¸
      if (request.options?.useCache !== false) {
        const cached = await this.getCachedResponse(request.query);
        if (cached) {
          return {
            ...cached,
            processingTime: Date.now() - startTime,
            cached: true
          };
        }
      }

      // ëª¨ë¸ ì„ íƒ
      const model = this.selectBestModel(request.options?.model);
      
      // AI ì²˜ë¦¬ ìˆ˜í–‰
      let aiResponse: AIProcessingResponse;
      
      if (this.enabledModels.length > 0) {
        aiResponse = await this.processWithAI(request, model);
      } else {
        aiResponse = await this.processWithLocalAI(request);
      }

      // Python ë¶„ì„ ì¶”ê°€ (ì„ íƒì )
      if (request.options?.usePython) {
        aiResponse.pythonAnalysis = await this.getPythonAnalysis(request);
      }

      aiResponse.processingTime = Date.now() - startTime;
      aiResponse.cached = false;

      // ìºì‹œ ì €ì¥
      if (request.options?.useCache !== false) {
        await this.setCachedResponse(request.query, aiResponse);
      }

      return aiResponse;

    } catch (error) {
      console.error('âŒ AI ì²˜ë¦¬ ì‹¤íŒ¨:', error);
      return this.createFallbackResponse(request, Date.now() - startTime, error);
    }
  }

  /**
   * ğŸ¤– ì‹¤ì œ AI ëª¨ë¸ë¡œ ì²˜ë¦¬
   */
  private async processWithAI(request: AIProcessingRequest, model: string): Promise<AIProcessingResponse> {
    const systemPrompt = this.buildSystemPrompt(request.context);
    const userPrompt = this.buildUserPrompt(request.query, request.context);

    const jsonPrompt = `${systemPrompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: ${userPrompt}\n\n` +
      'ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”: ' +
      '{"intent":"","confidence":0,"summary":"","details":[],"actions":[],"urgency":""}';

    try {
      const raw = await this.callModelAPI(
        model,
        jsonPrompt,
        request.options?.temperature || 0.7,
        request.options?.maxTokens || 1000
      );
      const object = AIResponseSchema.parse(JSON.parse(raw));
      return {
        success: true,
        intent: object.intent,
        confidence: object.confidence,
        summary: object.summary,
        details: object.details,
        actions: object.actions,
        urgency: object.urgency,
        processingTime: 0,
        model,
        cached: false
      };
    } catch (error) {
      console.warn(`âš ï¸ ${model} ì²˜ë¦¬ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ìƒì„±ìœ¼ë¡œ ëŒ€ì²´:`, error);
      const text = await this.callModelAPI(
        model,
        `${systemPrompt}\n\nì‚¬ìš©ì ì§ˆë¬¸: ${userPrompt}\n\ní•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.`,
        request.options?.temperature || 0.7,
        request.options?.maxTokens || 800
      );
      return this.parseTextResponse(text, model);
    }
  }

  /**
   * ğŸ› ï¸ ë¡œì»¬ AI ì²˜ë¦¬ (API í‚¤ ì—†ì„ ë•Œ)
   */
  private async processWithLocalAI(request: AIProcessingRequest): Promise<AIProcessingResponse> {
    console.log('ğŸ› ï¸ ë¡œì»¬ AI ì²˜ë¦¬ ëª¨ë“œ');
    
    // í‚¤ì›Œë“œ ê¸°ë°˜ intent ë¶„ë¥˜
    const intent = this.classifyIntentLocal(request.query);
    
    // ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„
    const analysis = this.analyzeContextLocal(request.context, intent);
    
    return {
      success: true,
      intent: intent.type,
      confidence: intent.confidence,
      summary: analysis.summary,
      details: analysis.details,
      actions: analysis.actions,
      urgency: analysis.urgency,
      processingTime: 0,
      model: 'local-analyzer',
      cached: false
    };
  }

  /**
   * ğŸ Python ì„œë²„ ë¶„ì„
   */
  private async getPythonAnalysis(request: AIProcessingRequest): Promise<any> {
    try {
      const response = await fetch(`${this.pythonServiceUrl}/analyze`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          query: request.query,
          metrics: request.context?.serverMetrics || [],
          logs: request.context?.logEntries || []
        }),
        signal: AbortSignal.timeout(10000) // 10ì´ˆ íƒ€ì„ì•„ì›ƒ
      });

      if (!response.ok) {
        throw new Error(`Python ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: ${response.status}`);
      }

      return await response.json();

    } catch (error) {
      console.warn('âš ï¸ Python ë¶„ì„ ì‹¤íŒ¨:', error);
      return {
        status: 'unavailable',
        message: 'Python ë¶„ì„ ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'
      };
    }
  }

  /**
   * ğŸ”§ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
   */
  private selectBestModel(preferred?: string): string {
    if (preferred && this.enabledModels.includes(preferred)) {
      return preferred;
    }
    
    // ê¸°ë³¸ ìš°ì„ ìˆœìœ„: GPT-3.5 > Claude > Gemini
    const priority = ['gpt-3.5-turbo', 'claude-3-haiku', 'gemini-1.5-flash'];
    
    for (const model of priority) {
      if (this.enabledModels.includes(model)) {
        return model;
      }
    }
    
    return 'local-analyzer';
  }

  private async callModelAPI(
    model: string,
    prompt: string,
    temperature: number,
    maxTokens: number
  ): Promise<string> {
    const controller = new AbortController();
    const options = { signal: controller.signal } as RequestInit;
    setTimeout(() => controller.abort(), 20000);

    let url = '';
    let headers: Record<string, string> = { 'Content-Type': 'application/json' };
    let body: any = {};

    if (model === 'gpt-3.5-turbo') {
      url = 'https://api.openai.com/v1/chat/completions';
      headers['Authorization'] = `Bearer ${process.env.OPENAI_API_KEY}`;
      body = {
        model,
        messages: [{ role: 'user', content: prompt }],
        temperature,
        max_tokens: maxTokens
      };
    } else if (model === 'claude-3-haiku') {
      url = 'https://api.anthropic.com/v1/messages';
      headers['x-api-key'] = process.env.ANTHROPIC_API_KEY || '';
      headers['anthropic-version'] = '2023-06-01';
      body = {
        model: 'claude-3-haiku-20250630',
        messages: [{ role: 'user', content: prompt }],
        temperature,
        max_tokens: maxTokens
      };
    } else if (model === 'gemini-1.5-flash') {
      url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${process.env.GOOGLE_GENERATIVE_AI_API_KEY}`;
      body = {
        contents: [{ parts: [{ text: prompt }] }],
        generationConfig: { temperature, maxOutputTokens: maxTokens }
      };
    } else {
      throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: ${model}`);
    }

    options.method = 'POST';
    options.headers = headers;
    options.body = JSON.stringify(body);

    const res = await fetch(url, options);
    if (!res.ok) {
      throw new Error(`${model} API error: ${res.status}`);
    }
    const data = await res.json();

    if (model === 'gpt-3.5-turbo') {
      return data.choices[0].message.content as string;
    }
    if (model === 'claude-3-haiku') {
      return data.content[0].text as string;
    }
    // gemini
    return (data.candidates?.[0]?.content?.parts || [])
      .map((p: any) => p.text)
      .join('');
  }

  private buildSystemPrompt(context?: any): string {
    return `ë‹¹ì‹ ì€ ì„œë²„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ AI ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ì£¼ìš” ì—­í• :
1. ì„œë²„ ìƒíƒœ ë¶„ì„ ë° ì§„ë‹¨
2. ì„±ëŠ¥ ì´ìŠˆ ì‹ë³„ ë° í•´ê²°ë°©ì•ˆ ì œì‹œ
3. ì‹œìŠ¤í…œ ìµœì í™” ê¶Œì¥ì‚¬í•­ ì œê³µ
4. ì¥ì•  ì˜ˆì¸¡ ë° ì˜ˆë°©ì±… ì œì•ˆ

ì‘ë‹µ ê·œì¹™:
- í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€
- ê¸°ìˆ ì  ì •í™•ì„± í™•ë³´
- ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±… ì œì‹œ
- ìš°ì„ ìˆœìœ„ë³„ ì •ë¦¬

ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
${context ? JSON.stringify(context, null, 2) : 'ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì—†ìŒ'}`;
  }

  private buildUserPrompt(query: string, context?: any): string {
    let prompt = query;
    
    if (context?.serverMetrics?.length > 0) {
      prompt += `\n\nì„œë²„ ë©”íŠ¸ë¦­ ì •ë³´:\n${JSON.stringify(context.serverMetrics.slice(-3), null, 2)}`;
    }
    
    if (context?.logEntries?.length > 0) {
      prompt += `\n\nìµœê·¼ ë¡œê·¸ ì •ë³´:\n${JSON.stringify(context.logEntries.slice(-5), null, 2)}`;
    }
    
    return prompt;
  }

  private classifyIntentLocal(query: string): { type: string; confidence: number } {
    const lowercaseQuery = query.toLowerCase();
    
    // ìƒíƒœ í™•ì¸
    if (lowercaseQuery.includes('ìƒíƒœ') || lowercaseQuery.includes('status') || 
        lowercaseQuery.includes('ì–´ë•Œ') || lowercaseQuery.includes('ê´œì°®')) {
      return { type: 'status_check', confidence: 0.9 };
    }
    
    // ë¬¸ì œ í•´ê²°
    if (lowercaseQuery.includes('ë¬¸ì œ') || lowercaseQuery.includes('ì˜¤ë¥˜') || 
        lowercaseQuery.includes('error') || lowercaseQuery.includes('ì¥ì• ')) {
      return { type: 'troubleshooting', confidence: 0.95 };
    }
    
    // ë¶„ì„ ìš”ì²­
    if (lowercaseQuery.includes('ë¶„ì„') || lowercaseQuery.includes('analyze') || 
        lowercaseQuery.includes('ë³´ì—¬ì¤˜') || lowercaseQuery.includes('ì•Œë ¤ì¤˜')) {
      return { type: 'analysis', confidence: 0.85 };
    }
    
    // ì˜ˆì¸¡
    if (lowercaseQuery.includes('ì˜ˆì¸¡') || lowercaseQuery.includes('predict') || 
        lowercaseQuery.includes('ì•ìœ¼ë¡œ') || lowercaseQuery.includes('í–¥í›„')) {
      return { type: 'prediction', confidence: 0.8 };
    }
    
    return { type: 'recommendation', confidence: 0.6 };
  }

  private analyzeContextLocal(context?: any, intent?: any): any {
    const analysis = {
      summary: 'ì‹œìŠ¤í…œ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.',
      details: [] as string[],
      actions: [] as string[],
      urgency: 'medium' as 'low' | 'medium' | 'high' | 'critical'
    };

    // ë©”íŠ¸ë¦­ ë¶„ì„
    if (context?.serverMetrics?.length > 0) {
      const latest = context.serverMetrics[context.serverMetrics.length - 1];
      
      if (latest.cpu > 80) {
        analysis.details.push(`CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: ${latest.cpu}%`);
        analysis.actions.push('CPU ì‚¬ìš©ë¥ ì´ ë†’ì€ í”„ë¡œì„¸ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”');
        analysis.urgency = 'high';
      }
      
      if (latest.memory > 85) {
        analysis.details.push(`ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤: ${latest.memory}%`);
        analysis.actions.push('ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°€ëŠ¥ì„±ì„ ì ê²€í•˜ì„¸ìš”');
        analysis.urgency = 'high';
      }
      
      if (latest.disk > 90) {
        analysis.details.push(`ë””ìŠ¤í¬ ì‚¬ìš©ë¥ ì´ ìœ„í—˜í•©ë‹ˆë‹¤: ${latest.disk}%`);
        analysis.actions.push('ë””ìŠ¤í¬ ì •ë¦¬ë¥¼ ì¦‰ì‹œ ìˆ˜í–‰í•˜ì„¸ìš”');
        analysis.urgency = 'critical';
      }
    }

    // ë¡œê·¸ ë¶„ì„
    if (context?.logEntries?.length > 0) {
      const errorLogs = context.logEntries.filter((log: any) => log.level === 'ERROR');
      if (errorLogs.length > 0) {
        analysis.details.push(`${errorLogs.length}ê°œì˜ ì—ëŸ¬ ë¡œê·¸ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤`);
        analysis.actions.push('ì—ëŸ¬ ë¡œê·¸ë¥¼ ìì„¸íˆ í™•ì¸í•˜ì„¸ìš”');
      }
    }

    if (analysis.details.length === 0) {
      analysis.details.push('ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤');
      analysis.actions.push('ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”');
      analysis.urgency = 'low';
    }

    return analysis;
  }

  private parseTextResponse(text: string, model: string): AIProcessingResponse {
    // í…ìŠ¤íŠ¸ì—ì„œ ì˜ë„ì™€ ë‚´ìš© ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
    const lines = text.split('\n').filter(line => line.trim());
    
    return {
      success: true,
      intent: 'analysis',
      confidence: 0.7,
      summary: lines[0] || text.substring(0, 100),
      details: lines.slice(1, 4),
      actions: lines.slice(-2) || ['ì¶”ê°€ ëª¨ë‹ˆí„°ë§ ê¶Œì¥'],
      urgency: 'medium',
      processingTime: 0,
      model,
      cached: false
    };
  }

  private createFallbackResponse(request: AIProcessingRequest, processingTime: number, error: any): AIProcessingResponse {
    return {
      success: false,
      intent: 'error',
      confidence: 0,
      summary: `ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`,
      details: ['ì‹œìŠ¤í…œì´ ì¼ì‹œì ìœ¼ë¡œ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤'],
      actions: ['ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”', 'ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”'],
      urgency: 'medium',
      processingTime,
      model: 'fallback',
      cached: false
    };
  }

  private async getCachedResponse(query: string): Promise<AIProcessingResponse | null> {
    try {
      if (!this.redis) return null;
      
      const cacheKey = `ai:response:${Buffer.from(query).toString('base64').substring(0, 50)}`;
      const cached = await this.redis.get(cacheKey);
      
      return cached ? JSON.parse(cached) : null;
    } catch (error) {
      console.warn('âš ï¸ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨:', error);
      return null;
    }
  }

  private async setCachedResponse(query: string, response: AIProcessingResponse): Promise<void> {
    try {
      if (!this.redis) return;
      
      const cacheKey = `ai:response:${Buffer.from(query).toString('base64').substring(0, 50)}`;
      await this.redis.setex(cacheKey, 300, JSON.stringify(response)); // 5ë¶„ ìºì‹œ
    } catch (error) {
      console.warn('âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨:', error);
    }
  }

  /**
   * ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ ë¶„ì„
   */
  public async analyzeSystemMetrics(metrics: any[]): Promise<any> {
    if (metrics.length === 0) return { status: 'no_data' };

    const request: AIProcessingRequest = {
      query: 'ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”',
      context: { serverMetrics: metrics },
      options: { model: 'gpt-3.5-turbo', useCache: true }
    };

    return await this.processQuery(request);
  }

  /**
   * ğŸ” ë¡œê·¸ ë¶„ì„
   */
  public async analyzeLogs(logs: any[]): Promise<any> {
    if (logs.length === 0) return { status: 'no_data' };

    const request: AIProcessingRequest = {
      query: 'ë¡œê·¸ ì—”íŠ¸ë¦¬ë¥¼ ë¶„ì„í•´ì„œ ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”',
      context: { logEntries: logs },
      options: { model: 'claude-3-haiku', useCache: true }
    };

    return await this.processQuery(request);
  }

  /**
   * ğŸ¯ ìŠ¤ë§ˆíŠ¸ ê¶Œì¥ì‚¬í•­
   */
  public async generateRecommendations(context: any): Promise<string[]> {
    const request: AIProcessingRequest = {
      query: 'í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œì„  ê¶Œì¥ì‚¬í•­ì„ ì œì‹œí•´ì£¼ì„¸ìš”',
      context,
      options: { model: 'gemini-1.5-flash', useCache: false }
    };

    const response = await this.processQuery(request);
    return response.actions;
  }

  /**
   * ğŸ“ˆ ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
   */
  public calculatePerformanceScore(metrics: any[]): number {
    if (metrics.length === 0) return 0;

    const latest = metrics[metrics.length - 1];
    const cpuScore = Math.max(0, (100 - latest.cpu) / 100);
    const memoryScore = Math.max(0, (100 - latest.memory) / 100);
    const diskScore = Math.max(0, (100 - latest.disk) / 100);

    return Math.round((cpuScore + memoryScore + diskScore) / 3 * 100);
  }

  /**
   * ğŸ¥ í—¬ìŠ¤ì²´í¬
   */
  public async healthCheck(): Promise<any> {
    return {
      status: 'healthy',
      enabledModels: this.enabledModels,
      redisConnected: !!this.redis,
      pythonServiceUrl: this.pythonServiceUrl,
      timestamp: new Date().toISOString()
    };
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const realAIProcessor = RealAIProcessor.getInstance(); 