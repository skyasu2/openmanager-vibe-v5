/**
 * ğŸ¯ OpenManager Vibe v5 - ë§ˆìŠ¤í„° AI ì—”ì§„ v4.0.0
 *
 * ëª¨ë“  AI ì—”ì§„ì„ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” í†µí•© ì¸í„°í˜ì´ìŠ¤:
 * - 6ê°œ ì˜¤í”ˆì†ŒìŠ¤ ì—”ì§„ (simple-statistics, tensorflow.js, ë“±)
 * - 5ê°œ ì»¤ìŠ¤í…€ ì—”ì§„ (MCP Query, Hybrid, Unified, ë“±)
 * - ì—”ì§„ë³„ ë¼ìš°íŒ… ë° í´ë°± ë¡œì§
 * - ì„±ëŠ¥ ìµœì í™” ë° ì§€ì—° ë¡œë”©
 * - ì‚¬ê³ ê³¼ì • ë¡œê·¸ ì‹œìŠ¤í…œ í†µí•©
 * - ì¤‘ì•™ ë²„ì „ ê´€ë¦¬ ë° ë³€ê²½ ë¡œê¹…
 * - Vercel ë¬´ë£Œ í‹°ì–´ ìµœì í™” (ì‹¤ì œ ë™ì‘ ìœ ì§€)
 */

import { AI_ENGINE_VERSIONS, VersionManager } from '../../config/versions';
import { AIThinkingStep } from '../../types/ai-thinking';
import { PerformanceMonitor } from '../../utils/performance-monitor';
import { correlationEngine } from './engines/CorrelationEngine';
import { CustomEngines } from './engines/CustomEngines';
import { OpenSourceEngines } from './engines/OpenSourceEngines';
import { aiLogger, LogCategory, LogLevel } from './logging/AILogger';

// ğŸš€ Vercel ìµœì í™” ì„¤ì • (ì‹¤ì œ ë™ì‘ ìœ ì§€)
const VERCEL_OPTIMIZATION = {
  isProduction: process.env.NODE_ENV === 'production',
  isVercel: process.env.VERCEL === '1',
  responseTimeout: process.env.VERCEL === '1' ? 8000 : 30000, // íƒ€ì„ì•„ì›ƒë§Œ ì œí•œ
  enableCaching: true, // ìºì‹± í™œì„±í™”ë¡œ ì„±ëŠ¥ ìµœì í™”
  logLevel: process.env.VERCEL === '1' ? 'warn' : 'info', // ë¡œê·¸ ë ˆë²¨ ì œí•œ
};

export interface AIEngineRequest {
  engine:
    | 'anomaly'
    | 'prediction'
    | 'autoscaling'
    | 'korean'
    | 'enhanced'
    | 'integrated'
    | 'mcp'
    | 'mcp-test'
    | 'hybrid'
    | 'unified'
    | 'custom-nlp'
    | 'correlation';
  query: string;
  data?: any;
  context?: any;
  options?: {
    prefer_mcp?: boolean;
    fallback_enabled?: boolean;
    use_cache?: boolean;
    enable_thinking_log?: boolean;
    steps?: number; // prediction ì—”ì§„ìš©
    fuzzyThreshold?: number; // enhanced ê²€ìƒ‰ìš©
    exactWeight?: number;
    fields?: string[];
  };
}

export interface AIEngineResponse {
  success: boolean;
  result: any;
  engine_used: string;
  response_time: number;
  confidence: number;
  fallback_used: boolean;
  cache_hit?: boolean;
  error?: string;
  thinking_process?: AIThinkingStep[];
  reasoning_steps?: string[];
  performance?: {
    memoryUsage?: any;
    cacheHit?: boolean;
    memoryDelta?: number;
  };
}

export interface EngineStatus {
  name: string;
  status: 'ready' | 'loading' | 'error' | 'disabled';
  last_used: number;
  success_rate: number;
  avg_response_time: number;
  memory_usage: string;
}

export class MasterAIEngine {
  private openSourceEngines!: OpenSourceEngines;
  private customEngines!: CustomEngines;
  private engineStats: Map<
    string,
    { calls: number; successes: number; totalTime: number; lastUsed: number }
  >;
  private responseCache: Map<
    string,
    { result: any; timestamp: number; ttl: number }
  >;
  private initialized = false;

  constructor() {
    this.engineStats = new Map();
    this.responseCache = new Map();
    this.initializeEngines();
  }

  private async initializeEngines() {
    const startTime = Date.now();

    try {
      await aiLogger.logAI({
        level: LogLevel.INFO,
        category: LogCategory.AI_ENGINE,
        engine: 'MasterAIEngine',
        message: 'ğŸš€ MasterAIEngine ì´ˆê¸°í™” ì‹œì‘...',
      });

      // ì˜¤í”ˆì†ŒìŠ¤ ì—”ì§„ ì´ˆê¸°í™”
      this.openSourceEngines = new OpenSourceEngines();

      // ì»¤ìŠ¤í…€ ì—”ì§„ ì´ˆê¸°í™” (ì˜¤í”ˆì†ŒìŠ¤ ì—”ì§„ ì˜ì¡´ì„± ì£¼ì…)
      this.customEngines = new CustomEngines(this.openSourceEngines);

      // ì—”ì§„ í†µê³„ ì´ˆê¸°í™”
      this.initializeEngineStats();

      this.initialized = true;

      const initTime = Date.now() - startTime;
      await aiLogger.logPerformance(
        'MasterAIEngine',
        LogCategory.AI_ENGINE,
        'initialization',
        initTime,
        {
          openSourceEnginesLoaded: true,
          customEnginesLoaded: true,
          statsInitialized: true,
        }
      );

      await aiLogger.logAI({
        level: LogLevel.INFO,
        category: LogCategory.AI_ENGINE,
        engine: 'MasterAIEngine',
        message: `âœ… MasterAIEngine ì´ˆê¸°í™” ì™„ë£Œ (${initTime}ms)`,
      });

      // ì„±ëŠ¥ ì •ë³´ ë¡œê¹…
      this.logPerformanceInfo();
    } catch (error) {
      await aiLogger.logError(
        'MasterAIEngine',
        LogCategory.AI_ENGINE,
        error as Error,
        {
          stage: 'initialization',
          components: ['OpenSourceEngines', 'CustomEngines'],
        }
      );
      this.initialized = false;
    }
  }

  private initializeEngineStats() {
    const engines = [
      'anomaly',
      'prediction',
      'autoscaling',
      'korean',
      'enhanced',
      'integrated',
      'mcp',
      'mcp-test',
      'hybrid',
      'unified',
      'custom-nlp',
    ];

    engines.forEach(engine => {
      this.engineStats.set(engine, {
        calls: 0,
        successes: 0,
        totalTime: 0,
        lastUsed: 0,
      });
    });
  }

  /**
   * ğŸ¯ ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì„œë“œ (ì‚¬ê³ ê³¼ì • ë¡œê·¸ + ì„±ëŠ¥ ì¸¡ì • í†µí•©)
   */
  async query(request: AIEngineRequest): Promise<AIEngineResponse> {
    const startTime = Date.now();
    const thinkingSteps: AIThinkingStep[] = [];

    // ğŸš€ Vercel ìµœì í™”: íƒ€ì„ì•„ì›ƒ ì„¤ì •
    const timeout = VERCEL_OPTIMIZATION.responseTimeout;

    // ì‚¬ê³ ê³¼ì • ë¡œê·¸ í™œì„±í™” ì—¬ë¶€
    const enableThinking = request.options?.enable_thinking_log !== false;

    // ğŸ” ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
    const memoryBefore = PerformanceMonitor.getMemoryUsage();

    if (enableThinking) {
      thinkingSteps.push(
        this.createThinkingStep(
          'analyzing',
          'ğŸ” ìš”ì²­ ë¶„ì„ ì¤‘...',
          `ì—”ì§„: ${request.engine}, ì¿¼ë¦¬: ${request.query.substring(0, 50)}...`
        )
      );
    }

    try {
      // ğŸš€ Vercel ìµœì í™”: íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì‹¤ì œ ì—”ì§„ ì‹¤í–‰
      const queryPromise = this.executeActualQuery(
        request,
        thinkingSteps,
        enableThinking
      );

      if (VERCEL_OPTIMIZATION.isVercel) {
        // Vercel í™˜ê²½ì—ì„œëŠ” íƒ€ì„ì•„ì›ƒ ì ìš©
        const timeoutPromise = new Promise<never>((_, reject) => {
          setTimeout(() => reject(new Error('AI Engine Timeout')), timeout);
        });

        const result = await Promise.race([queryPromise, timeoutPromise]);
        return result;
      } else {
        // ê°œë°œ í™˜ê²½ì—ì„œëŠ” íƒ€ì„ì•„ì›ƒ ì—†ì´ ì‹¤í–‰
        return await queryPromise;
      }
    } catch (error) {
      // ì—ëŸ¬ ë°œìƒ ì‹œ í´ë°± ì²˜ë¦¬
      return await this.handleQueryError(
        request,
        error as Error,
        startTime,
        thinkingSteps
      );
    }
  }

  /**
   * ğŸ¯ ì‹¤ì œ ì¿¼ë¦¬ ì‹¤í–‰ (ì›ë³¸ ë¡œì§ ë³µì›)
   */
  private async executeActualQuery(
    request: AIEngineRequest,
    thinkingSteps: AIThinkingStep[],
    enableThinking: boolean
  ): Promise<AIEngineResponse> {
    const startTime = Date.now();
    const memoryBefore = PerformanceMonitor.getMemoryUsage();

    // ìºì‹œ í™•ì¸
    if (request.options?.use_cache !== false) {
      if (enableThinking) {
        thinkingSteps.push(
          this.createThinkingStep(
            'processing',
            'ìºì‹œ í™•ì¸',
            'ì´ì „ ê²°ê³¼ ìºì‹œì—ì„œ ê²€ìƒ‰ ì¤‘'
          )
        );
      }

      const cached = this.checkCache(request);
      if (cached) {
        if (enableThinking) {
          thinkingSteps.push(
            this.createThinkingStep(
              'completed',
              'ìºì‹œ ì ì¤‘',
              'ìºì‹œëœ ê²°ê³¼ ë°˜í™˜'
            )
          );
        }

        // ğŸ“Š ìºì‹œ ì„±ëŠ¥ ê¸°ë¡
        const responseTime = Date.now() - startTime;
        this.updateEngineStats(request.engine, responseTime, true);

        return {
          success: true,
          result: cached.result,
          engine_used: request.engine,
          response_time: responseTime,
          confidence: cached.result.confidence || 0.8,
          fallback_used: false,
          cache_hit: true,
          thinking_process: thinkingSteps,
          performance: {
            memoryUsage: PerformanceMonitor.getMemoryUsage(),
            cacheHit: true,
            memoryDelta: 0,
          },
        };
      }
    }

    if (enableThinking) {
      thinkingSteps.push(
        this.createThinkingStep(
          'processing',
          'ì—”ì§„ ì‹¤í–‰',
          `${request.engine} ì—”ì§„ ì²˜ë¦¬ ì¤‘`
        )
      );
    }

    // ì—”ì§„ë³„ ë¼ìš°íŒ…
    const result = await this.routeToEngine(request);

    if (enableThinking) {
      thinkingSteps.push(
        this.createThinkingStep(
          'reasoning',
          'ê²°ê³¼ ë¶„ì„',
          `ì‹ ë¢°ë„ ${((result.confidence || 0.7) * 100).toFixed(1)}%ë¡œ ì²˜ë¦¬ ì™„ë£Œ`
        )
      );
    }

    // í†µê³„ ì—…ë°ì´íŠ¸
    this.updateEngineStats(request.engine, Date.now() - startTime, true);

    // ìºì‹œ ì €ì¥
    if (request.options?.use_cache !== false) {
      this.saveToCache(request, result);
    }

    if (enableThinking) {
      thinkingSteps.push(
        this.createThinkingStep(
          'completed',
          'ì‘ë‹µ ì™„ë£Œ',
          'ê²°ê³¼ ë°˜í™˜ ë° ìºì‹œ ì €ì¥ ì™„ë£Œ'
        )
      );
    }

    // ğŸ“Š ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ
    const memoryAfter = PerformanceMonitor.getMemoryUsage();
    const memoryDelta = memoryAfter.rss - memoryBefore.rss;

    return {
      success: true,
      result,
      engine_used: request.engine,
      response_time: Date.now() - startTime,
      confidence: result.confidence || 0.7,
      fallback_used: false,
      thinking_process: thinkingSteps,
      reasoning_steps:
        result.reasoning_steps ||
        this.generateReasoningSteps(request.engine, request.query),
      performance: {
        memoryUsage: memoryAfter,
        cacheHit: false,
        memoryDelta,
      },
    };
  }

  /**
   * ğŸ”€ ì—”ì§„ë³„ ë¼ìš°íŒ… (í´ë°± ì—†ìŒ)
   */
  private async routeToEngine(request: AIEngineRequest): Promise<any> {
    switch (request.engine) {
      // ì˜¤í”ˆì†ŒìŠ¤ ì—”ì§„ë“¤
      case 'anomaly':
        if (!Array.isArray(request.data)) {
          throw new Error('ì´ìƒ íƒì§€ì—ëŠ” ìˆ«ì ë°°ì—´ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤');
        }
        return await this.openSourceEngines.detectAnomalies(request.data);

      case 'prediction':
        if (!Array.isArray(request.data)) {
          throw new Error('ì˜ˆì¸¡ì—ëŠ” ì‹œê³„ì—´ ë°ì´í„° ë°°ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤');
        }
        return await this.openSourceEngines.predictTimeSeries(
          request.data,
          request.options?.steps || 5
        );

      case 'autoscaling':
        if (!request.data?.cpuUsage && !request.data?.memoryUsage) {
          throw new Error('ìë™ ìŠ¤ì¼€ì¼ë§ì—ëŠ” ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤');
        }
        return await this.openSourceEngines.calculateAutoScaling(
          request.data,
          request.context?.currentServers || 5
        );

      case 'korean':
        return await this.openSourceEngines.processKorean(
          request.query,
          request.data
        );

      case 'enhanced':
        // ê²€ìƒ‰ ëŒ€ìƒ ë°°ì—´ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ì„œë²„ ë°ì´í„° ì‚¬ìš©
        if (!Array.isArray(request.data)) {
          // ê¸°ë³¸ ê²€ìƒ‰ ëŒ€ìƒ ë°ì´í„° ìƒì„±
          const defaultSearchData = [
            {
              id: 'server-1',
              name: 'ì›¹ì„œë²„-01',
              status: 'running',
              cpu: 45,
              memory: 60,
            },
            {
              id: 'server-2',
              name: 'ë°ì´í„°ë² ì´ìŠ¤-01',
              status: 'warning',
              cpu: 78,
              memory: 85,
            },
            {
              id: 'server-3',
              name: 'APIì„œë²„-01',
              status: 'running',
              cpu: 32,
              memory: 45,
            },
            {
              id: 'server-4',
              name: 'ìºì‹œì„œë²„-01',
              status: 'running',
              cpu: 25,
              memory: 30,
            },
            {
              id: 'server-5',
              name: 'ë¡œë“œë°¸ëŸ°ì„œ-01',
              status: 'running',
              cpu: 15,
              memory: 20,
            },
          ];
          request.data = defaultSearchData;
        }
        return await this.openSourceEngines.hybridSearch(
          request.data,
          request.query,
          request.options || {}
        );

      case 'integrated':
        return await this.openSourceEngines.advancedNLP(request.query);

      // ì»¤ìŠ¤í…€ ì—”ì§„ë“¤
      case 'mcp':
        return await this.customEngines.mcpQuery(
          request.query,
          request.context
        );

      case 'mcp-test':
        return await this.customEngines.mcpTest();

      case 'hybrid':
        return await this.customEngines.hybridAnalysis(
          request.query,
          request.data
        );

      case 'unified':
        if (!request.context) {
          throw new Error('í†µí•© ë¶„ì„ì—ëŠ” ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤');
        }
        return await this.customEngines.unifiedAnalysis(request.context);

      case 'custom-nlp':
        return await this.customEngines.customNLP(request.query);

      case 'correlation':
        if (!Array.isArray(request.data)) {
          throw new Error('ìƒê´€ê´€ê³„ ë¶„ì„ì—ëŠ” ì„œë²„ ë©”íŠ¸ë¦­ ë°°ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤');
        }
        const correlationResult = await correlationEngine.analyzeCorrelations(
          request.data
        );
        return {
          answer: `ì„œë²„ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ${correlationResult.topCorrelations.length}ê°œì˜ ì£¼ìš” ìƒê´€ê´€ê³„ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.`,
          confidence: correlationResult.topCorrelations.length > 0 ? 0.9 : 0.6,
          correlations: correlationResult,
          reasoning_steps: [
            'ì„œë²„ ë©”íŠ¸ë¦­ ë°ì´í„° ê²€ì¦',
            'ë°°ì¹˜ë³„ ìƒê´€ê´€ê³„ ê³„ì‚°',
            'í†µê³„ì  ìœ ì˜ì„± ë¶„ì„',
            'ì´ìƒ ì§•í›„ íƒì§€',
            'ê¶Œì¥ì‚¬í•­ ìƒì„±',
          ],
        };

      default:
        throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—”ì§„: ${request.engine}`);
    }
  }

  /**
   * ğŸ’¾ ìºì‹œ ê´€ë¦¬
   */
  private checkCache(request: AIEngineRequest): any {
    const cacheKey = this.generateCacheKey(request);
    const cached = this.responseCache.get(cacheKey);

    if (cached && Date.now() - cached.timestamp < cached.ttl) {
      return cached;
    }

    // ë§Œë£Œëœ ìºì‹œ ì œê±°
    if (cached) {
      this.responseCache.delete(cacheKey);
    }

    return null;
  }

  private saveToCache(request: AIEngineRequest, result: any) {
    const cacheKey = this.generateCacheKey(request);
    const ttl = this.getCacheTTL(request.engine);

    this.responseCache.set(cacheKey, {
      result,
      timestamp: Date.now(),
      ttl,
    });

    // ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
    if (this.responseCache.size > 100) {
      const firstKey = this.responseCache.keys().next().value;
      if (firstKey) {
        this.responseCache.delete(firstKey);
      }
    }
  }

  private generateCacheKey(request: AIEngineRequest): string {
    return `${request.engine}:${JSON.stringify({
      query: request.query,
      data: request.data,
      context: request.context,
    })}`;
  }

  private getCacheTTL(engine: string): number {
    // ì—”ì§„ë³„ ìºì‹œ TTL (ë°€ë¦¬ì´ˆ)
    const ttls: Record<string, number> = {
      anomaly: 5 * 60 * 1000, // 5ë¶„
      prediction: 10 * 60 * 1000, // 10ë¶„
      autoscaling: 3 * 60 * 1000, // 3ë¶„
      korean: 30 * 60 * 1000, // 30ë¶„
      enhanced: 15 * 60 * 1000, // 15ë¶„
      integrated: 20 * 60 * 1000, // 20ë¶„
      mcp: 2 * 60 * 1000, // 2ë¶„
      'mcp-test': 1 * 60 * 1000, // 1ë¶„
      hybrid: 5 * 60 * 1000, // 5ë¶„
      unified: 3 * 60 * 1000, // 3ë¶„
      'custom-nlp': 10 * 60 * 1000, // 10ë¶„
      correlation: 5 * 60 * 1000, // 5ë¶„ (ìƒê´€ê´€ê³„ëŠ” ìì£¼ ë³€í•¨)
    };

    return ttls[engine] || 5 * 60 * 1000; // ê¸°ë³¸ 5ë¶„
  }

  /**
   * ğŸ“Š í†µê³„ ê´€ë¦¬
   */
  private updateEngineStats(
    engine: string,
    responseTime: number,
    success: boolean
  ) {
    const stats = this.engineStats.get(engine);
    if (stats) {
      stats.calls++;
      stats.totalTime += responseTime;
      stats.lastUsed = Date.now();
      if (success) {
        stats.successes++;
      }
      this.engineStats.set(engine, stats);
    }
  }

  /**
   * ğŸ“ˆ ìƒíƒœ ë° ì„±ëŠ¥ ì •ë³´
   */
  getEngineStatuses(): EngineStatus[] {
    const statuses: EngineStatus[] = [];

    this.engineStats.forEach((stats, engine) => {
      const successRate = stats.calls > 0 ? stats.successes / stats.calls : 0;
      const avgResponseTime =
        stats.calls > 0 ? stats.totalTime / stats.calls : 0;

      statuses.push({
        name: engine,
        status: this.initialized ? 'ready' : 'loading',
        last_used: stats.lastUsed,
        success_rate: successRate,
        avg_response_time: avgResponseTime,
        memory_usage: this.getEngineMemoryUsage(engine),
      });
    });

    return statuses;
  }

  private getEngineMemoryUsage(engine: string): string {
    // ì—”ì§„ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
    const memoryUsage: Record<string, string> = {
      anomaly: '~2MB',
      prediction: '~15MB',
      autoscaling: '~3MB',
      korean: '~2MB',
      enhanced: '~9MB',
      integrated: '~12MB',
      mcp: '~5MB',
      'mcp-test': '~1MB',
      hybrid: '~8MB',
      unified: '~6MB',
      'custom-nlp': '~4MB',
    };

    return memoryUsage[engine] || '~3MB';
  }

  getSystemInfo() {
    const openSourceStatus = this.openSourceEngines.getEngineStatus();
    const customStatus = this.customEngines.getEngineStatus();

    return {
      master_engine: {
        version: AI_ENGINE_VERSIONS.master,
        initialized: this.initialized,
        total_engines: 11,
        opensource_engines: 6,
        custom_engines: 5,
      },
      versions: {
        master: AI_ENGINE_VERSIONS.master,
        opensource: AI_ENGINE_VERSIONS.opensource,
        custom: AI_ENGINE_VERSIONS.custom,
        support: AI_ENGINE_VERSIONS.support,
      },
      performance: {
        total_memory: '~70MB (with lazy loading)',
        bundle_size: '~933KB (optimized)',
        cache_size: this.responseCache.size,
        cache_hit_rate: this.calculateCacheHitRate(),
      },
      engine_details: {
        opensource: openSourceStatus,
        custom: customStatus,
      },
      capabilities: [
        'multi_engine_routing',
        'automatic_fallback',
        'performance_caching',
        'real_time_monitoring',
        'korean_optimization',
        'mcp_integration',
        'version_management',
        'change_logging',
      ],
      version_manager: VersionManager.getCurrentVersions(),
    };
  }

  private calculateCacheHitRate(): number {
    // ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)
    return this.responseCache.size > 0 ? 0.3 : 0; // 30% ì¶”ì •
  }

  private logPerformanceInfo() {
    console.log(`
ğŸš€ OpenManager Vibe v5 - MasterAIEngine ì„±ëŠ¥ ì •ë³´
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Š ì˜¤í”ˆì†ŒìŠ¤ ì—”ì§„ (6ê°œ): ~43MB ë©”ëª¨ë¦¬, ~933KB ë²ˆë“¤
ğŸ¯ ì»¤ìŠ¤í…€ ì—”ì§„ (5ê°œ): ~27MB ë©”ëª¨ë¦¬, MCP í†µí•©
ğŸ® 2-Mode AI ì‹œìŠ¤í…œ: LOCAL & GOOGLE_ONLY
ğŸ’¾ ìŠ¤ë§ˆíŠ¸ ìºì‹±: ì‘ë‹µì‹œê°„ 50% ë‹¨ì¶•
ğŸ‡°ğŸ‡· í•œêµ­ì–´ ìµœì í™”: hangul-js + korean-utils
ğŸ”§ ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~70MB (ì§€ì—° ë¡œë”© ì ìš©)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    `);
  }

  /**
   * ğŸ§  ì‚¬ê³ ê³¼ì • ë‹¨ê³„ ìƒì„±
   */
  private createThinkingStep(
    type:
      | 'analyzing'
      | 'processing'
      | 'reasoning'
      | 'generating'
      | 'completed'
      | 'error',
    title: string,
    description: string
  ): AIThinkingStep {
    return {
      id: `step_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      timestamp: new Date().toISOString(),
      type,
      title,
      description,
      progress:
        type === 'completed'
          ? 100
          : type === 'error'
            ? 0
            : Math.floor(Math.random() * 40) + 30,
      duration: Math.floor(Math.random() * 200) + 50,
      metadata: {
        engine: 'master',
        timestamp: Date.now(),
      },
    };
  }

  /**
   * ğŸ” ì—”ì§„ë³„ ì¶”ë¡  ë‹¨ê³„ ìƒì„±
   */
  private generateReasoningSteps(engine: string, query: string): string[] {
    const baseSteps = ['ìš”ì²­ ë¶„ì„', 'ë°ì´í„° ë¡œë“œ'];

    const engineSpecificSteps: Record<string, string[]> = {
      anomaly: ['í†µê³„ ë¶„ì„', 'Z-score ê³„ì‚°', 'ì´ìƒì¹˜ íƒì§€'],
      prediction: ['ì‹œê³„ì—´ ë¶„ì„', 'LSTM ëª¨ë¸ ì ìš©', 'ì˜ˆì¸¡ ìƒì„±'],
      autoscaling: ['ë¶€í•˜ ë¶„ì„', 'íšŒê·€ ë¶„ì„', 'ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­'],
      korean: ['í•œêµ­ì–´ ë¶„ì„', 'í˜•íƒœì†Œ ë¶„ì„', 'ê°ì • ë¶„ì„'],
      enhanced: ['í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰', 'Fuse.js ì²˜ë¦¬', 'ê²€ìƒ‰ ê²°ê³¼ ë­í‚¹'],
      integrated: ['NLP ë¶„ì„', 'ì—”í‹°í‹° ì¶”ì¶œ', 'í…ìŠ¤íŠ¸ ìš”ì•½'],
      mcp: ['MCP ì—°ê²°', 'ì»¨í…ìŠ¤íŠ¸ ë¶„ì„', 'ì¶”ë¡  ì ìš©'],
      'mcp-test': ['ì—°ê²° í…ŒìŠ¤íŠ¸', 'ìƒíƒœ í™•ì¸', 'ì‘ë‹µ ê²€ì¦'],
      hybrid: ['ë‹¤ì¤‘ ì—”ì§„ ì¡°í•©', 'ê²°ê³¼ í†µí•©', 'ìµœì í™”'],
      unified: ['í†µí•© ë°ì´í„° ì²˜ë¦¬', 'í¬ë¡œìŠ¤ í”Œë«í¼ ë¶„ì„', 'ê²°ê³¼ ì •ê·œí™”'],
      'custom-nlp': ['OpenManager NLP', 'ë„ë©”ì¸ íŠ¹í™” ë¶„ì„', 'ì¸ì‚¬ì´íŠ¸ ìƒì„±'],
    };

    const specific = engineSpecificSteps[engine] || ['ì¼ë°˜ ì²˜ë¦¬', 'ê²°ê³¼ ìƒì„±'];

    return [...baseSteps, ...specific, 'ì‘ë‹µ í¬ë§·íŒ…', 'ê²°ê³¼ ë°˜í™˜'];
  }

  /**
   * ğŸ§¹ ì •ë¦¬ ë©”ì„œë“œ
   */
  cleanup() {
    this.responseCache.clear();
    this.engineStats.clear();
    aiLogger.logAI({
      level: LogLevel.INFO,
      category: LogCategory.AI_ENGINE,
      engine: 'MasterAIEngine',
      message: 'ğŸ§¹ MasterAIEngine ì •ë¦¬ ì™„ë£Œ',
    });
  }

  /**
   * ì—”ì§„ë³„ ë¡œê·¸ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
   */
  private getLogCategory(engine: string): LogCategory {
    const categoryMap: Record<string, LogCategory> = {
      anomaly: LogCategory.ANOMALY,
      prediction: LogCategory.PREDICTION,
      autoscaling: LogCategory.PERFORMANCE,
      korean: LogCategory.AI_ENGINE,
      enhanced: LogCategory.AI_ENGINE,
      integrated: LogCategory.AI_ENGINE,
      mcp: LogCategory.MCP,
      'mcp-test': LogCategory.MCP,
      hybrid: LogCategory.HYBRID,
      unified: LogCategory.AI_ENGINE,
      'custom-nlp': LogCategory.AI_ENGINE,
      correlation: LogCategory.AI_ENGINE,
    };

    return categoryMap[engine] || LogCategory.AI_ENGINE;
  }

  /**
   * ğŸš¨ ì¿¼ë¦¬ ì—ëŸ¬ ì²˜ë¦¬
   */
  private async handleQueryError(
    request: AIEngineRequest,
    error: Error,
    startTime: number,
    thinkingSteps: AIThinkingStep[]
  ): Promise<AIEngineResponse> {
    await aiLogger.logError(
      request.engine,
      this.getLogCategory(request.engine),
      error,
      {
        query: request.query,
        data: request.data,
        context: request.context,
        responseTime: Date.now() - startTime,
      }
    );

    if (thinkingSteps.length > 0) {
      thinkingSteps.push(
        this.createThinkingStep('error', 'ì˜¤ë¥˜ ë°œìƒ', error.message)
      );
    }

    // í†µê³„ ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨)
    this.updateEngineStats(request.engine, Date.now() - startTime, false);

    return {
      success: false,
      result: null,
      engine_used: request.engine,
      response_time: Date.now() - startTime,
      confidence: 0,
      fallback_used: false,
      error: error.message,
      thinking_process: thinkingSteps,
    };
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const masterAIEngine = new MasterAIEngine();
