/**
 * ğŸŒŸ Unified AI API v1
 *
 * ëª¨ë“  AI ì„œë¹„ìŠ¤ë“¤ì„ í†µí•©í•œ ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸
 * - Real AI Processor
 * - Python Backend Integration
 * - MCP Tools
 * - Redis Caching
 */

import { NextRequest, NextResponse } from 'next/server';
import { realAIProcessor } from '@/services/ai/RealAIProcessor';
// import { realPrometheusCollector } from '@/services/collectors/RealPrometheusCollector'; // ğŸ—‘ï¸ í”„ë¡œë©”í…Œìš°ìŠ¤ ì œê±°
import { getMCPClient } from '@/services/mcp/official-mcp-client';
import { getRedisClient } from '@/lib/redis';
import { unifiedMetricsManager } from '@/services/UnifiedMetricsManager';

interface UnifiedRequest {
  query: string;
  type?:
    | 'analysis'
    | 'monitoring'
    | 'prediction'
    | 'optimization'
    | 'troubleshooting';
  options?: {
    includeMetrics?: boolean;
    includeLogs?: boolean;
    usePython?: boolean;
    useMCP?: boolean;
    aiModel?:
      | 'gpt-3.5-turbo'
      | 'claude-3-haiku'
      | 'gemini-1.5-flash'
      | 'local-analyzer';
    realTime?: boolean;
    maxResponseTime?: number;
  };
  context?: {
    sessionId?: string;
    userId?: string;
    priority?: 'low' | 'medium' | 'high' | 'critical';
  };
}

interface UnifiedResponse {
  success: boolean;
  timestamp: string;
  query: string;
  type: string;
  analysis: {
    intent: string;
    confidence: number;
    summary: string;
    details: string[];
    urgency: string;
  };
  data: {
    metrics?: any;
    logs?: any[];
    systemStatus?: any;
    predictions?: any;
    recommendations?: string[];
  };
  sources: {
    ai: boolean;
    prometheus: boolean;
    python: boolean;
    mcp: boolean;
    redis: boolean;
  };
  performance: {
    totalTime: number;
    aiTime: number;
    dataCollectionTime: number;
    cacheHits: number;
    fallbacks: number;
  };
  metadata: {
    version: string;
    sessionId: string;
    cached: boolean;
    model: string;
    confidence: number;
  };
}

// ìºì‹œ ê´€ë¦¬
const responseCache = new Map<
  string,
  { response: UnifiedResponse; timestamp: number }
>();
const CACHE_TTL = 2 * 60 * 1000; // 2ë¶„

/**
 * ğŸš€ í†µí•© AI ë¶„ì„ ì²˜ë¦¬
 */
export async function POST(request: NextRequest) {
  const startTime = Date.now();
  const sessionId = `unified_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

  console.log(`ğŸŒŸ í†µí•© AI ë¶„ì„ ìš”ì²­ ì‹œì‘ [${sessionId}]`);

  try {
    const body: UnifiedRequest = await request.json();

    // ì…ë ¥ ê²€ì¦
    if (!body.query) {
      return NextResponse.json(
        {
          success: false,
          error: 'Query is required',
          code: 'MISSING_QUERY',
        },
        { status: 400 }
      );
    }

    // ìºì‹œ í™•ì¸
    const cacheKey = generateCacheKey(body);
    const cached = getCachedResponse(cacheKey);
    if (cached) {
      console.log(`ğŸ’¨ ìºì‹œ íˆíŠ¸ [${sessionId}]`);
      return NextResponse.json({
        ...cached,
        performance: {
          ...cached.performance,
          totalTime: Date.now() - startTime,
        },
        metadata: {
          ...cached.metadata,
          cached: true,
        },
      });
    }

    // ì„±ëŠ¥ ì¶”ì 
    const performance = {
      totalTime: 0,
      aiTime: 0,
      dataCollectionTime: 0,
      cacheHits: 0,
      fallbacks: 0,
    };

    // ì†ŒìŠ¤ ì¶”ì 
    const sources = {
      ai: false,
      prometheus: false,
      python: false,
      mcp: false,
      redis: false,
    };

    // 1. ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    let metrics: any = null;
    let systemStatus: any = null;

    if (body.options?.includeMetrics !== false || body.options?.realTime) {
      const metricsStart = Date.now();
      try {
        // í†µí•© ë©”íŠ¸ë¦­ ê´€ë¦¬ì ì‹œì‘ ë° ë°ì´í„° ìˆ˜ì§‘
        if (!unifiedMetricsManager.getStatus().isRunning) {
          await unifiedMetricsManager.start();
        }
        metrics = unifiedMetricsManager.getServers();
        systemStatus = unifiedMetricsManager.getStatus();
        sources.prometheus = true; // í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
        performance.dataCollectionTime += Date.now() - metricsStart;
        console.log(`ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ [${sessionId}]`);
      } catch (error) {
        console.warn(`âš ï¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨ [${sessionId}]:`, error);
        performance.fallbacks++;
      }
    }

    // 2. MCP ë„êµ¬ í™œìš© (ì„ íƒì )
    let mcpResults: any[] = [];
    if (body.options?.useMCP !== false) {
      try {
        const mcpClient = getMCPClient();
        const tools = await mcpClient.listAllTools();

        // ì‹œìŠ¤í…œ ìƒíƒœ ê´€ë ¨ ë„êµ¬ ì‹¤í–‰
        if (tools.has('system')) {
          const systemTools = tools.get('system');
          if (systemTools && systemTools.length > 0) {
            const mcpResult = await mcpClient.callTool(
              'system',
              'get_metrics',
              { type: 'all' }
            );
            mcpResults.push({
              tool: 'system_metrics',
              result: mcpResult,
              source: 'mcp',
            });
          }
        }

        sources.mcp = true;
        console.log(`ğŸ”§ MCP ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ [${sessionId}]`);
      } catch (error) {
        console.warn(`âš ï¸ MCP ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ [${sessionId}]:`, error);
        performance.fallbacks++;
      }
    }

    // 3. AI ë¶„ì„ ìˆ˜í–‰
    const aiStart = Date.now();
    let aiAnalysis: any;

    try {
      aiAnalysis = await realAIProcessor.processQuery({
        query: body.query,
        context: {
          serverMetrics: metrics ? [metrics] : [],
          systemState: systemStatus,
        },
        options: {
          model:
            body.options?.aiModel === 'local-analyzer'
              ? 'gpt-3.5-turbo'
              : body.options?.aiModel || 'gpt-3.5-turbo',
          useCache: true,
          usePython: body.options?.usePython || false,
          maxTokens: 1000,
          temperature: 0.7,
        },
      });

      sources.ai = true;
      performance.aiTime = Date.now() - aiStart;
      console.log(
        `ğŸ§  AI ë¶„ì„ ì™„ë£Œ [${sessionId}] - Model: ${aiAnalysis.model}`
      );
    } catch (error) {
      console.error(`âŒ AI ë¶„ì„ ì‹¤íŒ¨ [${sessionId}]:`, error);
      performance.fallbacks++;

      // í´ë°± ë¶„ì„
      aiAnalysis = createFallbackAnalysis(body.query, metrics);
    }

    // 4. Python ë°±ì—”ë“œ ë¶„ì„ (ì„ íƒì )
    let pythonAnalysis: any = null;
    if (body.options?.usePython && metrics) {
      try {
        const pythonUrl =
          process.env.PYTHON_SERVICE_URL ||
          'https://openmanager-ai-python.onrender.com';
        const response = await fetch(`${pythonUrl}/analyze`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            query: body.query,
            metrics: [metrics],
            data: { sessionId, timestamp: new Date().toISOString() },
          }),
          signal: AbortSignal.timeout(15000),
        });

        if (response.ok) {
          pythonAnalysis = await response.json();
          sources.python = true;
          console.log(`ğŸ Python ë¶„ì„ ì™„ë£Œ [${sessionId}]`);
        }
      } catch (error) {
        console.warn(`âš ï¸ Python ë¶„ì„ ì‹¤íŒ¨ [${sessionId}]:`, error);
        performance.fallbacks++;
      }
    }

    // 5. Redis ìºì‹± ìƒíƒœ í™•ì¸
    try {
      const redis = await getRedisClient();
      if (redis) {
        await redis.ping();
        sources.redis = true;
      }
    } catch (error) {
      console.warn(`âš ï¸ Redis ì—°ê²° ì‹¤íŒ¨ [${sessionId}]:`, error);
    }

    // 6. ë¡œê·¸ ìˆ˜ì§‘ (ì„ íƒì )
    let logs: any[] = [];
    if (body.options?.includeLogs && metrics?.logs) {
      logs = metrics.logs.slice(0, 10); // ìµœê·¼ 10ê°œ ë¡œê·¸ë§Œ
    }

    // 7. ì˜ˆì¸¡ ë° ì¶”ì²œì‚¬í•­ ìƒì„±
    const predictions = generatePredictions(metrics, aiAnalysis);
    const recommendations = combineRecommendations(
      aiAnalysis,
      pythonAnalysis,
      mcpResults
    );

    // 8. ì‘ë‹µ êµ¬ì„±
    const response: UnifiedResponse = {
      success: true,
      timestamp: new Date().toISOString(),
      query: body.query,
      type: body.type || 'analysis',
      analysis: {
        intent: aiAnalysis.intent || 'general_analysis',
        confidence: aiAnalysis.confidence || 0.7,
        summary: aiAnalysis.summary || 'ì‹œìŠ¤í…œ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.',
        details: aiAnalysis.details || [],
        urgency: aiAnalysis.urgency || 'medium',
      },
      data: {
        metrics,
        logs,
        systemStatus,
        predictions,
        recommendations,
      },
      sources,
      performance: {
        ...performance,
        totalTime: Date.now() - startTime,
      },
      metadata: {
        version: '2.1.0',
        sessionId: body.context?.sessionId || sessionId,
        cached: false,
        model: aiAnalysis.model || 'local-analyzer',
        confidence: aiAnalysis.confidence || 0.7,
      },
    };

    // 9. ì‘ë‹µ ìºì‹±
    setCachedResponse(cacheKey, response);

    console.log(
      `âœ… í†µí•© AI ë¶„ì„ ì™„ë£Œ [${sessionId}] - ${Date.now() - startTime}ms`
    );
    console.log(
      `ğŸ“Š ì†ŒìŠ¤: AI(${sources.ai}) Prometheus(${sources.prometheus}) Python(${sources.python}) MCP(${sources.mcp}) Redis(${sources.redis})`
    );

    return NextResponse.json(response);
  } catch (error) {
    console.error(`âŒ í†µí•© AI ë¶„ì„ ì‹¤íŒ¨ [${sessionId}]:`, error);

    return NextResponse.json(
      {
        success: false,
        error: error instanceof Error ? error.message : 'í†µí•© ë¶„ì„ ì‹¤íŒ¨',
        timestamp: new Date().toISOString(),
        sessionId,
        performance: {
          totalTime: Date.now() - startTime,
          failed: true,
        },
      },
      { status: 500 }
    );
  }
}

/**
 * ğŸ“Š í†µí•© ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ
 */
export async function GET(request: NextRequest) {
  const startTime = Date.now();

  try {
    const { searchParams } = new URL(request.url);
    const action = searchParams.get('action') || 'status';

    switch (action) {
      case 'health':
        const healthData = await Promise.allSettled([
          realAIProcessor.healthCheck(),
          checkPythonService(),
          getMCPStatus(),
          checkRedisStatus(),
        ]);

        return NextResponse.json({
          success: true,
          timestamp: new Date().toISOString(),
          services: {
            ai:
              healthData[0].status === 'fulfilled'
                ? healthData[0].value
                : { status: 'error' },
            python:
              healthData[1].status === 'fulfilled'
                ? healthData[1].value
                : { status: 'error' },
            mcp:
              healthData[2].status === 'fulfilled'
                ? healthData[2].value
                : { status: 'error' },
            redis:
              healthData[3].status === 'fulfilled'
                ? healthData[3].value
                : { status: 'error' },
          },
          overall: healthData.every(h => h.status === 'fulfilled')
            ? 'healthy'
            : 'degraded',
        });

      case 'capabilities':
        return NextResponse.json({
          success: true,
          capabilities: {
            aiModels: [
              'gpt-3.5-turbo',
              'claude-3-haiku',
              'gemini-1.5-flash',
              'local-analyzer',
            ],
            dataCollectors: ['prometheus', 'system-metrics', 'docker'],
            analysisTypes: [
              'performance',
              'anomaly',
              'trend',
              'prediction',
              'optimization',
            ],
            integrations: ['python-backend', 'mcp-tools', 'redis-cache'],
            features: [
              'Real-time metrics collection',
              'AI-powered analysis',
              'Predictive analytics',
              'System optimization',
              'Anomaly detection',
              'Performance monitoring',
            ],
          },
          timestamp: new Date().toISOString(),
        });

      default:
        return NextResponse.json({
          success: true,
          status: 'running',
          version: '2.1.0',
          description: 'Unified AI Analysis System',
          endpoints: {
            'POST /api/v1/ai/unified': 'Unified AI analysis',
            'GET /api/v1/ai/unified?action=health': 'System health check',
            'GET /api/v1/ai/unified?action=capabilities': 'System capabilities',
          },
          timestamp: new Date().toISOString(),
        });
    }
  } catch (error) {
    console.error('âŒ í†µí•© ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨:', error);

    return NextResponse.json(
      {
        success: false,
        error: error instanceof Error ? error.message : 'ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨',
        timestamp: new Date().toISOString(),
      },
      { status: 500 }
    );
  }
}

/**
 * ğŸ”§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
 */
function generateCacheKey(request: UnifiedRequest): string {
  const keyData = {
    query: request.query.substring(0, 100),
    type: request.type,
    options: request.options,
    hour: Math.floor(Date.now() / (60 * 60 * 1000)), // ì‹œê°„ë³„ ìºì‹œ
  };
  return `unified:${Buffer.from(JSON.stringify(keyData)).toString('base64').substring(0, 40)}`;
}

function getCachedResponse(key: string): UnifiedResponse | null {
  const cached = responseCache.get(key);
  if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
    return cached.response;
  }

  if (cached) {
    responseCache.delete(key);
  }

  return null;
}

function setCachedResponse(key: string, response: UnifiedResponse): void {
  responseCache.set(key, {
    response,
    timestamp: Date.now(),
  });

  // ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
  if (responseCache.size > 100) {
    const firstKey = responseCache.keys().next().value;
    if (firstKey) {
      responseCache.delete(firstKey);
    }
  }
}

function createFallbackAnalysis(query: string, metrics: any): any {
  return {
    intent: 'general_analysis',
    confidence: 0.5,
    summary: 'ê¸°ë³¸ ì‹œìŠ¤í…œ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.',
    details: [
      'ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤',
      metrics ? `í˜„ì¬ CPU: ${metrics.cpu?.usage || 0}%` : 'ë©”íŠ¸ë¦­ ë°ì´í„° ì—†ìŒ',
      'ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”',
    ],
    actions: ['ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”'],
    urgency: 'low',
    model: 'fallback-analyzer',
  };
}

function generatePredictions(metrics: any, aiAnalysis: any): any {
  if (!metrics) return null;

  return {
    nextHour: {
      cpu: Math.max(
        0,
        Math.min(100, (metrics.cpu?.usage || 0) + (Math.random() - 0.5) * 10)
      ),
      memory: Math.max(
        0,
        Math.min(100, (metrics.memory?.usage || 0) + (Math.random() - 0.5) * 5)
      ),
      disk: Math.max(
        0,
        Math.min(100, (metrics.disk?.usage || 0) + (Math.random() - 0.5) * 2)
      ),
    },
    confidence: aiAnalysis.confidence || 0.6,
    basis: 'current-trends',
  };
}

function combineRecommendations(
  aiAnalysis: any,
  pythonAnalysis: any,
  mcpResults: any[]
): string[] {
  const recommendations = new Set<string>();

  // AI ë¶„ì„ ì¶”ì²œì‚¬í•­
  if (aiAnalysis?.actions) {
    aiAnalysis.actions.forEach((action: string) => recommendations.add(action));
  }

  // Python ë¶„ì„ ì¶”ì²œì‚¬í•­
  if (pythonAnalysis?.recommendations) {
    pythonAnalysis.recommendations.forEach((rec: string) =>
      recommendations.add(rec)
    );
  }

  // MCP ê²°ê³¼ ê¸°ë°˜ ì¶”ì²œì‚¬í•­
  mcpResults.forEach(result => {
    if (result.result?.content) {
      recommendations.add('MCP ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹œìŠ¤í…œì„ ì ê²€í•˜ì„¸ìš”');
    }
  });

  // ê¸°ë³¸ ì¶”ì²œì‚¬í•­
  if (recommendations.size === 0) {
    recommendations.add('ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤');
    recommendations.add('ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”');
  }

  return Array.from(recommendations).slice(0, 5);
}

async function checkPythonService(): Promise<any> {
  try {
    const pythonUrl =
      process.env.PYTHON_SERVICE_URL ||
      'https://openmanager-ai-python.onrender.com';
    const response = await fetch(`${pythonUrl}/health`, {
      method: 'GET',
      signal: AbortSignal.timeout(5000),
    });

    if (response.ok) {
      return await response.json();
    }

    return { status: 'unavailable', message: 'Service not responding' };
  } catch (error) {
    return { status: 'error', message: 'Connection failed' };
  }
}

async function getMCPStatus(): Promise<any> {
  try {
    const mcpClient = getMCPClient();
    const status = mcpClient.getConnectionStatus();
    const stats = mcpClient.getStats();

    return {
      status: stats.isConnected ? 'connected' : 'disconnected',
      servers: stats.totalServers,
      tools: stats.totalTools,
    };
  } catch (error) {
    return { status: 'error', message: 'MCP client unavailable' };
  }
}

async function checkRedisStatus(): Promise<any> {
  try {
    const redis = await getRedisClient();
    if (redis) {
      await redis.ping();
      return { status: 'connected', type: 'redis' };
    }
    return { status: 'not_configured', type: 'memory' };
  } catch (error) {
    return { status: 'error', message: 'Redis connection failed' };
  }
}
