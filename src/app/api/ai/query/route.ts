/**
 * ğŸ¤– AI í†µí•© ì¿¼ë¦¬ API v3.2 (PDF Support)
 *
 * * v3.2 Upgrade: PDF Text Extraction (backend-side) using pdf-parse.
 *
 * POST /api/ai/query
 */

import { google } from '@ai-sdk/google';
import { groq } from '@ai-sdk/groq';
import { type CoreMessage, generateText, tool } from 'ai';
import type { NextRequest } from 'next/server';
// @ts-expect-error
import pdf from 'pdf-parse';
import { z } from 'zod';
import {
  checkGoogleAIRateLimit,
  getGoogleAIKey,
} from '@/lib/ai/google-ai-manager';
import {
  checkGroqAIRateLimit,
  type GroqModel,
  isGroqAIAvailable,
} from '@/lib/ai/groq-ai-manager';
import { classifyQuery } from '@/lib/ai/query-classifier';
import { withAuth } from '@/lib/auth/api-auth';
import { createClient } from '@/lib/supabase/server';
import { SupabaseRAGEngine } from '@/services/ai/supabase-rag-engine';
import { loadHourlyScenarioData } from '@/services/scenario/scenario-loader';

// ìµœëŒ€ ì‹¤í–‰ ì‹œê°„: 60ì´ˆ (PDF íŒŒì‹± ê³ ë ¤)
export const maxDuration = 60;

// ... [Existing Tools: callUnifiedProcessor, getServerMetrics, searchKnowledgeBase, analyzePattern, recommendCommands, analyzeRequest] ...
const callUnifiedProcessor = tool({
  description:
    'ë³µì¡í•œ ë¶„ì„ ìš”ì²­ì„ í†µí•© AI í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (GCP Cloud Functions)',
  inputSchema: z.object({
    query: z.string().describe('ì‚¬ìš©ì ì§ˆë¬¸'),
    processors: z
      .array(z.string())
      .describe(
        'ì‹¤í–‰í•  í”„ë¡œì„¸ì„œ ëª©ë¡ (korean_nlp, ml_analytics, server_analyzer)'
      ),
  }),
  execute: async ({
    query,
    processors,
  }: {
    query: string;
    processors: string[];
  }) => {
    try {
      const gcpEndpoint =
        process.env.NEXT_PUBLIC_GCP_UNIFIED_PROCESSOR_ENDPOINT;
      if (!gcpEndpoint) {
        throw new Error(
          'NEXT_PUBLIC_GCP_UNIFIED_PROCESSOR_ENDPOINT is not configured'
        );
      }

      const allServers = await loadHourlyScenarioData();
      const serverIds = allServers.map((s) => s.id);

      const response = await fetch(gcpEndpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          query,
          context: {
            server_ids: serverIds,
            timestamp: new Date().toISOString(),
          },
          processors,
          options: { ml_model: 'anomaly_detection' },
        }),
        signal: AbortSignal.timeout(15000),
      });

      if (!response.ok) {
        throw new Error(`Unified Processor API error: ${response.status}`);
      }

      const result = await response.json();
      return {
        success: true,
        data: result.data,
        timestamp: new Date().toISOString(),
        _source: 'GCP Unified AI Processor',
        _performance: result.performance,
      };
    } catch (error) {
      console.error('âŒ Unified Processor í˜¸ì¶œ ì‹¤íŒ¨:', error);
      return {
        success: false,
        error: 'í†µí•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        _fallback_needed: true,
      };
    }
  },
});

const getServerMetrics = tool({
  description:
    'ì„œë²„ CPU/ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜)',
  inputSchema: z.object({
    serverId: z.string().optional().describe('ì¡°íšŒí•  ì„œë²„ ID (ì„ íƒ)'),
    metric: z
      .enum(['cpu', 'memory', 'disk', 'all'])
      .describe('ì¡°íšŒí•  ë©”íŠ¸ë¦­ íƒ€ì…'),
  }),
  execute: async ({
    serverId,
    metric: _metric,
  }: {
    serverId?: string;
    metric: 'cpu' | 'memory' | 'disk' | 'all';
  }) => {
    const allServers = await loadHourlyScenarioData();
    const target = serverId
      ? allServers.find((s) => s.id === serverId)
      : allServers;

    const servers = Array.isArray(target)
      ? target
      : target
        ? [target]
        : allServers;

    return {
      success: true,
      servers: servers.map((s) => ({
        id: s.id,
        name: s.name,
        status: s.status,
        cpu: s.cpu,
        memory: s.memory,
        disk: s.disk,
      })),
      summary: {
        total: servers.length,
        alertCount: servers.filter(
          (s) => s.status === 'warning' || s.status === 'critical'
        ).length,
      },
      timestamp: new Date().toISOString(),
      _dataSource: 'scenario-loader',
    };
  },
});

const searchKnowledgeBase = tool({
  description: 'ê³¼ê±° ì¥ì•  ì´ë ¥ ë° í•´ê²° ë°©ë²•ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (Real RAG)',
  inputSchema: z.object({
    query: z.string().describe('ê²€ìƒ‰ ì¿¼ë¦¬'),
  }),
  execute: async ({ query }: { query: string }) => {
    try {
      const supabase = await createClient();
      const ragEngine = new SupabaseRAGEngine(supabase);

      const searchResult = await ragEngine.searchHybrid(query, {
        maxResults: 3,
        enableKeywordFallback: true,
      });

      if (!searchResult.success || searchResult.results.length === 0) {
        return {
          success: false,
          message: 'ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
        };
      }

      return {
        success: true,
        results: searchResult.results.map((r) => ({
          content: r.content,
          similarity: r.similarity,
        })),
        _source: 'Supabase pgvector',
      };
    } catch (error) {
      console.error('âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨:', error);
      return { success: false, error: 'ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜' };
    }
  },
});

const analyzePattern = tool({
  description:
    'ì‚¬ìš©ì ì§ˆë¬¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì¦‰ê°ì ì¸ ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤ (Offline)',
  inputSchema: z.object({
    query: z.string().describe('ë¶„ì„í•  ì‚¬ìš©ì ì§ˆë¬¸'),
  }),
  execute: async ({ query }: { query: string }) => {
    const patterns: string[] = [];
    const q = query.toLowerCase();

    if (/cpu|í”„ë¡œì„¸ì„œ|ì„±ëŠ¥/i.test(q)) patterns.push('system_performance');
    if (/ë©”ëª¨ë¦¬|ram|memory/i.test(q)) patterns.push('memory_status');
    if (/ë””ìŠ¤í¬|ì €ì¥ì†Œ|ìš©ëŸ‰/i.test(q)) patterns.push('storage_info');
    if (/ì„œë²„|ì‹œìŠ¤í…œ|ìƒíƒœ/i.test(q)) patterns.push('server_status');

    if (patterns.length === 0) {
      return { success: false, message: 'ë§¤ì¹­ë˜ëŠ” íŒ¨í„´ ì—†ìŒ' };
    }

    return {
      success: true,
      patterns,
      detectedIntent: patterns[0],
      _mode: 'offline-pattern-match',
    };
  },
});

const recommendCommands = tool({
  description: 'ì‚¬ìš©ì ì§ˆë¬¸ì— ì í•©í•œ CLI ëª…ë ¹ì–´ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤ (Offline)',
  inputSchema: z.object({
    keywords: z.array(z.string()).describe('ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ í‚¤ì›Œë“œ'),
  }),
  execute: async ({ keywords }: { keywords: string[] }) => {
    const recommendations = [
      {
        keywords: ['ì„œë²„', 'ëª©ë¡', 'ì¡°íšŒ'],
        command: 'list servers',
        description: 'ì„œë²„ ëª©ë¡ ì¡°íšŒ',
      },
      {
        keywords: ['ìƒíƒœ', 'ì²´í¬', 'í™•ì¸'],
        command: 'status check',
        description: 'ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€',
      },
      {
        keywords: ['ë¡œê·¸', 'ë¶„ì„', 'ì—ëŸ¬'],
        command: 'analyze logs',
        description: 'ë¡œê·¸ ë¶„ì„',
      },
    ];

    const matched = recommendations.filter((rec) =>
      keywords.some((k) =>
        rec.keywords.some((rk) => rk.includes(k) || k.includes(rk))
      )
    );

    return {
      success: true,
      recommendations:
        matched.length > 0 ? matched : recommendations.slice(0, 2),
      _mode: 'offline-command-recommendation',
    };
  },
});

const analyzeRequest = tool({
  description: 'ì§ˆë¬¸ì˜ ì˜ë„ì™€ ë³µì¡ë„ë¥¼ í•œ ë²ˆì— ë¶„ì„í•©ë‹ˆë‹¤ (Thinking Step)',
  inputSchema: z.object({
    query: z.string().describe('ì‚¬ìš©ì ì§ˆë¬¸'),
  }),
  execute: async ({ query }: { query: string }) => {
    const lowerQuery = query.toLowerCase();
    let intent = 'general';
    let complexity = 1;

    if (
      lowerQuery.includes('cpu') ||
      lowerQuery.includes('ë©”ëª¨ë¦¬') ||
      lowerQuery.includes('ìƒíƒœ')
    ) {
      intent = 'monitoring';
      complexity = 2;
    } else if (
      lowerQuery.includes('ì¥ì• ') ||
      lowerQuery.includes('ì›ì¸') ||
      lowerQuery.includes('ë¶„ì„')
    ) {
      intent = 'analysis';
      complexity = 4;
    } else if (lowerQuery.includes('ì¶”ì²œ') || lowerQuery.includes('ë°©ë²•')) {
      intent = 'guide';
      complexity = 3;
    }

    const recommendation =
      complexity >= 3
        ? 'unified-processor'
        : complexity >= 2
          ? 'rag-search'
          : 'offline-tool';

    return {
      intent,
      complexity,
      recommendation,
      reasoning: `ì˜ë„: ${intent}, ë³µì¡ë„: ${complexity} -> ì „ëµ: ${recommendation}`,
    };
  },
});

// ============================================================================
// Request/Response Types
// ============================================================================

interface QueryRequest {
  query: string;
  images?: string[]; // Base64 encoded images
  documents?: { name: string; content: string }[]; // Base64 encoded docs (PDFs)
  context?: string;
  temperature?: number;
  maxTokens?: number;
  includeThinking?: boolean;
  thinking?: boolean;
  metadata?: {
    totalServers?: number;
    onlineServers?: number;
    warningServers?: number;
    criticalServers?: number;
    avgCpu?: number;
    avgMemory?: number;
    timestamp?: string;
  };
}

// ... [Existing ModelSelection and selectModels] ...
interface ModelSelection {
  primary: ReturnType<typeof google> | ReturnType<typeof groq>;
  fallback: ReturnType<typeof groq> | ReturnType<typeof google> | null;
  primaryName: string;
  fallbackName: string | null;
  level: 1 | 2 | 3 | 4 | 5 | 'thinking' | 'multimodal';
  useTools: boolean;
  maxTokens: number;
  temperature: number;
}

function selectModels(
  complexity: number,
  thinking: boolean = false,
  hasImages: boolean = false
): ModelSelection {
  const googleApiKey = getGoogleAIKey();
  const googleRateCheck = checkGoogleAIRateLimit();
  const groqAvailable = isGroqAIAvailable();
  const groq8bCheck = checkGroqAIRateLimit('llama-3.1-8b-instant' as GroqModel);
  const groq70bCheck = checkGroqAIRateLimit(
    'llama-3.3-70b-versatile' as GroqModel
  );

  const googleAvailable = googleApiKey && googleRateCheck.allowed;
  const groq8bAllowed = groqAvailable && groq8bCheck.allowed;
  const groq70bAllowed = groqAvailable && groq70bCheck.allowed;

  if (hasImages) {
    if (googleAvailable) {
      return {
        primary: google('gemini-2.5-flash'),
        fallback: google('gemini-2.5-pro'),
        primaryName: 'gemini-2.5-flash',
        fallbackName: 'gemini-2.5-pro',
        level: 'multimodal',
        useTools: true,
        maxTokens: 4096,
        temperature: 0.5,
      };
    }
    throw new Error('ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•œ Google AI ëª¨ë¸ ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.');
  }

  if (thinking) {
    if (googleAvailable) {
      return {
        primary: google('gemini-2.5-pro'),
        fallback: groq70bAllowed ? groq('llama-3.3-70b-versatile') : null,
        primaryName: 'gemini-2.5-pro',
        fallbackName: groq70bAllowed ? 'llama-3.3-70b-versatile' : null,
        level: 'thinking',
        useTools: true,
        maxTokens: 8192,
        temperature: 0.7,
      };
    }
    if (groq70bAllowed) {
      return {
        primary: groq('llama-3.3-70b-versatile'),
        fallback: null,
        primaryName: 'llama-3.3-70b-versatile',
        fallbackName: null,
        level: 'thinking',
        useTools: true,
        maxTokens: 4096,
        temperature: 0.7,
      };
    }
  }

  if (complexity === 5) {
    if (googleAvailable) {
      return {
        primary: google('gemini-2.5-flash'),
        fallback: groq70bAllowed ? groq('llama-3.3-70b-versatile') : null,
        primaryName: 'gemini-2.5-flash',
        fallbackName: groq70bAllowed ? 'llama-3.3-70b-versatile' : null,
        level: 5,
        useTools: true,
        maxTokens: 4096,
        temperature: 0.6,
      };
    }
    if (groq70bAllowed) {
      return {
        primary: groq('llama-3.3-70b-versatile'),
        fallback: groq8bAllowed ? groq('llama-3.1-8b-instant') : null,
        primaryName: 'llama-3.3-70b-versatile',
        fallbackName: groq8bAllowed ? 'llama-3.1-8b-instant' : null,
        level: 5,
        useTools: true,
        maxTokens: 4096,
        temperature: 0.6,
      };
    }
  }

  if (complexity === 4) {
    if (groq70bAllowed) {
      return {
        primary: groq('llama-3.3-70b-versatile'),
        fallback: googleAvailable ? google('gemini-2.5-flash') : null,
        primaryName: 'llama-3.3-70b-versatile',
        fallbackName: googleAvailable ? 'gemini-2.5-flash' : null,
        level: 4,
        useTools: true,
        maxTokens: 4096,
        temperature: 0.5,
      };
    }
    if (googleAvailable) {
      return {
        primary: google('gemini-2.5-flash'),
        fallback: groq8bAllowed ? groq('llama-3.1-8b-instant') : null,
        primaryName: 'gemini-2.5-flash',
        fallbackName: groq8bAllowed ? 'llama-3.1-8b-instant' : null,
        level: 4,
        useTools: true,
        maxTokens: 4096,
        temperature: 0.5,
      };
    }
  }

  if (complexity >= 2 && complexity <= 3) {
    if (groq8bAllowed) {
      return {
        primary: groq('llama-3.1-8b-instant'),
        fallback: googleAvailable ? google('gemini-2.5-flash') : null,
        primaryName: 'llama-3.1-8b-instant',
        fallbackName: googleAvailable ? 'gemini-2.5-flash' : null,
        level: complexity as 2 | 3,
        useTools: true,
        maxTokens: 2048,
        temperature: 0.4,
      };
    }
    if (googleAvailable) {
      return {
        primary: google('gemini-2.5-flash'),
        fallback: null,
        primaryName: 'gemini-2.5-flash',
        fallbackName: null,
        level: complexity as 2 | 3,
        useTools: true,
        maxTokens: 2048,
        temperature: 0.4,
      };
    }
  }

  if (groq8bAllowed) {
    return {
      primary: groq('llama-3.1-8b-instant'),
      fallback: googleAvailable ? google('gemini-2.5-flash') : null,
      primaryName: 'llama-3.1-8b-instant',
      fallbackName: googleAvailable ? 'gemini-2.5-flash' : null,
      level: 1,
      useTools: false,
      maxTokens: 1024,
      temperature: 0.3,
    };
  }

  if (googleAvailable) {
    return {
      primary: google('gemini-2.5-flash'),
      fallback: null,
      primaryName: 'gemini-2.5-flash',
      fallbackName: null,
      level: 1,
      useTools: false,
      maxTokens: 1024,
      temperature: 0.3,
    };
  }

  throw new Error('AI APIê°€ ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤ (Google AI, Groq)');
}

// ============================================================================
// POST Handler (5-Level Routing Architecture v3.2)
// ============================================================================

export const POST = withAuth(async (req: NextRequest) => {
  const startTime = Date.now();

  try {
    // ğŸ“‚ Payload Parsing
    const body: QueryRequest = await req.json();
    const {
      images,
      documents,
      metadata,
      includeThinking = false,
      thinking = false,
    } = body;

    let { query } = body;

    if (
      (!query || typeof query !== 'string') &&
      !(images && images.length > 0) &&
      !(documents && documents.length > 0)
    ) {
      return Response.json(
        { error: 'query, images ë˜ëŠ” documents íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤' },
        { status: 400 }
      );
    }

    // ğŸ“ Document Parsing (PDF/TXT)
    let documentContext = '';
    const parsingSteps: string[] = [];

    if (documents && documents.length > 0) {
      parsingSteps.push(`ğŸ“„ ë¬¸ì„œ ${documents.length}ê°œ ì²˜ë¦¬ ì‹œì‘`);

      for (const doc of documents) {
        try {
          let text = '';
          if (doc.name.toLowerCase().endsWith('.pdf')) {
            const buffer = Buffer.from(doc.content, 'base64');
            const data = await pdf(buffer);
            text = data.text;
            parsingSteps.push(
              `âœ… PDF íŒŒì‹± ì„±ê³µ: ${doc.name} (${text.length}ì)`
            );
          } else {
            // TXT, MD, etc (assume base64 encoded text or just plain text if decoded)
            // Check if content is base64
            try {
              text = Buffer.from(doc.content, 'base64').toString('utf-8');
            } catch {
              text = doc.content;
            }
            parsingSteps.push(`âœ… í…ìŠ¤íŠ¸ ë¡œë“œ ì„±ê³µ: ${doc.name}`);
          }

          documentContext += `\n--- [Document: ${doc.name}] ---\n${text.slice(0, 30000)}\n---------------------------\n`; // 30k chars limit per doc for safety
        } catch (e: any) {
          console.error(`âŒ ë¬¸ì„œ íŒŒì‹± ì‹¤íŒ¨ (${doc.name}):`, e);
          parsingSteps.push(`âŒ íŒŒì‹± ì‹¤íŒ¨ (${doc.name}): ${e.message}`);
        }
      }
    }

    if (documentContext) {
      query += `\n\n[ì²¨ë¶€ ë¬¸ì„œ ë‚´ìš©]\n${documentContext}`;
    }

    // ============================================================
    // Step 1: Router (8B) - ë¹ ë¥¸ ë³µì¡ë„ ë¶„ë¥˜
    // ============================================================
    let complexity = 1;
    let intent = 'general';
    let routingReason = 'default';

    if (images && images.length > 0) {
      complexity = 5;
      intent = 'multimodal_analysis';
      routingReason = 'Image input detected -> Force Multimodal';
    } else if (documents && documents.length > 0) {
      // ë¬¸ì„œê°€ ìˆìœ¼ë©´ ë¶„ì„ í•„ìš”í•˜ë¯€ë¡œ ë³µì¡ë„ ìƒí–¥
      complexity = 4; // Use Groq 70B or Gemini Flash
      intent = 'document_analysis';
      routingReason = 'Document attached';
    } else {
      const classification = await classifyQuery(query);
      complexity = classification.complexity;
      intent = classification.intent;
      routingReason = classification.reasoning;
    }

    // ============================================================
    // Step 2: Model Selection (5-Level Architecture)
    // ============================================================
    let modelSelection: ModelSelection;
    try {
      modelSelection = selectModels(
        complexity,
        thinking,
        !!(images && images.length > 0)
      );
    } catch (error) {
      return Response.json(
        {
          error:
            error instanceof Error
              ? error.message
              : 'AI ëª¨ë¸ ì„ íƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ',
        },
        { status: 500 }
      );
    }

    const {
      primary,
      fallback,
      primaryName,
      fallbackName,
      level,
      useTools,
      maxTokens,
      temperature,
    } = modelSelection;

    // ... [System Prompt & Tool Selection logic same as before but now includes documents in context implicitly] ...

    // Level í‘œì‹œ ë¬¸ìì—´ ìƒì„±
    const levelDisplay =
      level === 'multimodal'
        ? 'ğŸ–¼ï¸ Vision (Gemini)'
        : level === 'thinking'
          ? 'ğŸ§  Thinking (Pro)'
          : level === 5
            ? 'âš¡ Advanced (Flash)'
            : level === 4
              ? 'ğŸ“Š Complex (70B)'
              : level >= 2
                ? 'ğŸ”§ Tool-enabled (8B)'
                : 'ğŸ’¬ Direct (8B)';

    const systemPrompt = `ë‹¹ì‹ ì€ **OpenManager Vibe**ì˜ **AI ì–´ì‹œìŠ¤í„´íŠ¸**ì…ë‹ˆë‹¤.
í˜„ì¬ ëª¨ë“œ: ${levelDisplay}
ì§ˆë¬¸ ì˜ë„: ${intent} (ë³µì¡ë„: ${complexity}/5)
${thinking ? 'ğŸ§  **Thinking ëª¨ë“œ í™œì„±í™”**: ê¹Šì€ ì¶”ë¡ ê³¼ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.\n' : ''}
${documentContext ? 'ğŸ“„ **ë¬¸ì„œ ì²¨ë¶€ë¨**: ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.\n' : ''}

**í˜„ì¬ ëŒ€ì‹œë³´ë“œ ì»¨í…ìŠ¤íŠ¸:**
${
  metadata
    ? `- ì´ ì„œë²„: ${metadata.totalServers || 0}ê°œ
- ì •ìƒ ì„œë²„: ${metadata.onlineServers || 0}ê°œ
- ê²½ê³  ì„œë²„: ${metadata.warningServers || 0}ê°œ
- ì‹¬ê° ì„œë²„: ${metadata.criticalServers || 0}ê°œ
- í‰ê·  CPU: ${metadata.avgCpu || 0}%
- í‰ê·  ë©”ëª¨ë¦¬: ${metadata.avgMemory || 0}%`
    : 'ë©”íƒ€ë°ì´í„° ì—†ìŒ'
}

**ğŸš¨ ì²˜ë¦¬ ì „ëµ (5-Level Routing v3.2)**
- Level 1: ê°„ë‹¨í•œ ì¸ì‚¬, FAQ â†’ ì§ì ‘ ì‘ë‹µ (ë„êµ¬ ì—†ì´)
- Level 2-3: ì„œë²„ ìƒíƒœ, ë©”íŠ¸ë¦­ ì¡°íšŒ â†’ \`getServerMetrics\`, \`searchKnowledgeBase\`
- Level 4: ë³µì¡í•œ ë¶„ì„, ë¬¸ì„œ ìš”ì•½ â†’ \`callUnifiedProcessor\`
- Level 5: ê³ ê¸‰ ë¶„ì„, ì˜ˆì¸¡ â†’ ëª¨ë“  ë„êµ¬ í™œìš©
- Thinking: ì‹¬ì¸µ ì¶”ë¡ , ì „ëµ ìˆ˜ë¦½ â†’ ì¢…í•©ì  ë¶„ì„
- Multimodal: ì´ë¯¸ì§€ ë¶„ì„ ëª¨ë“œ.

${useTools ? '**ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:** getServerMetrics, searchKnowledgeBase, callUnifiedProcessor, analyzePattern, recommendCommands' : '**ì§ì ‘ ì‘ë‹µ ëª¨ë“œ:** ë„êµ¬ ì—†ì´ ì¦‰ì‹œ ë‹µë³€í•©ë‹ˆë‹¤.'}

í•­ìƒ íŒ©íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¶ˆí™•ì‹¤í•  ê²½ìš° ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  í•˜ì‹­ì‹œì˜¤.
í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì‹­ì‹œì˜¤.`;

    // ============================================================
    // Step 4: Primary Model ì‹¤í–‰
    // ============================================================
    let responseText = '';
    let usedEngine = primaryName;
    let fallbackUsed = false;
    const thinkingSteps: string[] = [];

    if (includeThinking) {
      if (parsingSteps.length > 0) thinkingSteps.push(...parsingSteps);
      thinkingSteps.push(`ğŸ” Router: ${intent} (ë³µì¡ë„ ${complexity}/5)`);
      thinkingSteps.push(`ğŸ¯ Level: ${level} â†’ ${primaryName}`);
      thinkingSteps.push(`ğŸ”§ Tools: ${useTools ? 'í™œì„±í™”' : 'ë¹„í™œì„±í™”'}`);
      if (thinking) thinkingSteps.push(`ğŸ§  Thinking ëª¨ë“œ: í™œì„±í™”`);
    }

    const allTools = {
      analyzeRequest,
      callUnifiedProcessor,
      getServerMetrics,
      searchKnowledgeBase,
      analyzePattern,
      recommendCommands,
    };

    const tools = useTools ? allTools : undefined;

    // messages êµ¬ì„±
    const userMessageContent: any[] = [{ type: 'text', text: query }];
    if (images && images.length > 0) {
      images.forEach((img) => {
        userMessageContent.push({ type: 'image', image: img });
      });
    }

    try {
      const result = await generateText({
        model: primary,
        messages: [
          { role: 'user', content: userMessageContent },
        ] as CoreMessage[],
        tools,
        system: systemPrompt,
        maxOutputTokens: maxTokens,
        temperature: temperature,
      });

      responseText = result.text || 'ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.';

      if (includeThinking && result.toolCalls && result.toolCalls.length > 0) {
        for (const toolCall of result.toolCalls) {
          thinkingSteps.push(
            `ğŸ”§ ${toolCall.toolName}: ${JSON.stringify('args' in toolCall ? toolCall.args : {})}`
          );
        }
      }
      if (includeThinking && result.usage) {
        thinkingSteps.push(`ğŸ“Š í† í°: ${result.usage.totalTokens}`);
      }
    } catch (primaryError) {
      console.warn(`âš ï¸ Primary Model (${primaryName}) ì‹¤íŒ¨:`, primaryError);

      if (fallback && fallbackName) {
        if (includeThinking)
          thinkingSteps.push(`âš ï¸ ${primaryName} ì‹¤íŒ¨ â†’ ${fallbackName} ì „í™˜`);

        try {
          // Fallback logic specific to multimodal (exclude images if needed)
          let fallbackMessages: CoreMessage[] = [
            { role: 'user', content: userMessageContent },
          ] as CoreMessage[];
          if (
            !fallbackName.includes('gemini') &&
            !fallbackName.includes('vision')
          ) {
            fallbackMessages = [{ role: 'user', content: query }]; // ë¬¸ì„œ ë‚´ìš©ì€ ì¿¼ë¦¬ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ OK
          }

          const fallbackResult = await generateText({
            model: fallback,
            messages: fallbackMessages,
            tools,
            system: systemPrompt + '\n(Fallback mode)',
            maxOutputTokens: maxTokens,
            temperature: temperature,
          });

          responseText = fallbackResult.text || 'ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.';
          usedEngine = fallbackName;
          fallbackUsed = true;

          if (includeThinking && fallbackResult.toolCalls) {
            for (const toolCall of fallbackResult.toolCalls) {
              thinkingSteps.push(`ğŸ”§ ${toolCall.toolName}`);
            }
          }
        } catch (fallbackError) {
          throw primaryError;
        }
      } else {
        throw primaryError;
      }
    }

    const responseTime = Date.now() - startTime;

    return Response.json({
      response: responseText,
      thinkingSteps,
      engine: usedEngine,
      routing: { level, complexity, intent },
      fallbackUsed,
      responseTime,
      timestamp: new Date().toISOString(),
    });
  } catch (error) {
    console.error('âŒ AI ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    return Response.json(
      { error: 'AI ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' },
      { status: 500 }
    );
  }
});
