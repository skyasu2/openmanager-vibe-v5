/**
 * ğŸŒ Google AI (Gemini) ìƒì„± API
 *
 * Gemini Pro ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±
 * POST /api/ai/google-ai/generate
 */

import type { NextRequest } from 'next/server';
import { NextResponse } from 'next/server';
import { createApiRoute } from '@/lib/api/zod-middleware';
import {
  GoogleAIGenerateRequestSchema,
  GoogleAIGenerateResponseSchema,
  GoogleAIStatusResponseSchema,
  type GoogleAIGenerateRequest,
  type GoogleAIGenerateResponse,
  type GoogleAIStatusResponse,
  type GoogleAIErrorResponse,
} from '@/schemas/api.schema';
import { getErrorMessage } from '@/types/type-utils';
import { getGoogleAIModel, shouldUseMockGoogleAI } from '@/lib/ai/google-ai-client';

export const runtime = 'nodejs';

// POST handler
const postHandler = createApiRoute()
  .body(GoogleAIGenerateRequestSchema)
  .response(GoogleAIGenerateResponseSchema)
  .configure({
    showDetailedErrors: process.env.NODE_ENV === 'development',
    enableLogging: true,
  })
  .build(async (_request, context): Promise<GoogleAIGenerateResponse> => {
    console.log('ğŸŒ Google AI ìƒì„± ìš”ì²­ ì²˜ë¦¬ ì‹œì‘...');

    const {
      prompt,
      temperature,
      maxTokens,
      model,
    } = context.body;

    const startTime = Date.now();

    // Mock ì‚¬ìš© ì—¬ë¶€ ë¡œê·¸
    if (shouldUseMockGoogleAI) {
      console.log('ğŸ­ Mock Google AIë¡œ ì‘ë‹µ ìƒì„± ì¤‘...');
    }

    // Google AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œ ë˜ëŠ” Mock ìë™ ì„ íƒ)
    const generativeModel = getGoogleAIModel(model || 'gemini-pro');

    // ìƒì„± ì„¤ì •
    const generationConfig = {
      temperature,
      maxOutputTokens: maxTokens,
      topK: 40,
      topP: 0.95,
    };

    // í…ìŠ¤íŠ¸ ìƒì„±
    const result = await generativeModel.generateContent({
      contents: [{ role: 'user', parts: [{ text: prompt }] }],
      generationConfig,
    });

    const response = result.response;
    const text = response.text();

    const processingTime = Date.now() - startTime;

    console.log(`âœ… Google AI ìƒì„± ì™„ë£Œ: ${processingTime}ms`);

    return {
      success: true,
      response: text,
      text, // í˜¸í™˜ì„±ì„ ìœ„í•´ ë‘˜ ë‹¤ ì œê³µ
      model: model || 'gemini-pro',
      confidence: 0.9, // Google AIëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì‹ ë¢°ë„
      metadata: {
        temperature: temperature || 0.7,
        maxTokens: maxTokens || 1000,
        actualTokens: response.usageMetadata?.totalTokenCount || 0,
        promptTokens: response.usageMetadata?.promptTokenCount || 0,
        completionTokens: response.usageMetadata?.candidatesTokenCount || 0,
        processingTime,
      },
      timestamp: new Date().toISOString(),
    };
  });

export async function POST(request: NextRequest) {
  try {
    const response = await postHandler(request);
    const responseData = await response.json();
    return NextResponse.json(responseData, {
      headers: {
        'Cache-Control': 'no-store',
        'X-Processing-Time': `${responseData.metadata?.processingTime || 0}ms`,
      },
    });
  } catch (error) {
    console.error('âŒ Google AI ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨:', error);

    const errorMessage = getErrorMessage(error);
    
    // API í•œë„ ì´ˆê³¼ ë“±ì˜ íŠ¹ì • ì˜¤ë¥˜ ì²˜ë¦¬
    if (
      errorMessage.includes('quota') ||
      errorMessage.includes('limit')
    ) {
      return NextResponse.json(
        {
          success: false,
          error: 'API quota exceeded',
          message:
            'Google AI API ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
          timestamp: new Date().toISOString(),
        } satisfies GoogleAIErrorResponse,
        { status: 429 }
      );
    }

    return NextResponse.json(
      {
        success: false,
        error: 'Request processing failed',
        message: errorMessage,
        timestamp: new Date().toISOString(),
      } satisfies GoogleAIErrorResponse,
      { status: 500 }
    );
  }
}

/**
 * ğŸ“Š Google AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
 *
 * GET /api/ai/google-ai/generate
 */
// GET handler
const getHandler = createApiRoute()
  .response(GoogleAIStatusResponseSchema)
  .configure({
    showDetailedErrors: process.env.NODE_ENV === 'development',
    enableLogging: true,
  })
  .build(async (): Promise<GoogleAIStatusResponse> => {
    const isUsingMock = shouldUseMockGoogleAI;
    const apiKey =
      process.env.GOOGLE_AI_API_KEY ||
      process.env.NEXT_PUBLIC_GOOGLE_AI_API_KEY;
    const isConfigured = isUsingMock || !!apiKey;

    return {
      success: true,
      service: 'google-ai-generate',
      status: isConfigured ? 'active' : 'not_configured',
      configured: isConfigured,
      models: ['gemini-pro', 'gemini-pro-vision'],
      capabilities: {
        textGeneration: true,
        streaming: false,
        multimodal: false, // í˜„ì¬ëŠ” í…ìŠ¤íŠ¸ë§Œ ì§€ì›
      },
      ...(isUsingMock && { mockMode: true }),
      timestamp: new Date().toISOString(),
    };
  });

export async function GET(_request: NextRequest) {
  try {
    const response = await getHandler(_request);
    return NextResponse.json(response, {
      headers: {
        'Cache-Control': 'public, max-age=60',
      },
    });
  } catch (error) {
    return NextResponse.json(
      {
        success: false,
        error: 'Status check failed',
        message: getErrorMessage(error),
        timestamp: new Date().toISOString(),
      } satisfies GoogleAIErrorResponse,
      { status: 500 }
    );
  }
}
