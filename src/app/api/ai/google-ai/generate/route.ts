/**
 * ğŸŒ Google AI (Gemini) ìƒì„± API
 *
 * Gemini Pro ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±
 * POST /api/ai/google-ai/generate
 */

import type { NextRequest } from 'next/server';
import { NextResponse } from 'next/server';
import { getGoogleAIModel } from '@/lib/ai/google-ai-client';
import { createApiRoute } from '@/lib/api/zod-middleware';
import { withAuth } from '@/lib/auth/api-auth';
import {
  type GoogleAIErrorResponse,
  GoogleAIGenerateRequestSchema,
  type GoogleAIGenerateResponse,
  GoogleAIGenerateResponseSchema,
  type GoogleAIStatusResponse,
  GoogleAIStatusResponseSchema,
} from '@/schemas/api.schema';
import { MockContextLoader } from '@/services/ai/MockContextLoader';
import { getErrorMessage } from '@/types/type-utils';
import debug from '@/utils/debug';

export const runtime = 'nodejs';

// ì§€ëŠ¥ì  ì»¨í…ìŠ¤íŠ¸ ê²°ì •ì„ ìœ„í•œ ì§ˆì˜ ë¶„ì„ ì‹œìŠ¤í…œ
interface QueryAnalysis {
  needsServerData: boolean;
  category:
    | 'greeting'
    | 'server_status'
    | 'performance'
    | 'troubleshooting'
    | 'general'
    | 'thanks';
  confidence: number;
}

// í–¥ìƒëœ ì§ˆì˜ ë¶„ì„ í•¨ìˆ˜
function analyzeQuery(query: string): QueryAnalysis {
  const lowerQuery = query.toLowerCase().trim();

  // ì¸ì‚¬ë§ íŒ¨í„´
  const greetingPatterns = [
    'ì•ˆë…•',
    'hello',
    'hi',
    'ì•ˆë…•í•˜ì„¸ìš”',
    'ì¢‹ì€',
    'ë°˜ê°€ì›Œ',
  ];
  if (greetingPatterns.some((pattern) => lowerQuery.includes(pattern))) {
    return { needsServerData: false, category: 'greeting', confidence: 0.9 };
  }

  // ê°ì‚¬ ì¸ì‚¬ íŒ¨í„´
  const thanksPatterns = ['ê°ì‚¬', 'thank', 'ê³ ë§ˆì›Œ', 'ë„ì›€'];
  if (
    thanksPatterns.some((pattern) => lowerQuery.includes(pattern)) &&
    lowerQuery.length < 50
  ) {
    return { needsServerData: false, category: 'thanks', confidence: 0.9 };
  }

  // ì„œë²„ ìƒíƒœ ê´€ë ¨ í‚¤ì›Œë“œ (ê°•í™”ëœ íŒ¨í„´)
  const serverKeywords = ['ì„œë²„', 'server', 'ì‹œìŠ¤í…œ', 'system'];
  const statusKeywords = [
    'ìƒíƒœ',
    'status',
    'í˜„ì¬',
    'current',
    'ì–´ë–¤',
    'how',
    'í™•ì¸',
    'check',
    'ëª¨ë‹ˆí„°ë§',
    'monitoring',
  ];
  const performanceKeywords = [
    'cpu',
    'memory',
    'disk',
    'ë©”ëª¨ë¦¬',
    'ë””ìŠ¤í¬',
    'ì„±ëŠ¥',
    'performance',
    'ì‚¬ìš©ë¥ ',
    'usage',
  ];
  const troubleshootingKeywords = [
    'ë¬¸ì œ',
    'problem',
    'error',
    'ì—ëŸ¬',
    'ì¥ì• ',
    'issue',
    'ê²½ê³ ',
    'warning',
    'ì•Œë¦¼',
    'alert',
  ];

  const hasServerKeyword = serverKeywords.some((keyword) =>
    lowerQuery.includes(keyword)
  );
  const hasStatusKeyword = statusKeywords.some((keyword) =>
    lowerQuery.includes(keyword)
  );
  const hasPerformanceKeyword = performanceKeywords.some((keyword) =>
    lowerQuery.includes(keyword)
  );
  const hasTroubleshootingKeyword = troubleshootingKeywords.some((keyword) =>
    lowerQuery.includes(keyword)
  );

  // ì„œë²„ ìƒíƒœ ì¿¼ë¦¬ íŒë‹¨
  if (hasServerKeyword && hasStatusKeyword) {
    return {
      needsServerData: true,
      category: 'server_status',
      confidence: 0.95,
    };
  }

  // ì„±ëŠ¥ ê´€ë ¨ ì¿¼ë¦¬
  if (hasPerformanceKeyword || (hasServerKeyword && hasPerformanceKeyword)) {
    return { needsServerData: true, category: 'performance', confidence: 0.9 };
  }

  // íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê´€ë ¨ ì¿¼ë¦¬
  if (hasTroubleshootingKeyword) {
    return {
      needsServerData: true,
      category: 'troubleshooting',
      confidence: 0.85,
    };
  }

  // íŠ¹ì • ì„œë²„ ë¦¬ì†ŒìŠ¤ ì–¸ê¸‰
  if (hasPerformanceKeyword) {
    return { needsServerData: true, category: 'performance', confidence: 0.8 };
  }

  // ê¸°ë³¸ê°’: ì¼ë°˜ì ì¸ ì§ˆë¬¸
  return { needsServerData: false, category: 'general', confidence: 0.6 };
}

// ë ˆê±°ì‹œ í•¨ìˆ˜ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)
function _isServerStatusQuery(query: string): boolean {
  return analyzeQuery(query).needsServerData;
}

// ì§€ëŠ¥ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
function buildEnhancedPrompt(originalPrompt: string): {
  prompt: string;
  analysis: QueryAnalysis;
} {
  const analysis = analyzeQuery(originalPrompt);
  const mockContextLoader = MockContextLoader.getInstance();

  const mockContext = mockContextLoader.getMockContext();

  // í”„ë¡œë•ì…˜ì—ì„œëŠ” ê°„ì†Œí™”ëœ ë¡œê¹…
  if (process.env.NODE_ENV === 'development') {
    debug.log('ğŸ” ì§ˆì˜ ë¶„ì„:', {
      needsServerData: analysis.needsServerData,
      category: analysis.category,
      confidence: analysis.confidence,
      mockContextAvailable: !!mockContext,
      serverCount: mockContext?.servers?.length || 0,
    });
  } else {
    // í”„ë¡œë•ì…˜ì—ì„œëŠ” ì¤‘ìš”í•œ ì •ë³´ë§Œ ë¡œê¹…
    if (analysis.needsServerData && !mockContext) {
      debug.warn('MockContextê°€ nullì…ë‹ˆë‹¤', { category: analysis.category });
    }
  }

  // ì„œë²„ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš°
  if (!analysis.needsServerData || !mockContext) {
    if (analysis.needsServerData && !mockContext) {
      debug.warn('âš ï¸ ì„œë²„ ë°ì´í„°ê°€ í•„ìš”í•˜ì§€ë§Œ MockContextê°€ nullì…ë‹ˆë‹¤', {
        needsServerData: analysis.needsServerData,
        mockContext: !!mockContext,
        category: analysis.category,
        confidence: analysis.confidence,
      });
    } else {
      debug.log('â„¹ï¸ Mock ë°ì´í„° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ:', {
        reason: !analysis.needsServerData
          ? 'needsServerData=false'
          : 'mockContext=null',
      });
    }
    let contextualPrompt = originalPrompt;

    // ì¹´í…Œê³ ë¦¬ë³„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    switch (analysis.category) {
      case 'greeting':
        contextualPrompt = `${originalPrompt}\n\nì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” OpenManager VIBEì˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì„œë²„ ëª¨ë‹ˆí„°ë§ê³¼ ì‹œìŠ¤í…œ ê´€ë¦¬ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.`;
        break;
      case 'thanks':
        contextualPrompt = `${originalPrompt}\n\nì²œë§Œì—ìš”! ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”. ì„œë²„ ìƒíƒœ í™•ì¸, ì„±ëŠ¥ ë¶„ì„, ë¬¸ì œ í•´ê²° ë“± ë‹¤ì–‘í•œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.`;
        break;
      case 'general':
        contextualPrompt = `${originalPrompt}\n\nì¼ë°˜ì ì¸ ì§ˆë¬¸ì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ë§Œì•½ ì„œë²„ë‚˜ ì‹œìŠ¤í…œ ê´€ë ¨ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ êµ¬ì²´ì ìœ¼ë¡œ ë§ì”€í•´ ì£¼ì„¸ìš”.`;
        break;
    }

    return { prompt: contextualPrompt, analysis };
  }

  // ì‹¤ì‹œê°„ ì„œë²„ ë°ì´í„°ë§Œ ì œê³µ (ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ ì œê±°)
  let enhancedPrompt = `ì‚¬ìš©ì ì§ˆë¬¸: ${originalPrompt}\n\n`;

  // ìˆœìˆ˜í•œ ì‹¤ì‹œê°„ ì„œë²„ ë©”íŠ¸ë¦­ë§Œ ì œê³µ
  enhancedPrompt += `ğŸ“Š í˜„ì¬ ì‹¤ì‹œê°„ ì„œë²„ ìƒíƒœ:\n`;
  enhancedPrompt += `â° í˜„ì¬ ì‹œê°: ${mockContext.currentTime}\n`;

  enhancedPrompt += `ğŸ“ˆ ì „ì²´ ì„œë²„ ë©”íŠ¸ë¦­:\n`;
  enhancedPrompt += `- ì „ì²´ ì„œë²„: ${mockContext.metrics.serverCount}ëŒ€\n`;
  enhancedPrompt += `- ìœ„í—˜ ìƒíƒœ: ${mockContext.metrics.criticalCount}ëŒ€\n`;
  enhancedPrompt += `- ê²½ê³  ìƒíƒœ: ${mockContext.metrics.warningCount}ëŒ€\n`;
  enhancedPrompt += `- ì •ìƒ ìƒíƒœ: ${mockContext.metrics.healthyCount}ëŒ€\n`;
  enhancedPrompt += `- í‰ê·  CPU: ${mockContext.metrics.avgCpu}%\n`;
  enhancedPrompt += `- í‰ê·  ë©”ëª¨ë¦¬: ${mockContext.metrics.avgMemory}%\n`;
  enhancedPrompt += `- í‰ê·  ë””ìŠ¤í¬: ${mockContext.metrics.avgDisk}%\n\n`;

  enhancedPrompt += `ğŸ“Š íŠ¸ë Œë“œ ë¶„ì„:\n`;
  enhancedPrompt += `- CPU íŠ¸ë Œë“œ: ${mockContext.trends.cpuTrend}\n`;
  enhancedPrompt += `- ë©”ëª¨ë¦¬ íŠ¸ë Œë“œ: ${mockContext.trends.memoryTrend}\n`;
  enhancedPrompt += `- ì•Œë¦¼ íŠ¸ë Œë“œ: ${mockContext.trends.alertTrend}\n\n`;

  // ê°œë³„ ì„œë²„ ìƒì„¸ ì •ë³´ (ìƒìœ„ 5ê°œë§Œ)
  if (mockContext.servers.length > 0) {
    enhancedPrompt += `ğŸ–¥ï¸ ì£¼ìš” ì„œë²„ ìƒì„¸ ì •ë³´:\n`;
    mockContext.servers.slice(0, 5).forEach((server, index) => {
      enhancedPrompt += `${index + 1}. ${server.name} (${server.type})\n`;
      enhancedPrompt += `   - ìƒíƒœ: ${server.status}\n`;
      enhancedPrompt += `   - CPU: ${server.cpu}%, ë©”ëª¨ë¦¬: ${server.memory}%, ë””ìŠ¤í¬: ${server.disk}%\n`;
      enhancedPrompt += `   - ì‘ë‹µì‹œê°„: ${server.responseTime || 'N/A'}ms\n\n`;
    });
  }

  enhancedPrompt += `\nìœ„ì˜ ì‹¤ì‹œê°„ ì„œë²„ ë°ì´í„°ë¥¼ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë°ì´í„°ì—ì„œ ê´€ì°°ë˜ëŠ” íŒ¨í„´ì´ë‚˜ ì´ìƒ ì§•í›„ê°€ ìˆë‹¤ë©´ ì„¤ëª…í•´ì£¼ì„¸ìš”.`;

  return { prompt: enhancedPrompt, analysis };
}

// POST handler with AI Assistant access control
const postHandler = createApiRoute()
  .body(GoogleAIGenerateRequestSchema)
  .response(GoogleAIGenerateResponseSchema)
  .configure({
    showDetailedErrors: process.env.NODE_ENV === 'development',
    enableLogging: true,
  })
  .build(async (request, context): Promise<GoogleAIGenerateResponse> => {
    // ğŸ”’ AI Assistant ì „ìš© ì ‘ê·¼ ì œì–´
    const aiAssistantHeader = request.headers.get('X-AI-Assistant');
    const userAgent = request.headers.get('User-Agent') || '';
    const isDiagnosticMode =
      request.headers.get('X-Diagnostic-Mode') === 'true';

    // AI ì–´ì‹œìŠ¤í„´íŠ¸ì—ì„œ Google AI ëª¨ë“œë¡œ í˜¸ì¶œëœ ê²½ìš°ë§Œ í—ˆìš©
    const isValidAIAssistant =
      aiAssistantHeader === 'true' ||
      userAgent.includes('AI-Assistant') ||
      isDiagnosticMode; // ì§„ë‹¨ ëª¨ë“œ í—ˆìš©

    if (!isValidAIAssistant) {
      debug.warn('âŒ Google AI API ë¬´ë‹¨ ì ‘ê·¼ ì‹œë„ ì°¨ë‹¨ë¨', {
        aiAssistant: aiAssistantHeader,
        userAgent: userAgent.substring(0, 50),
        diagnostic: isDiagnosticMode,
      });

      // êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ë¡œ ê°œì„ 
      const errorDetails = {
        message: 'AI Assistant ì „ìš© API ì ‘ê·¼ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤.',
        requiredHeaders: [
          'X-AI-Assistant: true',
          'User-Agent containing AI-Assistant',
          'X-Diagnostic-Mode: true (í…ŒìŠ¤íŠ¸ìš©)',
        ],
        currentHeaders: {
          'X-AI-Assistant': aiAssistantHeader,
          'User-Agent': userAgent.substring(0, 50),
          'X-Diagnostic-Mode': isDiagnosticMode,
        },
      };

      throw new Error(JSON.stringify(errorDetails));
    }

    debug.log('ğŸŒ Google AI ìƒì„± ìš”ì²­ ì²˜ë¦¬ ì‹œì‘... (AI Assistant)');

    const { prompt, temperature, maxTokens, model } = context.body;

    const startTime = Date.now();

    // ğŸš€ ì§€ëŠ¥ì  ì»¨í…ìŠ¤íŠ¸ ê²°ì •ì„ í†µí•œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
    const { prompt: enhancedPrompt, analysis } = buildEnhancedPrompt(prompt);

    debug.log(
      `ğŸ¯ ì§ˆì˜ ë¶„ì„ ì™„ë£Œ - ì¹´í…Œê³ ë¦¬: ${analysis.category}, ì„œë²„ ë°ì´í„° í•„ìš”: ${analysis.needsServerData}, ì‹ ë¢°ë„: ${analysis.confidence}`
    );

    // Google AI ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    const generativeModel = getGoogleAIModel(model || 'gemini-1.5-flash');

    // ìƒì„± ì„¤ì •
    const generationConfig = {
      temperature,
      maxOutputTokens: maxTokens,
      topK: 40,
      topP: 0.95,
    };

    // í…ìŠ¤íŠ¸ ìƒì„± (í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
    const result = await generativeModel.generateContent({
      contents: [{ role: 'user', parts: [{ text: enhancedPrompt }] }],
      generationConfig,
    });

    const response = result.response;
    const text = response.text();

    const processingTime = Date.now() - startTime;

    debug.log(`âœ… Google AI ìƒì„± ì™„ë£Œ: ${processingTime}ms`);

    return {
      success: true,
      response: text,
      text, // í˜¸í™˜ì„±ì„ ìœ„í•´ ë‘˜ ë‹¤ ì œê³µ
      model: model || 'gemini-1.5-flash',
      confidence: 0.9, // Google AIëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ì‹ ë¢°ë„
      metadata: {
        temperature: temperature || 0.7,
        maxTokens: maxTokens || 1000,
        actualTokens: response.usageMetadata?.totalTokenCount || 0,
        promptTokens: response.usageMetadata?.promptTokenCount || 0,
        completionTokens: response.usageMetadata?.candidatesTokenCount || 0,
        processingTime,
      },
      timestamp: new Date().toISOString(),
    };
  });

export const POST = withAuth(async (request: NextRequest) => {
  try {
    const response = await postHandler(request);
    const responseData = await response.json();
    return NextResponse.json(responseData, {
      headers: {
        'Cache-Control': 'no-store',
        'X-Processing-Time': `${responseData.metadata?.processingTime || 0}ms`,
      },
    });
  } catch (error) {
    debug.error('âŒ Google AI ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨:', error);

    const errorMessage = getErrorMessage(error);

    // ì ‘ê·¼ ì œì–´ ì—ëŸ¬ ì²˜ë¦¬ (JSON íŒŒì‹± ì‹œë„)
    if (errorMessage.includes('AI Assistant ì „ìš© API ì ‘ê·¼ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤')) {
      try {
        const errorDetails = JSON.parse(errorMessage);
        return NextResponse.json(
          {
            success: false,
            error: 'Access denied',
            message: errorDetails.message,
            timestamp: new Date().toISOString(),
          } satisfies GoogleAIErrorResponse,
          { status: 403 }
        );
      } catch {
        return NextResponse.json(
          {
            success: false,
            error: 'Access denied',
            message:
              'AI Assistant ì „ìš© API ì ‘ê·¼ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤. í•„ìš”í•œ í—¤ë”ë¥¼ í¬í•¨í•˜ì—¬ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.',
            timestamp: new Date().toISOString(),
          } satisfies GoogleAIErrorResponse,
          { status: 403 }
        );
      }
    }

    // API í‚¤ ê´€ë ¨ ì—ëŸ¬ ì²˜ë¦¬
    if (errorMessage.includes('API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤')) {
      return NextResponse.json(
        {
          success: false,
          error: 'API key not configured',
          message:
            'Google AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.',
          timestamp: new Date().toISOString(),
        } satisfies GoogleAIErrorResponse,
        { status: 503 }
      );
    }

    // API í•œë„ ì´ˆê³¼ ë“±ì˜ íŠ¹ì • ì˜¤ë¥˜ ì²˜ë¦¬
    if (errorMessage.includes('quota') || errorMessage.includes('limit')) {
      return NextResponse.json(
        {
          success: false,
          error: 'API quota exceeded',
          message:
            'Google AI API ì‚¬ìš©ëŸ‰ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
          timestamp: new Date().toISOString(),
        } satisfies GoogleAIErrorResponse,
        { status: 429 }
      );
    }

    return NextResponse.json(
      {
        success: false,
        error: 'Request processing failed',
        message: errorMessage,
        timestamp: new Date().toISOString(),
      } satisfies GoogleAIErrorResponse,
      { status: 500 }
    );
  }
});

/**
 * ğŸ“Š Google AI ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
 *
 * GET /api/ai/google-ai/generate
 */
// GET handler
const getHandler = createApiRoute()
  .response(GoogleAIStatusResponseSchema)
  .configure({
    showDetailedErrors: process.env.NODE_ENV === 'development',
    enableLogging: true,
  })
  .build((): GoogleAIStatusResponse => {
    const apiKey =
      process.env.GOOGLE_AI_API_KEY ||
      process.env.NEXT_PUBLIC_GOOGLE_AI_API_KEY;
    const isConfigured = !!apiKey;

    return {
      success: true,
      service: 'google-ai-generate',
      status: isConfigured ? 'active' : 'not_configured',
      configured: isConfigured,
      models: ['gemini-1.5-flash', 'gemini-1.5-pro'],
      capabilities: {
        textGeneration: true,
        streaming: false,
        multimodal: false, // í˜„ì¬ëŠ” í…ìŠ¤íŠ¸ë§Œ ì§€ì›
      },
      timestamp: new Date().toISOString(),
    };
  });

export const GET = withAuth(async (_request: NextRequest) => {
  try {
    const response = await getHandler(_request);
    return NextResponse.json(response, {
      headers: {
        'Cache-Control': 'public, max-age=60',
      },
    });
  } catch (error) {
    return NextResponse.json(
      {
        success: false,
        error: 'Status check failed',
        message: getErrorMessage(error),
        timestamp: new Date().toISOString(),
      } satisfies GoogleAIErrorResponse,
      { status: 500 }
    );
  }
});
