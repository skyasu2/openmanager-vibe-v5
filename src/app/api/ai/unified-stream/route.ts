import { google } from '@ai-sdk/google';
import { groq } from '@ai-sdk/groq';
import { type CoreMessage, streamText, tool } from 'ai';
import type { NextRequest } from 'next/server';
import { z } from 'zod';
import { getGoogleAIKey } from '@/lib/ai/google-ai-manager';
import { classifyQuery } from '@/lib/ai/query-classifier'; // [NEW]
import { withAuth } from '@/lib/auth/api-auth';
import { createClient } from '@/lib/supabase/server';
import { SupabaseRAGEngine } from '@/services/ai/supabase-rag-engine';
import { loadHourlyScenarioData } from '@/services/scenario/scenario-loader';

// Allow streaming responses up to 60 seconds (increased for Pro model)
export const maxDuration = 60;

// [Previous Tools Definition Omitted for Brevity - They are preserved in the file context or should be re-declared if replacing whole file.
// Since replace_file_content replaces a block, I need to be careful.
// The user prompt implies I should rewrite the file or careful replace.
// I will rewrite the Import section and the POST handler, keeping the tools in between if possible,
// BUT replace_file_content with Line 1-373 overwrites EVERYTHING.
// I must include all tools in the ReplacementContent to avoid deleting them.
// I will copy the tools from the previous `view_file` output]

// ... [The previous tools code is identical, so I will include them below] ...

// ============================================================================
// ğŸ“Š Action Tools (Execution Layer)
// ============================================================================

/**
 * ğŸš€ Tool: Unified AI Processor (GCP Cloud Functions)
 * ë³µì¡í•œ ë¶„ì„ ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬ (NLP + ML + Server Analysis)
 */
const callUnifiedProcessor = tool({
  description:
    'ë³µì¡í•œ ë¶„ì„ ìš”ì²­ì„ í†µí•© AI í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (GCP Cloud Functions)',
  inputSchema: z.object({
    query: z.string().describe('ì‚¬ìš©ì ì§ˆë¬¸'),
    processors: z
      .array(z.string())
      .describe(
        'ì‹¤í–‰í•  í”„ë¡œì„¸ì„œ ëª©ë¡ (korean_nlp, ml_analytics, server_analyzer)'
      ),
  }),
  execute: async ({
    query,
    processors,
  }: {
    query: string;
    processors: string[];
  }) => {
    try {
      const gcpEndpoint =
        process.env.NEXT_PUBLIC_GCP_UNIFIED_PROCESSOR_ENDPOINT || '';

      // ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ì„œë²„ ID ë“±)
      const allServers = await loadHourlyScenarioData();
      const serverIds = allServers.map((s) => s.id);

      const response = await fetch(gcpEndpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          query,
          context: {
            server_ids: serverIds,
            timestamp: new Date().toISOString(),
          },
          processors,
          options: {
            ml_model: 'anomaly_detection',
          },
        }),
        signal: AbortSignal.timeout(15000), // 15ì´ˆ íƒ€ì„ì•„ì›ƒ
      });

      if (!response.ok) {
        throw new Error(`Unified Processor API error: ${response.status}`);
      }

      const result = await response.json();

      return {
        success: true,
        data: result.data,
        timestamp: new Date().toISOString(),
        _source: 'GCP Unified AI Processor',
        _performance: result.performance,
      };
    } catch (error) {
      console.error('âŒ Unified Processor í˜¸ì¶œ ì‹¤íŒ¨:', error);
      return {
        success: false,
        error: 'í†µí•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê°œë³„ ë„êµ¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.',
        _fallback_needed: true,
      };
    }
  },
});

/**
 * ğŸ“Š Tool: ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒ (Local Simulation)
 */
const getServerMetrics = tool({
  description:
    'ì„œë²„ CPU/ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜)',
  inputSchema: z.object({
    serverId: z.string().optional().describe('ì¡°íšŒí•  ì„œë²„ ID (ì„ íƒ)'),
    metric: z
      .enum(['cpu', 'memory', 'disk', 'all'])
      .describe('ì¡°íšŒí•  ë©”íŠ¸ë¦­ íƒ€ì…'),
  }),
  execute: async ({
    serverId,
    metric: _metric,
  }: {
    serverId?: string;
    metric: 'cpu' | 'memory' | 'disk' | 'all';
  }) => {
    const allServers = await loadHourlyScenarioData();
    const target = serverId
      ? allServers.find((s) => s.id === serverId)
      : allServers;

    const servers = Array.isArray(target)
      ? target
      : target
        ? [target]
        : allServers;

    return {
      success: true,
      servers: servers.map((s) => ({
        id: s.id,
        name: s.name,
        status: s.status,
        cpu: s.cpu,
        memory: s.memory,
        disk: s.disk,
      })),
      summary: {
        total: servers.length,
        alertCount: servers.filter(
          (s) => s.status === 'warning' || s.status === 'critical'
        ).length,
      },
      timestamp: new Date().toISOString(),
      _dataSource: 'scenario-loader',
    };
  },
});

/**
 * ğŸ“š Tool: RAG ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ (Real RAG)
 */
const searchKnowledgeBase = tool({
  description: 'ê³¼ê±° ì¥ì•  ì´ë ¥ ë° í•´ê²° ë°©ë²•ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (Real RAG)',
  inputSchema: z.object({
    query: z.string().describe('ê²€ìƒ‰ ì¿¼ë¦¬'),
  }),
  execute: async ({ query }: { query: string }) => {
    try {
      const supabase = await createClient();
      const ragEngine = new SupabaseRAGEngine(supabase);

      const searchResult = await ragEngine.searchHybrid(query, {
        maxResults: 3,
        enableKeywordFallback: true,
      });

      if (!searchResult.success || searchResult.results.length === 0) {
        return {
          success: false,
          message: 'ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
        };
      }

      return {
        success: true,
        results: searchResult.results.map((r) => ({
          content: r.content,
          similarity: r.similarity,
        })),
        _source: 'Supabase pgvector',
      };
    } catch (error) {
      console.error('âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨:', error);
      return { success: false, error: 'ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜' };
    }
  },
});

/**
 * âš¡ Tool: íŒ¨í„´ ë¶„ì„ (Offline Capability)
 */
const analyzePattern = tool({
  description:
    'ì‚¬ìš©ì ì§ˆë¬¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì¦‰ê°ì ì¸ ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤ (Offline)',
  inputSchema: z.object({
    query: z.string().describe('ë¶„ì„í•  ì‚¬ìš©ì ì§ˆë¬¸'),
  }),
  execute: async ({ query }: { query: string }) => {
    const patterns: string[] = [];
    const q = query.toLowerCase();

    if (/cpu|í”„ë¡œì„¸ì„œ|ì„±ëŠ¥/i.test(q)) patterns.push('system_performance');
    if (/ë©”ëª¨ë¦¬|ram|memory/i.test(q)) patterns.push('memory_status');
    if (/ë””ìŠ¤í¬|ì €ì¥ì†Œ|ìš©ëŸ‰/i.test(q)) patterns.push('storage_info');
    if (/ì„œë²„|ì‹œìŠ¤í…œ|ìƒíƒœ/i.test(q)) patterns.push('server_status');

    if (patterns.length === 0) {
      return { success: false, message: 'ë§¤ì¹­ë˜ëŠ” íŒ¨í„´ ì—†ìŒ' };
    }

    return {
      success: true,
      patterns,
      detectedIntent: patterns[0],
      _mode: 'offline-pattern-match',
    };
  },
});

/**
 * âŒ¨ï¸ Tool: ëª…ë ¹ì–´ ì¶”ì²œ (Offline Capability)
 */
const recommendCommands = tool({
  description: 'ì‚¬ìš©ì ì§ˆë¬¸ì— ì í•©í•œ CLI ëª…ë ¹ì–´ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤ (Offline)',
  inputSchema: z.object({
    keywords: z.array(z.string()).describe('ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ í‚¤ì›Œë“œ'),
  }),
  execute: async ({ keywords }: { keywords: string[] }) => {
    const recommendations = [
      {
        keywords: ['ì„œë²„', 'ëª©ë¡', 'ì¡°íšŒ'],
        command: 'list servers',
        description: 'ì„œë²„ ëª©ë¡ ì¡°íšŒ',
      },
      {
        keywords: ['ìƒíƒœ', 'ì²´í¬', 'í™•ì¸'],
        command: 'status check',
        description: 'ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€',
      },
      {
        keywords: ['ë¡œê·¸', 'ë¶„ì„', 'ì—ëŸ¬'],
        command: 'analyze logs',
        description: 'ë¡œê·¸ ë¶„ì„',
      },
    ];

    const matched = recommendations.filter((rec) =>
      keywords.some((k) =>
        rec.keywords.some((rk) => rk.includes(k) || k.includes(rk))
      )
    );

    return {
      success: true,
      recommendations:
        matched.length > 0 ? matched : recommendations.slice(0, 2),
      _mode: 'offline-command-recommendation',
    };
  },
});

// ============================================================================
// ğŸ§  Main Handler with Dynamic Routing
// ============================================================================

export const POST = withAuth(async (req: NextRequest) => {
  try {
    const { messages }: { messages: CoreMessage[] } = await req.json();
    const apiKey = getGoogleAIKey();

    if (!apiKey) {
      return new Response('Google AI API Key not found', { status: 500 });
    }

    // 1. ìœ ì €ì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ ì¶”ì¶œ
    const lastMessage =
      messages.length > 0 ? messages[messages.length - 1] : null;
    const userQuery =
      lastMessage && typeof lastMessage.content === 'string'
        ? lastMessage.content
        : 'System status check';

    // 2. [Router] Groqë¥¼ ì‚¬ìš©í•œ ì´ˆê¸° ë¶„ë¥˜ (Fast!)
    // 0.2ì´ˆ ì´ë‚´ì— ë‹µë³€ì´ ê²°ì •ë©ë‹ˆë‹¤.
    const { complexity, intent } = await classifyQuery(userQuery);

    console.log(`ğŸ“¡ [AI Router] Intent: ${intent}, Complexity: ${complexity}`);

    // 3. ëª¨ë¸ ì„ íƒ ë¡œì§ (Dynamic Model Selection)
    // Complexity 1-3: Flash (Fast/Cheap) -> Fallback: Llama 8B
    // Complexity 4-5: Pro (Reasoning) -> Fallback: Llama 70B
    const isComplex = complexity >= 4;

    const primaryModel = isComplex
      ? google('gemini-2.5-pro') // Pro (Reasoning) - Updated to 2.5
      : google('gemini-2.5-flash'); // Flash (Speed) - Updated to 2.5

    const fallbackModel = isComplex
      ? groq('llama-3.3-70b-versatile') // High Intelligence Fallback
      : groq('llama-3.1-8b-instant'); // High Speed Fallback

    const systemPrompt = `ë‹¹ì‹ ì€ **OpenManager Vibe**ì˜ **AI ì–´ì‹œìŠ¤í„´íŠ¸**ì…ë‹ˆë‹¤.
í˜„ì¬ ëª¨ë“œ: ${isComplex ? 'ğŸ§  Deep Reasoning (Gemini 2.5 Pro)' : 'âš¡ Fast Response (Gemini 2.5 Flash)'}
ì‚¬ìš©ì ì§ˆë¬¸ ì˜ë„: ${intent} (ë³µì¡ë„: ${complexity}/5)

ëª©í‘œ: ì •í™•í•˜ê³  ë¹ ë¥¸ ë‹µë³€ì„ ì œê³µí•˜ì‹­ì‹œì˜¤.

**ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œ:**
- "ì„œë²„ ìƒíƒœ ì–´ë•Œ?" -> \`getServerMetrics\`
- "ì¥ì•  ì›ì¸ ë¶„ì„í•´ì¤˜" -> \`callUnifiedProcessor\`
- "í•´ê²° ë°©ë²• ì•Œë ¤ì¤˜" -> \`searchKnowledgeBase\`
- ë‹¨ìˆœ ìƒíƒœ í™•ì¸ -> \`analyzePattern\` (Offline)

í•­ìƒ íŒ©íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¶ˆí™•ì‹¤í•  ê²½ìš° ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  í•˜ì‹­ì‹œì˜¤.`;

    try {
      // 4. Primary Model ì‹¤í–‰
      return streamText({
        model: primaryModel,
        messages,
        system: systemPrompt,
        tools: {
          callUnifiedProcessor,
          getServerMetrics,
          searchKnowledgeBase,
          analyzePattern,
          recommendCommands,
        },
      }).toTextStreamResponse();
    } catch (error) {
      console.warn(
        `âš ï¸ Primary Model Failed. Switching to Fallback (${isComplex ? 'Llama 70B' : 'Llama 8B'}). Error:`,
        error
      );

      // 5. Fallback Model ì‹¤í–‰
      if (process.env.GROQ_API_KEY) {
        return streamText({
          model: fallbackModel,
          messages,
          system: systemPrompt + '\n(Note: Fallback model active)',
          tools: {
            callUnifiedProcessor,
            getServerMetrics,
            searchKnowledgeBase,
            analyzePattern,
            recommendCommands,
          },
        }).toTextStreamResponse();
      }

      throw error;
    }
  } catch (error) {
    console.error('âŒ AI ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    return new Response('AI streaming failed', { status: 500 });
  }
});
