/**
 * ğŸ¤– AI í†µí•© ìŠ¤íŠ¸ë¦¬ë° API (Vercel AI SDK)
 *
 * ëª©í‘œ: í¬íŠ¸í´ë¦¬ì˜¤ìš© AI ì–´ì‹œìŠ¤í„´íŠ¸ ì‹œë®¬ë ˆì´ì…˜ (MVP/PoC ìµœì í™”)
 * - ì‚¬ìš©ì ê·œëª¨: ì¼ì¼ 5ëª…, ë™ì‹œ 2ëª… ì˜ˆìƒ
 * - ì „ëµ: Hybrid Engine (Local Speed + Cloud Intelligence)
 *
 * POST /api/ai/unified-stream
 */

import { google } from '@ai-sdk/google';
import { streamText, tool } from 'ai';
import type { NextRequest } from 'next/server';
import { z } from 'zod';
import { getGoogleAIKey } from '@/lib/ai/google-ai-manager';
import { withAuth } from '@/lib/auth/api-auth';
import { createClient } from '@/lib/supabase/server';
import { SupabaseRAGEngine } from '@/services/ai/supabase-rag-engine';
import { loadHourlyScenarioData } from '@/services/scenario/scenario-loader';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

// ============================================================================
// ğŸ“Š Action Tools (Execution Layer)
// ============================================================================

/**
 * ğŸš€ Tool: Unified AI Processor (GCP Cloud Functions)
 * ë³µì¡í•œ ë¶„ì„ ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬ (NLP + ML + Server Analysis)
 */
const callUnifiedProcessor = tool({
  description:
    'ë³µì¡í•œ ë¶„ì„ ìš”ì²­ì„ í†µí•© AI í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (GCP Cloud Functions)',
  inputSchema: z.object({
    query: z.string().describe('ì‚¬ìš©ì ì§ˆë¬¸'),
    processors: z
      .array(z.string())
      .describe(
        'ì‹¤í–‰í•  í”„ë¡œì„¸ì„œ ëª©ë¡ (korean_nlp, ml_analytics, server_analyzer)'
      ),
  }),
  execute: async ({
    query,
    processors,
  }: {
    query: string;
    processors: string[];
  }) => {
    try {
      const gcpEndpoint =
        process.env.NEXT_PUBLIC_GCP_UNIFIED_PROCESSOR_ENDPOINT || '';

      // ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ì„œë²„ ID ë“±)
      const allServers = await loadHourlyScenarioData();
      const serverIds = allServers.map((s) => s.id);

      const response = await fetch(gcpEndpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          query,
          context: {
            server_ids: serverIds,
            timestamp: new Date().toISOString(),
          },
          processors,
          options: {
            ml_model: 'anomaly_detection',
          },
        }),
        signal: AbortSignal.timeout(15000), // 15ì´ˆ íƒ€ì„ì•„ì›ƒ
      });

      if (!response.ok) {
        throw new Error(`Unified Processor API error: ${response.status}`);
      }

      const result = await response.json();

      return {
        success: true,
        data: result.data,
        timestamp: new Date().toISOString(),
        _source: 'GCP Unified AI Processor',
        _performance: result.performance,
      };
    } catch (error) {
      console.error('âŒ Unified Processor í˜¸ì¶œ ì‹¤íŒ¨:', error);
      return {
        success: false,
        error: 'í†µí•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê°œë³„ ë„êµ¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.',
        _fallback_needed: true,
      };
    }
  },
});

/**
 * ğŸ“Š Tool: ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒ (Local Simulation)
 */
const getServerMetrics = tool({
  description:
    'ì„œë²„ CPU/ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜)',
  inputSchema: z.object({
    serverId: z.string().optional().describe('ì¡°íšŒí•  ì„œë²„ ID (ì„ íƒ)'),
    metric: z
      .enum(['cpu', 'memory', 'disk', 'all'])
      .describe('ì¡°íšŒí•  ë©”íŠ¸ë¦­ íƒ€ì…'),
  }),
  execute: async ({
    serverId,
    metric: _metric,
  }: {
    serverId?: string;
    metric: 'cpu' | 'memory' | 'disk' | 'all';
  }) => {
    const allServers = await loadHourlyScenarioData();
    const target = serverId
      ? allServers.find((s) => s.id === serverId)
      : allServers;

    const servers = Array.isArray(target)
      ? target
      : target
        ? [target]
        : allServers;

    return {
      success: true,
      servers: servers.map((s) => ({
        id: s.id,
        name: s.name,
        status: s.status,
        cpu: s.cpu,
        memory: s.memory,
        disk: s.disk,
      })),
      summary: {
        total: servers.length,
        alertCount: servers.filter(
          (s) => s.status === 'warning' || s.status === 'critical'
        ).length,
      },
      timestamp: new Date().toISOString(),
      _dataSource: 'scenario-loader',
    };
  },
});

/**
 * ğŸ“š Tool: RAG ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ (Real RAG)
 */
const searchKnowledgeBase = tool({
  description: 'ê³¼ê±° ì¥ì•  ì´ë ¥ ë° í•´ê²° ë°©ë²•ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (Real RAG)',
  inputSchema: z.object({
    query: z.string().describe('ê²€ìƒ‰ ì¿¼ë¦¬'),
  }),
  execute: async ({ query }: { query: string }) => {
    try {
      const supabase = await createClient();
      const ragEngine = new SupabaseRAGEngine(supabase);

      const searchResult = await ragEngine.searchHybrid(query, {
        maxResults: 3,
        enableKeywordFallback: true,
      });

      if (!searchResult.success || searchResult.results.length === 0) {
        return {
          success: false,
          message: 'ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
        };
      }

      return {
        success: true,
        results: searchResult.results.map((r) => ({
          content: r.content,
          similarity: r.similarity,
        })),
        _source: 'Supabase pgvector',
      };
    } catch (error) {
      console.error('âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨:', error);
      return { success: false, error: 'ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜' };
    }
  },
});

/**
 * âš¡ Tool: íŒ¨í„´ ë¶„ì„ (Offline Capability)
 */
const analyzePattern = tool({
  description:
    'ì‚¬ìš©ì ì§ˆë¬¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì¦‰ê°ì ì¸ ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤ (Offline)',
  inputSchema: z.object({
    query: z.string().describe('ë¶„ì„í•  ì‚¬ìš©ì ì§ˆë¬¸'),
  }),
  execute: async ({ query }: { query: string }) => {
    const patterns: string[] = [];
    const q = query.toLowerCase();

    if (/cpu|í”„ë¡œì„¸ì„œ|ì„±ëŠ¥/i.test(q)) patterns.push('system_performance');
    if (/ë©”ëª¨ë¦¬|ram|memory/i.test(q)) patterns.push('memory_status');
    if (/ë””ìŠ¤í¬|ì €ì¥ì†Œ|ìš©ëŸ‰/i.test(q)) patterns.push('storage_info');
    if (/ì„œë²„|ì‹œìŠ¤í…œ|ìƒíƒœ/i.test(q)) patterns.push('server_status');

    if (patterns.length === 0) {
      return { success: false, message: 'ë§¤ì¹­ë˜ëŠ” íŒ¨í„´ ì—†ìŒ' };
    }

    return {
      success: true,
      patterns,
      detectedIntent: patterns[0],
      _mode: 'offline-pattern-match',
    };
  },
});

/**
 * âŒ¨ï¸ Tool: ëª…ë ¹ì–´ ì¶”ì²œ (Offline Capability)
 */
const recommendCommands = tool({
  description: 'ì‚¬ìš©ì ì§ˆë¬¸ì— ì í•©í•œ CLI ëª…ë ¹ì–´ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤ (Offline)',
  inputSchema: z.object({
    keywords: z.array(z.string()).describe('ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ í‚¤ì›Œë“œ'),
  }),
  execute: async ({ keywords }: { keywords: string[] }) => {
    const recommendations = [
      {
        keywords: ['ì„œë²„', 'ëª©ë¡', 'ì¡°íšŒ'],
        command: 'list servers',
        description: 'ì„œë²„ ëª©ë¡ ì¡°íšŒ',
      },
      {
        keywords: ['ìƒíƒœ', 'ì²´í¬', 'í™•ì¸'],
        command: 'status check',
        description: 'ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€',
      },
      {
        keywords: ['ë¡œê·¸', 'ë¶„ì„', 'ì—ëŸ¬'],
        command: 'analyze logs',
        description: 'ë¡œê·¸ ë¶„ì„',
      },
    ];

    const matched = recommendations.filter((rec) =>
      keywords.some((k) =>
        rec.keywords.some((rk) => rk.includes(k) || k.includes(rk))
      )
    );

    return {
      success: true,
      recommendations:
        matched.length > 0 ? matched : recommendations.slice(0, 2),
      _mode: 'offline-command-recommendation',
    };
  },
});

// ============================================================================
// ğŸ§  Thinking Tools (Cognitive Layer) - Optimized
// ============================================================================

/**
 * ğŸ’¡ Thinking Tool: í†µí•© ìš”ì²­ ë¶„ì„ (Intent + Complexity)
 * í† í° ì ˆì•½ì„ ìœ„í•´ ë‘ ë‹¨ê³„ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
 */
const analyzeRequest = tool({
  description: 'ì§ˆë¬¸ì˜ ì˜ë„ì™€ ë³µì¡ë„ë¥¼ í•œ ë²ˆì— ë¶„ì„í•©ë‹ˆë‹¤ (Thinking Step)',
  inputSchema: z.object({
    query: z.string().describe('ì‚¬ìš©ì ì§ˆë¬¸'),
  }),
  execute: async ({ query }: { query: string }) => {
    const lowerQuery = query.toLowerCase();
    let intent = 'general';
    let complexity = 1; // 1(Simple) ~ 5(Complex)

    // ì˜ë„ ë° ë³µì¡ë„ ë¶„ì„ ë¡œì§
    if (
      lowerQuery.includes('cpu') ||
      lowerQuery.includes('ë©”ëª¨ë¦¬') ||
      lowerQuery.includes('ìƒíƒœ')
    ) {
      intent = 'monitoring';
      complexity = 2;
    } else if (
      lowerQuery.includes('ì¥ì• ') ||
      lowerQuery.includes('ì›ì¸') ||
      lowerQuery.includes('ë¶„ì„')
    ) {
      intent = 'analysis';
      complexity = 4; // ë³µì¡í•œ ë¶„ì„ í•„ìš”
    } else if (lowerQuery.includes('ì¶”ì²œ') || lowerQuery.includes('ë°©ë²•')) {
      intent = 'guide';
      complexity = 3;
    }

    // ğŸ†• ë³µì¡ë„ ì„ê³„ê°’ ì¡°ì • (GCP ë¬´ë£Œ í‹°ì–´ í™œìš© ê·¹ëŒ€í™”)
    // ê¸°ì¡´: 4-5 â†’ GCP, 3 â†’ RAG, 1-2 â†’ Offline
    // ë³€ê²½: 3-5 â†’ GCP, 2 â†’ RAG, 1 â†’ Offline
    const recommendation =
      complexity >= 3
        ? 'unified-processor' // GCP í†µí•© í”„ë¡œì„¸ì„œ ì‚¬ìš© (ë¬´ë£Œ 200ë§Œ í˜¸ì¶œ/ì›”)
        : complexity >= 2
          ? 'rag-search' // RAG ê²€ìƒ‰ ì‚¬ìš©
          : 'offline-tool'; // ì˜¤í”„ë¼ì¸ ë„êµ¬ ì‚¬ìš©

    return {
      intent,
      complexity,
      recommendation,
      reasoning: `ì˜ë„: ${intent}, ë³µì¡ë„: ${complexity} -> ì „ëµ: ${recommendation}`,
    };
  },
});

/**
 * POST í•¸ë“¤ëŸ¬
 */
export const POST = withAuth(async (req: NextRequest) => {
  try {
    const { messages } = await req.json();
    const apiKey = getGoogleAIKey();

    if (!apiKey) {
      return new Response('Google AI API Key not found', { status: 500 });
    }

    const result = streamText({
      model: google('gemini-1.5-flash'),
      messages,
      tools: {
        // ğŸ§  Thinking Tools (Optimized)
        analyzeRequest,
        // ğŸ“Š Action Tools
        callUnifiedProcessor, // New!
        getServerMetrics,
        searchKnowledgeBase,
        analyzePattern,
        recommendCommands,
      },
      system: `ë‹¹ì‹ ì€ **OpenManager Vibe**ì˜ **AI ì–´ì‹œìŠ¤í„´íŠ¸**ì…ë‹ˆë‹¤. (MVP/PoC ë²„ì „)
ëª©í‘œ: GCP ë¬´ë£Œ í‹°ì–´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ì •í™•í•˜ê³  ë¹ ë¥¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

**ğŸš¨ ì²˜ë¦¬ ì „ëµ (Hybrid Engine - GCP ìµœì í™”)**
1. **analyzeRequest**ë¥¼ ê°€ì¥ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ì „ëµì„ ìˆ˜ë¦½í•˜ì‹­ì‹œì˜¤.
2. **Simple (ë³µì¡ë„ 1)**: \`analyzePattern\` ë˜ëŠ” \`recommendCommands\` (Offline)ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
3. **Moderate (ë³µì¡ë„ 2)**: \`searchKnowledgeBase\` (RAG)ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.
4. **Complex (ë³µì¡ë„ 3-5)**: \`callUnifiedProcessor\` (GCP)ë¥¼ ì ê·¹ í™œìš©í•˜ì‹­ì‹œì˜¤. ğŸ†•

**ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œ (GCP ìš°ì„ ):**
- "ì„œë²„ ìƒíƒœ ì–´ë•Œ?" -> \`callUnifiedProcessor\` (GCP, processors: ['server_analyzer'])
- "ì¥ì•  ì›ì¸ ë¶„ì„í•´ì¤˜" -> \`callUnifiedProcessor\` (GCP, processors: ['ml_analytics', 'server_analyzer'])
- "ì¶”ì²œí•´ì¤˜", "ë°©ë²• ì•Œë ¤ì¤˜" -> \`callUnifiedProcessor\` (GCP, processors: ['korean_nlp'])
- ë‹¨ìˆœ ìƒíƒœ í™•ì¸ -> \`analyzePattern\` (Offline)
- ëª…ë ¹ì–´ ì§ˆë¬¸ -> \`recommendCommands\` (Offline)
- "í•´ê²° ë°©ë²• ì•Œë ¤ì¤˜" -> \`searchKnowledgeBase\` (RAG)

âš¡ GCP ë¬´ë£Œ í‹°ì–´: ì›” 200ë§Œ í˜¸ì¶œ (ì¼ ~32,000íšŒ) - ì ê·¹ í™œìš©í•˜ì‹­ì‹œì˜¤!
í•­ìƒ íŒ©íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¶ˆí™•ì‹¤í•  ê²½ìš° ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  í•˜ì‹­ì‹œì˜¤.`,
    });

    return result.toTextStreamResponse();
  } catch (error) {
    console.error('âŒ AI ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    return new Response('AI streaming failed', { status: 500 });
  }
});
