import { google } from '@ai-sdk/google';
import { groq } from '@ai-sdk/groq';
import { type CoreMessage, streamText, tool } from 'ai';
import type { NextRequest } from 'next/server';
import * as z from 'zod';
import { getGoogleAIKey } from '@/lib/ai/google-ai-manager';
import { withAuth } from '@/lib/auth/api-auth';
import { createClient } from '@/lib/supabase/server';
import {
  type RoutingDecision,
  recordModelFailure,
  routeQueryEnhanced,
} from '@/services/ai/smart-routing-engine';
import { SupabaseRAGEngine } from '@/services/ai/supabase-rag-engine';
import { loadHourlyScenarioData } from '@/services/scenario/scenario-loader';

// Allow streaming responses up to 60 seconds (increased for Pro model)
export const maxDuration = 60;

// [Previous Tools Definition Omitted for Brevity - They are preserved in the file context or should be re-declared if replacing whole file.
// Since replace_file_content replaces a block, I need to be careful.
// The user prompt implies I should rewrite the file or careful replace.
// I will rewrite the Import section and the POST handler, keeping the tools in between if possible,
// BUT replace_file_content with Line 1-373 overwrites EVERYTHING.
// I must include all tools in the ReplacementContent to avoid deleting them.
// I will copy the tools from the previous `view_file` output]

// ... [The previous tools code is identical, so I will include them below] ...

// ============================================================================
// ğŸ“Š Action Tools (Execution Layer)
// ============================================================================

/**
 * ğŸš€ Tool: Unified AI Processor (GCP Cloud Functions)
 * ë³µì¡í•œ ë¶„ì„ ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬ (NLP + ML + Server Analysis)
 */
const callUnifiedProcessor = tool({
  description:
    'ë³µì¡í•œ ë¶„ì„ ìš”ì²­ì„ í†µí•© AI í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (GCP Cloud Functions)',
  inputSchema: z.object({
    query: z.string().describe('ì‚¬ìš©ì ì§ˆë¬¸'),
    processors: z
      .array(z.string())
      .describe(
        'ì‹¤í–‰í•  í”„ë¡œì„¸ì„œ ëª©ë¡ (korean_nlp, ml_analytics, server_analyzer)'
      ),
  }),
  execute: async ({
    query,
    processors,
  }: {
    query: string;
    processors: string[];
  }) => {
    try {
      const gcpEndpoint =
        process.env.NEXT_PUBLIC_GCP_UNIFIED_PROCESSOR_ENDPOINT;

      // GCP ì—”ë“œí¬ì¸íŠ¸ ë¯¸ì„¤ì • ì‹œ graceful fallback
      if (!gcpEndpoint) {
        return {
          success: false,
          error: 'GCP Unified Processor ì—”ë“œí¬ì¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
          _fallback_needed: true,
        };
      }

      // ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ì„œë²„ ID ë“±)
      const allServers = await loadHourlyScenarioData();
      const serverIds = allServers.map((s) => s.id);

      const response = await fetch(gcpEndpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          query,
          context: {
            server_ids: serverIds,
            timestamp: new Date().toISOString(),
          },
          processors,
          options: {
            ml_model: 'anomaly_detection',
          },
        }),
        signal: AbortSignal.timeout(15000), // 15ì´ˆ íƒ€ì„ì•„ì›ƒ
      });

      if (!response.ok) {
        throw new Error(`Unified Processor API error: ${response.status}`);
      }

      const result = await response.json();

      return {
        success: true,
        data: result.data,
        timestamp: new Date().toISOString(),
        _source: 'GCP Unified AI Processor',
        _performance: result.performance,
      };
    } catch (error) {
      console.error('âŒ Unified Processor í˜¸ì¶œ ì‹¤íŒ¨:', error);
      return {
        success: false,
        error: 'í†µí•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê°œë³„ ë„êµ¬ë¥¼ ì‹œë„í•©ë‹ˆë‹¤.',
        _fallback_needed: true,
      };
    }
  },
});

/**
 * ğŸ“Š Tool: ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒ (Local Simulation)
 */
const getServerMetrics = tool({
  description:
    'ì„œë²„ CPU/ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜)',
  inputSchema: z.object({
    serverId: z.string().optional().describe('ì¡°íšŒí•  ì„œë²„ ID (ì„ íƒ)'),
    metric: z
      .enum(['cpu', 'memory', 'disk', 'all'])
      .describe('ì¡°íšŒí•  ë©”íŠ¸ë¦­ íƒ€ì…'),
  }),
  execute: async ({
    serverId,
    metric: _metric,
  }: {
    serverId?: string;
    metric: 'cpu' | 'memory' | 'disk' | 'all';
  }) => {
    const allServers = await loadHourlyScenarioData();
    const target = serverId
      ? allServers.find((s) => s.id === serverId)
      : allServers;

    const servers = Array.isArray(target)
      ? target
      : target
        ? [target]
        : allServers;

    return {
      success: true,
      servers: servers.map((s) => ({
        id: s.id,
        name: s.name,
        status: s.status,
        cpu: s.cpu,
        memory: s.memory,
        disk: s.disk,
      })),
      summary: {
        total: servers.length,
        alertCount: servers.filter(
          (s) => s.status === 'warning' || s.status === 'critical'
        ).length,
      },
      timestamp: new Date().toISOString(),
      _dataSource: 'scenario-loader',
    };
  },
});

/**
 * ğŸ“š Tool: RAG ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ (Real RAG)
 */
const searchKnowledgeBase = tool({
  description: 'ê³¼ê±° ì¥ì•  ì´ë ¥ ë° í•´ê²° ë°©ë²•ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (Real RAG)',
  inputSchema: z.object({
    query: z.string().describe('ê²€ìƒ‰ ì¿¼ë¦¬'),
  }),
  execute: async ({ query }: { query: string }) => {
    try {
      const supabase = await createClient();
      const ragEngine = new SupabaseRAGEngine(supabase);

      const searchResult = await ragEngine.searchHybrid(query, {
        maxResults: 3,
        enableKeywordFallback: true,
      });

      if (!searchResult.success || searchResult.results.length === 0) {
        return {
          success: false,
          message: 'ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
        };
      }

      return {
        success: true,
        results: searchResult.results.map((r) => ({
          content: r.content,
          similarity: r.similarity,
        })),
        _source: 'Supabase pgvector',
      };
    } catch (error) {
      console.error('âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨:', error);
      return { success: false, error: 'ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜' };
    }
  },
});

/**
 * âš¡ Tool: íŒ¨í„´ ë¶„ì„ (Offline Capability)
 */
const analyzePattern = tool({
  description:
    'ì‚¬ìš©ì ì§ˆë¬¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì¦‰ê°ì ì¸ ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤ (Offline)',
  inputSchema: z.object({
    query: z.string().describe('ë¶„ì„í•  ì‚¬ìš©ì ì§ˆë¬¸'),
  }),
  execute: async ({ query }: { query: string }) => {
    const patterns: string[] = [];
    const q = query.toLowerCase();

    if (/cpu|í”„ë¡œì„¸ì„œ|ì„±ëŠ¥/i.test(q)) patterns.push('system_performance');
    if (/ë©”ëª¨ë¦¬|ram|memory/i.test(q)) patterns.push('memory_status');
    if (/ë””ìŠ¤í¬|ì €ì¥ì†Œ|ìš©ëŸ‰/i.test(q)) patterns.push('storage_info');
    if (/ì„œë²„|ì‹œìŠ¤í…œ|ìƒíƒœ/i.test(q)) patterns.push('server_status');

    if (patterns.length === 0) {
      return { success: false, message: 'ë§¤ì¹­ë˜ëŠ” íŒ¨í„´ ì—†ìŒ' };
    }

    return {
      success: true,
      patterns,
      detectedIntent: patterns[0],
      _mode: 'offline-pattern-match',
    };
  },
});

/**
 * âŒ¨ï¸ Tool: ëª…ë ¹ì–´ ì¶”ì²œ (Offline Capability)
 */
const recommendCommands = tool({
  description: 'ì‚¬ìš©ì ì§ˆë¬¸ì— ì í•©í•œ CLI ëª…ë ¹ì–´ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤ (Offline)',
  inputSchema: z.object({
    keywords: z.array(z.string()).describe('ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ í‚¤ì›Œë“œ'),
  }),
  execute: async ({ keywords }: { keywords: string[] }) => {
    const recommendations = [
      {
        keywords: ['ì„œë²„', 'ëª©ë¡', 'ì¡°íšŒ'],
        command: 'list servers',
        description: 'ì„œë²„ ëª©ë¡ ì¡°íšŒ',
      },
      {
        keywords: ['ìƒíƒœ', 'ì²´í¬', 'í™•ì¸'],
        command: 'status check',
        description: 'ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€',
      },
      {
        keywords: ['ë¡œê·¸', 'ë¶„ì„', 'ì—ëŸ¬'],
        command: 'analyze logs',
        description: 'ë¡œê·¸ ë¶„ì„',
      },
    ];

    const matched = recommendations.filter((rec) =>
      keywords.some((k) =>
        rec.keywords.some((rk) => rk.includes(k) || k.includes(rk))
      )
    );

    return {
      success: true,
      recommendations:
        matched.length > 0 ? matched : recommendations.slice(0, 2),
      _mode: 'offline-command-recommendation',
    };
  },
});

// ============================================================================
// ğŸ§  Main Handler with Dynamic Routing
// ============================================================================

export const POST = withAuth(async (req: NextRequest) => {
  try {
    const { messages }: { messages: CoreMessage[] } = await req.json();
    const apiKey = getGoogleAIKey();

    if (!apiKey) {
      return new Response('Google AI API Key not found', { status: 500 });
    }

    // 1. ìœ ì €ì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ ì¶”ì¶œ
    const lastMessage =
      messages.length > 0 ? messages[messages.length - 1] : null;
    const userQuery =
      lastMessage && typeof lastMessage.content === 'string'
        ? lastMessage.content
        : 'System status check';

    // 2. [Smart Router] í–¥ìƒëœ ë¼ìš°íŒ… ê²°ì • (Circuit Breaker + Load Balancing)
    let decision: RoutingDecision;
    try {
      decision = routeQueryEnhanced(userQuery);
      console.log(
        `ğŸ“¡ [AI Router] Decision: ${decision.primaryModel} (Level: ${decision.level})`
      );
    } catch (error) {
      console.error('âŒ Router Error:', error);
      // Fallback if router fails completely
      return new Response('AI Service Unavailable: No models available', {
        status: 503,
      });
    }

    // 3. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë§¤í•‘
    // íƒ€ì… ì•ˆì „ì„±ì„ ìœ„í•´ ë¬¸ìì—´ í‚¤ë¡œ ë§¤í•‘
    const getModelInstance = (modelId: string) => {
      switch (modelId) {
        case 'gemini-2.5-pro':
          return google('gemini-2.5-pro');
        case 'gemini-2.5-flash':
          return google('gemini-2.5-flash');
        case 'llama-3.3-70b-versatile':
          return groq('llama-3.3-70b-versatile');
        case 'llama-3.1-8b-instant':
          return groq('llama-3.1-8b-instant');
        case 'qwen-qwq-32b':
          return groq('qwen-qwq-32b');
        default:
          return google('gemini-2.5-flash'); // Ultimate fallback
      }
    };

    const primaryModel = getModelInstance(decision.primaryModel);
    const fallbackModel = decision.fallbackModel
      ? getModelInstance(decision.fallbackModel)
      : null;

    const systemPrompt = `ë‹¹ì‹ ì€ **OpenManager Vibe**ì˜ **AI ì–´ì‹œìŠ¤í„´íŠ¸**ì…ë‹ˆë‹¤.
í˜„ì¬ ëª¨ë“œ: ${decision.level === 'thinking' ? 'ğŸ§  Deep Reasoning' : 'âš¡ Fast Response'} (${decision.primaryModel})
ë¼ìš°íŒ… ì´ìœ : ${decision.reasoning.join(' -> ')}

ëª©í‘œ: ì •í™•í•˜ê³  ë¹ ë¥¸ ë‹µë³€ì„ ì œê³µí•˜ì‹­ì‹œì˜¤.

**ë„êµ¬ ì‚¬ìš© ê°€ì´ë“œ:**
- "ì„œë²„ ìƒíƒœ ì–´ë•Œ?" -> \`getServerMetrics\`
- "ì¥ì•  ì›ì¸ ë¶„ì„í•´ì¤˜" -> \`callUnifiedProcessor\`
- "í•´ê²° ë°©ë²• ì•Œë ¤ì¤˜" -> \`searchKnowledgeBase\`
- ë‹¨ìˆœ ìƒíƒœ í™•ì¸ -> \`analyzePattern\` (Offline)

í•­ìƒ íŒ©íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¶ˆí™•ì‹¤í•  ê²½ìš° ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  í•˜ì‹­ì‹œì˜¤.`;

    // 4. ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰ ë° ì—ëŸ¬ í•¸ë“¤ë§ (Circuit Breaker ì—°ë™)
    try {
      const result = await streamText({
        model: primaryModel,
        messages,
        system: systemPrompt,
        tools: {
          callUnifiedProcessor,
          getServerMetrics,
          searchKnowledgeBase,
          analyzePattern,
          recommendCommands,
        },
        onFinish: (_result) => {
          // ì„±ê³µ ê¸°ë¡ (LatencyëŠ” ì •í™•í•˜ì§€ ì•Šì§€ë§Œ ëŒ€ëµì ìœ¼ë¡œ ì‚¬ìš©)
          // streamTextëŠ” onFinish ì‹œì ì— ì „ì²´ í…ìŠ¤íŠ¸ê°€ ìƒì„±ë¨
          // ëŒ€ëµì ì¸ ì„±ê³µ ë§ˆí‚¹
          // recordModelSuccess(decision.primaryModel, 1000);
          // Note: onFinish is client-side in some contexts, but here it's server.
          // We'll leave telemetry simpler for now to avoid complexity in this lambda.
        },
      });

      return result.toTextStreamResponse();
    } catch (error) {
      console.warn(
        `âš ï¸ Primary Model (${decision.primaryModel}) Failed. Error:`,
        error
      );

      // ì‹¤íŒ¨ ê¸°ë¡
      recordModelFailure(decision.primaryModel);

      // 5. Fallback ì‹¤í–‰
      if (fallbackModel) {
        console.log(`ğŸ”„ Switching to Fallback: ${decision.fallbackModel}`);
        return streamText({
          model: fallbackModel,
          messages,
          system: `${systemPrompt}\n(Note: Fallback model active)`,
          tools: {
            callUnifiedProcessor,
            getServerMetrics,
            searchKnowledgeBase,
            analyzePattern,
            recommendCommands,
          },
        }).toTextStreamResponse();
      }

      throw error;
    }
  } catch (error) {
    console.error('âŒ AI ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    return new Response('AI streaming failed', { status: 500 });
  }
});
