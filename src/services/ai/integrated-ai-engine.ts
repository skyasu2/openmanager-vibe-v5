/**
 * ğŸ¤– í†µí•© AI ì—”ì§„ v3.0
 * 
 * âœ… ê¸°ë³¸: MCP + TensorFlow.js + NLP ë¡œì»¬ ì¶”ë¡ 
 * âœ… LLM ì—†ì´ë„ ë™ì‘í•˜ëŠ” ìì—°ì–´ ì§ˆì˜ì‘ë‹µ
 * âœ… ì‹¤ì‹œê°„ ì¥ì•  ì˜ˆì¸¡
 * âœ… ìë™ ë³´ê³ ì„œ ìƒì„±
 * âœ… Vercel Edge Runtime ìµœì í™”
 * âœ… ë² íƒ€: ì™¸ë¶€ LLM ì—°ë™ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥
 */

import { realMCPClient } from '../mcp/real-mcp-client';
import { tensorFlowAIEngine } from './tensorflow-engine';
import { nlpProcessor } from './nlp-processor';
import { autoReportGenerator } from './report-generator';

interface AIQueryRequest {
  query: string;
  context?: {
    session_id?: string;
    user_id?: string;
    server_ids?: string[];
    include_predictions?: boolean;
    include_charts?: boolean;
    language?: 'ko' | 'en';
  };
  options?: {
    max_response_time?: number;
    confidence_threshold?: number;
    enable_streaming?: boolean;
    include_debug?: boolean;
  };
}

interface AIQueryResponse {
  success: boolean;
  query_id: string;
  intent: string;
  confidence: number;
  answer: string;
  analysis_results: {
    nlp_analysis: any;
    ai_predictions?: any;
    anomaly_detection?: any;
    trend_forecasts?: any;
    active_alerts?: any[];
    session_context?: any;
  };
  recommendations: string[];
  generated_report?: string;
  mcp_results?: any;
  processing_stats: {
    total_time: number;
    components_used: string[];
    models_executed: string[];
    data_sources: string[];
  };
  metadata: {
    timestamp: string;
    language: string;
    session_id?: string;
    debug_info?: any;
  };
}

interface SystemMetrics {
  servers: Record<string, Record<string, number[]>>;
  global_stats: any;
  alerts: any[];
  timestamp: string;
}

export class IntegratedAIEngine {
  private initialized = false;
  private lastAnalysisCache: Map<string, any> = new Map();
  private activeSessions: Set<string> = new Set();

  constructor() {
    // ì»´í¬ë„ŒíŠ¸ë“¤ì€ ì´ë¯¸ ì´ˆê¸°í™”ë¨
  }

  async initialize(): Promise<void> {
    if (this.initialized) return;

    console.log('ğŸ¤– í†µí•© AI ì—”ì§„ v3.0 ì´ˆê¸°í™” ì¤‘...');
    
    try {
      // ëª¨ë“  í•˜ìœ„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
      await Promise.all([
        realMCPClient.initialize(),
        tensorFlowAIEngine.initialize(),
        nlpProcessor.initialize(),
        autoReportGenerator.initialize()
      ]);
      
      this.initialized = true;
      console.log('âœ… í†µí•© AI ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ');
      console.log('ğŸ”§ í™œì„±í™”ëœ ì»´í¬ë„ŒíŠ¸:');
      console.log('  - âœ… ì‹¤ì œ MCP í´ë¼ì´ì–¸íŠ¸');
      console.log('  - âœ… TensorFlow.js AI ì—”ì§„');
      console.log('  - âœ… NLP í”„ë¡œì„¸ì„œ');
      console.log('  - âœ… ìë™ ë³´ê³ ì„œ ìƒì„±ê¸°');
      
    } catch (error: any) {
      console.error('âŒ í†µí•© AI ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      this.initialized = true; // í´ë°± ëª¨ë“œë¡œ ê³„ì† ì§„í–‰
    }
  }

  async processQuery(request: AIQueryRequest): Promise<AIQueryResponse> {
    await this.initialize();
    
    const startTime = Date.now();
    const queryId = this.generateQueryId();
    const sessionId = request.context?.session_id || queryId;
    
    console.log(`ğŸ¤– AI ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: ${queryId}`);
    console.log(`ğŸ“ ì§ˆë¬¸: "${request.query}"`);
    
    this.activeSessions.add(sessionId);

    const response: AIQueryResponse = {
      success: false,
      query_id: queryId,
      intent: 'unknown',
      confidence: 0,
      answer: '',
      analysis_results: {
        nlp_analysis: null
      },
      recommendations: [],
      processing_stats: {
        total_time: 0,
        components_used: [],
        models_executed: [],
        data_sources: []
      },
      metadata: {
        timestamp: new Date().toISOString(),
        language: request.context?.language || 'ko',
        session_id: sessionId
      }
    };

    try {
      // 1ë‹¨ê³„: ìì—°ì–´ ì²˜ë¦¬
      console.log('ğŸ“ 1ë‹¨ê³„: NLP ë¶„ì„ ì¤‘...');
      const nlpResult = await nlpProcessor.processFailurePredictionQuery(request.query);
      response.analysis_results.nlp_analysis = nlpResult;
      response.intent = nlpResult.intent;
      response.confidence = nlpResult.confidence;
      response.processing_stats.components_used.push('nlp_processor');

      console.log(`ğŸ¯ ì˜ë„ ë¶„ì„ ê²°ê³¼: ${nlpResult.intent} (ì‹ ë¢°ë„: ${(nlpResult.confidence * 100).toFixed(1)}%)`);

      // 2ë‹¨ê³„: ì˜ë„ë³„ ì²˜ë¦¬ ë¶„ê¸°
      await this.processIntentSpecificActions(nlpResult, request, response);

      // 3ë‹¨ê³„: ì¢…í•© ë‹µë³€ ìƒì„±
      await this.generateComprehensiveAnswer(nlpResult, request, response);

      // 4ë‹¨ê³„: ê¶Œì¥ì‚¬í•­ ìƒì„±
      this.generateRecommendations(nlpResult, response);

      // 5ë‹¨ê³„: ë³´ê³ ì„œ ìƒì„± (ìš”ì²­ëœ ê²½ìš°)
      if (this.shouldGenerateReport(nlpResult, request)) {
        await this.generateReport(response, request);
      }

      response.success = true;
      response.processing_stats.total_time = Date.now() - startTime;
      
      console.log(`âœ… AI ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: ${response.processing_stats.total_time}ms`);
      console.log(`ğŸ¯ ìµœì¢… ì‘ë‹µ: "${response.answer.substring(0, 100)}..."`);

    } catch (error: any) {
      console.error('âŒ AI ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨:', error);
      response.success = false;
      response.answer = response.metadata.language === 'ko' ? 
        'ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.' :
        'Sorry, an error occurred during processing. Please try again.';
      response.processing_stats.total_time = Date.now() - startTime;
      
      if (request.options?.include_debug) {
        response.metadata.debug_info = {
          error: error.message,
          stack: error.stack
        };
      }
    } finally {
      this.activeSessions.delete(sessionId);
    }

    return response;
  }

  private async processIntentSpecificActions(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    const intent = nlpResult.intent;
    
    switch (intent) {
      case 'troubleshooting':
      case 'emergency':
        await this.handleTroubleshootingIntent(nlpResult, request, response);
        break;
        
      case 'prediction':
        await this.handlePredictionIntent(nlpResult, request, response);
        break;
        
      case 'analysis':
        await this.handleAnalysisIntent(nlpResult, request, response);
        break;
        
      case 'monitoring':
        await this.handleMonitoringIntent(nlpResult, request, response);
        break;
        
      case 'reporting':
        await this.handleReportingIntent(nlpResult, request, response);
        break;
        
      case 'performance':
        await this.handlePerformanceIntent(nlpResult, request, response);
        break;
        
      default:
        await this.handleGeneralIntent(nlpResult, request, response);
    }
  }

  private async handleTroubleshootingIntent(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    // ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    const systemMetrics = await this.collectSystemMetrics(request.context?.server_ids);
    
    // MCPë¥¼ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰
    const mcpResults = await realMCPClient.searchDocuments(request.query);
    response.mcp_results = mcpResults;
    response.processing_stats.data_sources.push('mcp-docs');

    // AI ì¥ì•  ë¶„ì„ ì‹¤í–‰
    if (systemMetrics.servers && Object.keys(systemMetrics.servers).length > 0) {
      // ğŸ”§ íƒ€ì… ì—ëŸ¬ ìˆ˜ì •: ì¤‘ì²© êµ¬ì¡°ë¥¼ í”Œë« êµ¬ì¡°ë¡œ ë³€í™˜
      const flattenedMetrics: Record<string, number[]> = {};
      
      for (const [serverId, serverMetrics] of Object.entries(systemMetrics.servers)) {
        for (const [metricName, values] of Object.entries(serverMetrics)) {
          const key = `${serverId}_${metricName}`;
          flattenedMetrics[key] = values;
        }
      }
      
      const aiAnalysis = await tensorFlowAIEngine.analyzeMetricsWithAI(flattenedMetrics);
      response.analysis_results.ai_predictions = aiAnalysis.failure_predictions;
      response.analysis_results.anomaly_detection = aiAnalysis.anomaly_detections;
      response.processing_stats.models_executed.push(...aiAnalysis.processing_stats.models_used);
    }

    response.processing_stats.components_used.push('mcp-client', 'tensorflow-engine');
  }

  private async handlePredictionIntent(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    // ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    const systemMetrics = await this.collectSystemMetrics(request.context?.server_ids);
    
    if (systemMetrics.servers && Object.keys(systemMetrics.servers).length > 0) {
      // ğŸ”§ íƒ€ì… ì—ëŸ¬ ìˆ˜ì •: ì¤‘ì²© êµ¬ì¡°ë¥¼ í”Œë« êµ¬ì¡°ë¡œ ë³€í™˜
      const flattenedMetrics: Record<string, number[]> = {};
      
      for (const [serverId, serverMetrics] of Object.entries(systemMetrics.servers)) {
        for (const [metricName, values] of Object.entries(serverMetrics)) {
          const key = `${serverId}_${metricName}`;
          flattenedMetrics[key] = values;
        }
      }
      
      const aiAnalysis = await tensorFlowAIEngine.analyzeMetricsWithAI(flattenedMetrics);
      response.analysis_results.ai_predictions = aiAnalysis.failure_predictions;
      response.analysis_results.trend_forecasts = aiAnalysis.trend_predictions;
      response.processing_stats.models_executed.push(...aiAnalysis.processing_stats.models_used);
      
      // ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ë†’ì€ ê²°ê³¼ì— ëŒ€í•œ ì•Œë¦¼
      for (const [metric, prediction] of Object.entries(aiAnalysis.failure_predictions)) {
        if (prediction.confidence > 0.8 && prediction.prediction[0] > 0.6) {
          systemMetrics.alerts.push({
            type: 'prediction',
            severity: 'high',
            metric: metric,
            message: `ë†’ì€ í™•ë¥ ì˜ ì¥ì•  ì˜ˆì¸¡: ${(prediction.prediction[0] * 100).toFixed(1)}%`,
            confidence: prediction.confidence
          });
        }
      }
    }

    response.analysis_results.active_alerts = systemMetrics.alerts;
    response.processing_stats.components_used.push('tensorflow-engine', 'prediction-model');
  }

  private async handleAnalysisIntent(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    const systemMetrics = await this.collectSystemMetrics(request.context?.server_ids);
    
    if (systemMetrics.servers && Object.keys(systemMetrics.servers).length > 0) {
      // ğŸ”§ íƒ€ì… ì—ëŸ¬ ìˆ˜ì •: ì¤‘ì²© êµ¬ì¡°ë¥¼ í”Œë« êµ¬ì¡°ë¡œ ë³€í™˜
      const flattenedMetrics: Record<string, number[]> = {};
      
      for (const [serverId, serverMetrics] of Object.entries(systemMetrics.servers)) {
        for (const [metricName, values] of Object.entries(serverMetrics)) {
          const key = `${serverId}_${metricName}`;
          flattenedMetrics[key] = values;
        }
      }
      
      const aiAnalysis = await tensorFlowAIEngine.analyzeMetricsWithAI(flattenedMetrics);
      response.analysis_results.ai_predictions = aiAnalysis.failure_predictions;
      response.analysis_results.anomaly_detection = aiAnalysis.anomaly_detections;
      response.processing_stats.models_executed.push(...aiAnalysis.processing_stats.models_used);
    }

    // MCPë¥¼ í†µí•œ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    if (request.context?.session_id) {
      const sessionContext = await realMCPClient.retrieveContext(request.context.session_id);
      response.analysis_results.session_context = sessionContext;
    }

    response.processing_stats.components_used.push('tensorflow-engine', 'mcp-client');
  }

  private async handleMonitoringIntent(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    console.log('ğŸ“Š ëª¨ë‹ˆí„°ë§ ëª¨ë“œ ì‹¤í–‰');
    
    try {
      const systemMetrics = await this.collectSystemMetrics(request.context?.server_ids);
      response.processing_stats.data_sources.push('system_metrics', 'real_time_monitoring');

      // ì‹¤ì‹œê°„ ìƒíƒœ ë¶„ì„
      if (systemMetrics.alerts && systemMetrics.alerts.length > 0) {
        response.analysis_results.active_alerts = systemMetrics.alerts;
      }

    } catch (error: any) {
      console.error('ëª¨ë‹ˆí„°ë§ ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    }
  }

  private async handleReportingIntent(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    console.log('ğŸ“„ ë³´ê³ ì„œ ìƒì„± ëª¨ë“œ ì‹¤í–‰');
    
    try {
      // ë³´ê³ ì„œìš© ë°ì´í„° ìˆ˜ì§‘
      const systemMetrics = await this.collectSystemMetrics(request.context?.server_ids);
      
      const reportData = {
        timestamp: new Date().toISOString(),
        summary: response.metadata.language === 'ko' ? 
          'ì‹œìŠ¤í…œ ìƒíƒœ ë³´ê³ ì„œì…ë‹ˆë‹¤.' : 
          'System status report.',
        failure_analysis: response.analysis_results.anomaly_detection || {},
        prediction_results: response.analysis_results.ai_predictions || {},
        ai_insights: ['AI ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.'],
        recommendations: [],
        metrics_data: systemMetrics.servers || {},
        charts: [],
        system_status: { overall: 'healthy' },
        time_range: {
          start: new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString(),
          end: new Date().toISOString(),
          duration: '24h'
        }
      };

      const reportConfig = {
        format: 'markdown' as const,
        include_charts: true,
        include_raw_data: false,
        template: 'technical' as const,
        language: response.metadata.language as 'ko' | 'en'
      };

      const generatedReport = await autoReportGenerator.generateFailureReport(reportData, reportConfig);
      response.generated_report = generatedReport;
      response.processing_stats.components_used.push('report_generator');

    } catch (error: any) {
      console.error('ë³´ê³ ì„œ ìƒì„± ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    }
  }

  private async handlePerformanceIntent(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    const systemMetrics = await this.collectSystemMetrics(request.context?.server_ids);
    
    if (systemMetrics.servers && Object.keys(systemMetrics.servers).length > 0) {
      // ğŸ”§ íƒ€ì… ì—ëŸ¬ ìˆ˜ì •: ì¤‘ì²© êµ¬ì¡°ë¥¼ í”Œë« êµ¬ì¡°ë¡œ ë³€í™˜
      const flattenedMetrics: Record<string, number[]> = {};
      
      for (const [serverId, serverMetrics] of Object.entries(systemMetrics.servers)) {
        for (const [metricName, values] of Object.entries(serverMetrics)) {
          const key = `${serverId}_${metricName}`;
          flattenedMetrics[key] = values;
        }
      }
      
      const aiAnalysis = await tensorFlowAIEngine.analyzeMetricsWithAI(flattenedMetrics);
      response.analysis_results.ai_predictions = aiAnalysis.failure_predictions;
      response.analysis_results.anomaly_detection = aiAnalysis.anomaly_detections;
      response.processing_stats.models_executed.push(...aiAnalysis.processing_stats.models_used);
    }

    response.processing_stats.components_used.push('tensorflow-engine');
  }

  private async handleGeneralIntent(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    console.log('ğŸ¤– ì¼ë°˜ ì§ˆì˜ ëª¨ë“œ ì‹¤í–‰');
    
    try {
      // MCPë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
      const searchResults = await realMCPClient.searchDocuments(request.query);
      response.mcp_results = searchResults;
      response.processing_stats.components_used.push('mcp_client');

    } catch (error: any) {
      console.error('ì¼ë°˜ ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    }
  }

  private async generateComprehensiveAnswer(
    nlpResult: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<void> {
    const lang = response.metadata.language;
    let answer = '';

    // ì˜ë„ë³„ ê¸°ë³¸ ë‹µë³€ ìƒì„±
    switch (nlpResult.intent) {
      case 'troubleshooting':
      case 'emergency':
        answer = this.generateTroubleshootingAnswer(response, lang);
        break;
      case 'prediction':
        answer = this.generatePredictionAnswer(response, lang);
        break;
      case 'analysis':
        answer = this.generateAnalysisAnswer(response, lang);
        break;
      case 'monitoring':
        answer = this.generateMonitoringAnswer(response, lang);
        break;
      case 'reporting':
        answer = this.generateReportingAnswer(response, lang);
        break;
      case 'performance':
        answer = this.generatePerformanceAnswer(response, lang);
        break;
      default:
        answer = this.generateGeneralAnswer(response, lang);
    }

    response.answer = answer;
  }

  private generateTroubleshootingAnswer(response: AIQueryResponse, lang: string): string {
    const anomalies = response.analysis_results.anomaly_detection;
    const predictions = response.analysis_results.ai_predictions;
    
    let answer = lang === 'ko' ? 'ğŸš¨ ì‹œìŠ¤í…œ ì¥ì•  ë¶„ì„ ê²°ê³¼:\n\n' : 'ğŸš¨ System Troubleshooting Analysis:\n\n';

    if (anomalies && Object.keys(anomalies).length > 0) {
      answer += lang === 'ko' ? '**ì´ìƒ íƒì§€ ê²°ê³¼:**\n' : '**Anomaly Detection:**\n';
      Object.entries(anomalies).forEach(([metric, anomaly]: [string, any]) => {
        const status = anomaly.is_anomaly ? 
          (lang === 'ko' ? 'ì´ìƒ ê°ì§€' : 'Anomaly Detected') :
          (lang === 'ko' ? 'ì •ìƒ' : 'Normal');
        answer += `- ${metric}: ${status}\n`;
      });
      answer += '\n';
    }

    if (predictions && Object.keys(predictions).length > 0) {
      answer += lang === 'ko' ? '**ì¥ì•  ì˜ˆì¸¡:**\n' : '**Failure Prediction:**\n';
      Object.entries(predictions).forEach(([metric, prediction]: [string, any]) => {
        const probability = (prediction.prediction[0] * 100).toFixed(1);
        answer += `- ${metric}: ${probability}%\n`;
      });
      answer += '\n';
    }

    if (!anomalies && !predictions) {
      answer += lang === 'ko' ? 
        'í˜„ì¬ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.' :
        'Insufficient metric data available for analysis.';
    }

    return answer;
  }

  private generatePredictionAnswer(response: AIQueryResponse, lang: string): string {
    const forecasts = response.analysis_results.trend_forecasts;
    
    let answer = lang === 'ko' ? 'ğŸ”® ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼:\n\n' : 'ğŸ”® Time Series Forecast:\n\n';

    if (forecasts && Object.keys(forecasts).length > 0) {
      Object.entries(forecasts).forEach(([server, metrics]: [string, any]) => {
        answer += `**${server}:**\n`;
        Object.entries(metrics).forEach(([metric, values]: [string, any]) => {
          answer += `- ${metric}: [${Array.isArray(values) ? values.map(v => v.toFixed(2)).join(', ') : 'N/A'}]\n`;
        });
        answer += '\n';
      });
    } else {
      answer += lang === 'ko' ? 
        'ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ ê³¼ê±° ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.' :
        'Insufficient historical data for prediction.';
    }

    return answer;
  }

  private generateAnalysisAnswer(response: AIQueryResponse, lang: string): string {
    return lang === 'ko' ? 
      'ğŸ” AI ê¸°ë°˜ ì‹œìŠ¤í…œ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ìƒì„¸í•œ ê²°ê³¼ëŠ” ë¶„ì„ ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.' :
      'ğŸ” AI-powered system analysis completed. Please refer to analysis data for detailed results.';
  }

  private generateMonitoringAnswer(response: AIQueryResponse, lang: string): string {
    return lang === 'ko' ? 
      'ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.' :
      'ğŸ“Š Real-time monitoring data analyzed.';
  }

  private generateReportingAnswer(response: AIQueryResponse, lang: string): string {
    if (response.generated_report) {
      return lang === 'ko' ? 
        'ğŸ“„ ìë™ ë³´ê³ ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ì—ì„œ í™•ì¸í•˜ì„¸ìš”.' :
        'ğŸ“„ Automated report generated. Please see below.';
    }
    return lang === 'ko' ? 
      'ğŸ“„ ë³´ê³ ì„œ ìƒì„± ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.' :
      'ğŸ“„ Report generation in progress.';
  }

  private generatePerformanceAnswer(response: AIQueryResponse, lang: string): string {
    return lang === 'ko' ? 
      'âš¡ ì„±ëŠ¥ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. AI ëª¨ë¸ì„ í†µí•´ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤.' :
      'âš¡ Performance analysis completed. System performance evaluated through AI models.';
  }

  private generateGeneralAnswer(response: AIQueryResponse, lang: string): string {
    if (response.mcp_results?.success && response.mcp_results.results.length > 0) {
      return lang === 'ko' ? 
        'ğŸ“š ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì¶”ê°€ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.' :
        'ğŸ“š Relevant documentation found. Please check additional information.';
    }
    return lang === 'ko' ? 
      'ğŸ¤– ì§ˆë¬¸ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.' :
      'ğŸ¤– I understand your question. Please provide more specific information for a more accurate response.';
  }

  private generateRecommendations(nlpResult: any, response: AIQueryResponse): void {
    const lang = response.metadata.language;
    const intent = nlpResult.intent;
    
    const recommendations = [];

    switch (intent) {
      case 'troubleshooting':
      case 'emergency':
        if (lang === 'ko') {
          recommendations.push('ì¦‰ì‹œ ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”');
          recommendations.push('ë¶€í•˜ ë¶„ì‚° ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”');
          recommendations.push('ë°±ì—… ì‹œìŠ¤í…œ ì¤€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”');
        } else {
          recommendations.push('Check system logs immediately');
          recommendations.push('Verify load balancing status');
          recommendations.push('Confirm backup system readiness');
        }
        break;
        
      case 'prediction':
        if (lang === 'ko') {
          recommendations.push('ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ì „ ì¡°ì¹˜ë¥¼ ê³„íší•˜ì„¸ìš”');
          recommendations.push('ë¦¬ì†ŒìŠ¤ í™•ì¥ ê³„íšì„ ê²€í† í•˜ì„¸ìš”');
        } else {
          recommendations.push('Plan preventive actions based on predictions');
          recommendations.push('Review resource scaling plans');
        }
        break;
        
      case 'performance':
        if (lang === 'ko') {
          recommendations.push('ì„±ëŠ¥ ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ì—¬ ìµœì í™”í•˜ì„¸ìš”');
          recommendations.push('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”');
        } else {
          recommendations.push('Identify and optimize performance bottlenecks');
          recommendations.push('Monitor memory usage closely');
        }
        break;
        
      default:
        if (lang === 'ko') {
          recommendations.push('ì •ê¸°ì ì¸ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ì„ ìœ ì§€í•˜ì„¸ìš”');
          recommendations.push('ë°±ì—… ë° ë³µêµ¬ ê³„íšì„ ì ê²€í•˜ì„¸ìš”');
        } else {
          recommendations.push('Maintain regular system monitoring');
          recommendations.push('Review backup and recovery plans');
        }
    }

    response.recommendations = recommendations;
  }

  private shouldGenerateReport(nlpResult: any, request: AIQueryRequest): boolean {
    return nlpResult.intent === 'reporting' || 
           nlpResult.specialized_analysis?.urgency_level === 'critical' ||
           request.context?.include_charts === true;
  }

  private async generateReport(response: AIQueryResponse, request: AIQueryRequest): Promise<void> {
    try {
      if (!response.generated_report) {
        const reportData = {
          timestamp: response.metadata.timestamp,
          summary: response.answer,
          failure_analysis: response.analysis_results.anomaly_detection || {},
          prediction_results: response.analysis_results.ai_predictions || {},
          ai_insights: [response.answer],
          recommendations: response.recommendations,
          metrics_data: {},
          charts: [],
          system_status: { overall: 'analyzed' },
          time_range: {
            start: new Date(Date.now() - 60 * 60 * 1000).toISOString(),
            end: new Date().toISOString(),
            duration: '1h'
          }
        };

        const reportConfig = {
          format: 'markdown' as const,
          include_charts: request.context?.include_charts || false,
          include_raw_data: false,
          template: 'technical' as const,
          language: response.metadata.language as 'ko' | 'en'
        };

        response.generated_report = await autoReportGenerator.generateFailureReport(reportData, reportConfig);
        response.processing_stats.components_used.push('report_generator');
      }
    } catch (error: any) {
      console.error('ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨:', error);
    }
  }

  private async collectSystemMetrics(serverIds?: string[]): Promise<SystemMetrics> {
    // ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Prometheusë‚˜ ë©”íŠ¸ë¦­ ì„œë¹„ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
    // í˜„ì¬ëŠ” ë°ëª¨ìš© ë°ì´í„° ë°˜í™˜
    return {
      servers: {
        'server-01': {
          cpu: [45, 52, 48, 67, 73, 69, 71, 65, 58, 61],
          memory: [78, 82, 79, 85, 88, 84, 87, 83, 80, 85],
          disk: [45, 46, 47, 48, 49, 50, 51, 48, 47, 49],
          network: [23, 28, 31, 35, 29, 26, 33, 37, 24, 30]
        },
        'server-02': {
          cpu: [32, 38, 41, 45, 42, 39, 44, 47, 43, 40],
          memory: [65, 68, 71, 74, 69, 72, 75, 70, 67, 73],
          disk: [55, 57, 59, 58, 60, 62, 61, 59, 58, 60],
          network: [18, 22, 25, 28, 24, 21, 26, 29, 23, 27]
        }
      },
      global_stats: {
        total_servers: 2,
        healthy_servers: 2,
        average_cpu: 55.5,
        average_memory: 76.5
      },
      alerts: [],
      timestamp: new Date().toISOString()
    };
  }

  private generateQueryId(): string {
    return `query_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
  async *processQueryStream(request: AIQueryRequest): AsyncGenerator<Partial<AIQueryResponse>, void, unknown> {
    const startTime = Date.now();
    const queryId = this.generateQueryId();

    yield {
      query_id: queryId,
      metadata: { 
        timestamp: new Date().toISOString(),
        language: request.context?.language || 'ko'
      }
    };

    try {
      // NLP ë¶„ì„
      const nlpResult = await nlpProcessor.processQuery(request.query);
      yield {
        intent: nlpResult.intent,
        confidence: nlpResult.confidence,
        analysis_results: { nlp_analysis: nlpResult }
      };

      // ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
      const systemMetrics = await this.collectSystemMetrics(request.context?.server_ids);
      if (systemMetrics.servers && Object.keys(systemMetrics.servers).length > 0) {
        // ğŸ”§ íƒ€ì… ì—ëŸ¬ ìˆ˜ì •: ì¤‘ì²© êµ¬ì¡°ë¥¼ í”Œë« êµ¬ì¡°ë¡œ ë³€í™˜
        const flattenedMetrics: Record<string, number[]> = {};
        
        for (const [serverId, serverMetrics] of Object.entries(systemMetrics.servers)) {
          for (const [metricName, values] of Object.entries(serverMetrics)) {
            const key = `${serverId}_${metricName}`;
            flattenedMetrics[key] = values;
          }
        }
        
        const aiAnalysis = await tensorFlowAIEngine.analyzeMetricsWithAI(flattenedMetrics);
        yield {
          analysis_results: {
            nlp_analysis: nlpResult,
            ai_predictions: aiAnalysis.failure_predictions,
            anomaly_detection: aiAnalysis.anomaly_detections,
            trend_forecasts: aiAnalysis.trend_predictions
          }
        };
      }

      // ìµœì¢… ì‘ë‹µ
      yield {
        success: true,
        processing_stats: {
          total_time: Date.now() - startTime,
          components_used: ['nlp-processor', 'tensorflow-engine'],
          models_executed: ['neural-network', 'autoencoder'],
          data_sources: ['system-metrics']
        }
      };

    } catch (error: any) {
      yield {
        success: false,
        answer: `ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error.message}`,
        metadata: { 
          timestamp: new Date().toISOString(),
          language: request.context?.language || 'ko'
        }
      };
    }
  }

  async getEngineStatus(): Promise<any> {
    const componentStatuses = await Promise.all([
      realMCPClient.getConnectionInfo(),
      tensorFlowAIEngine.getModelInfo(),
      nlpProcessor.getProcessorInfo(),
      autoReportGenerator.getGeneratorInfo()
    ]);

    return {
      engine_version: '3.0.0',
      initialized: this.initialized,
      active_sessions: this.activeSessions.size,
      components: {
        mcp_client: componentStatuses[0],
        tensorflow_engine: componentStatuses[1],
        nlp_processor: componentStatuses[2],
        report_generator: componentStatuses[3]
      },
      capabilities: [
        'ìì—°ì–´ ì§ˆì˜ì‘ë‹µ',
        'ì‹¤ì‹œê°„ ì¥ì•  ì˜ˆì¸¡',
        'ì´ìƒ íƒì§€',
        'ì‹œê³„ì—´ ì˜ˆì¸¡',
        'ìë™ ë³´ê³ ì„œ ìƒì„±',
        'MCP í”„ë¡œí† ì½œ ì§€ì›',
        'ë‹¤êµ­ì–´ ì§€ì› (í•œêµ­ì–´/ì˜ì–´)',
        'Vercel Edge Runtime í˜¸í™˜'
      ],
      supported_intents: [
        'troubleshooting', 'prediction', 'analysis', 
        'monitoring', 'reporting', 'performance', 'emergency'
      ],
      last_cache_update: new Date().toISOString()
    };
  }

  dispose(): void {
    console.log('ğŸ—‘ï¸ í†µí•© AI ì—”ì§„ ì •ë¦¬ ì¤‘...');
    
    this.lastAnalysisCache.clear();
    this.activeSessions.clear();
    
    // TensorFlow.js ë©”ëª¨ë¦¬ ì •ë¦¬
    tensorFlowAIEngine.dispose();
    
    this.initialized = false;
    console.log('âœ… í†µí•© AI ì—”ì§„ ì •ë¦¬ ì™„ë£Œ');
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const integratedAIEngine = new IntegratedAIEngine(); 