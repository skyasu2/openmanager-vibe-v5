/**
 * SmartRoutingEngine - ë‹¤ì°¨ì› AI ë¼ìš°íŒ… ì—”ì§„
 *
 * ë³µì¡ë„, í† í° ê°€ìš©ì„±, ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­ ë“± ë‹¤ì–‘í•œ ì¡°ê±´ì„ ê³ ë ¤í•˜ì—¬
 * ìµœì ì˜ AI ëª¨ë¸ê³¼ ê¸°ëŠ¥ ì¡°í•©ì„ ê²°ì •í•©ë‹ˆë‹¤.
 *
 * @author OpenManager VIBE
 * @since v5.80.0
 */

import {
  checkGoogleAIRateLimit,
  getGoogleAIKey,
} from '../../lib/ai/google-ai-manager';
import {
  checkGroqAIRateLimit,
  type GroqModel,
  getGroqAIRateLimitStatus,
  isGroqAIAvailable,
} from '../../lib/ai/groq-ai-manager';
import {
  type AIModel,
  type AIProviderStatus,
  type CircuitState,
  COMPLEXITY_TOKEN_MAP,
  DEFAULT_CIRCUIT_BREAKER_CONFIG,
  DEFAULT_SCORE_WEIGHTS,
  type EnhancedRoutingDecision,
  type FeatureRequirements,
  INTENT_FEATURE_MAP,
  type LoadBalanceStrategy,
  MODEL_CAPABILITIES,
  MODEL_SPECIALIZATION,
  type ModelHealth,
  type ModelLoadState,
  type QueryAnalysis,
  type RateLimitStatus,
  type RouteScore,
  type RoutingContext,
  type RoutingDecision,
  type RoutingPreferences,
  type ScoreWeights,
  type TaskType,
} from '../../types/ai/routing-types';
import type { QueryIntent } from '../../types/rag/rag-types';

// ============================================================================
// í† í° ì¶”ì • ìœ í‹¸ë¦¬í‹°
// ============================================================================

/**
 * ë¬¸ìì—´ì˜ í† í° ìˆ˜ ì¶”ì • (í•œêµ­ì–´ ê³ ë ¤)
 * í•œêµ­ì–´ëŠ” ì˜ì–´ë³´ë‹¤ í† í°ë‹¹ ë¬¸ì ìˆ˜ê°€ ì ìŒ
 */
function estimateTokenCount(text: string): number {
  const koreanChars = (text.match(/[\uAC00-\uD7AF]/g) || []).length;
  const otherChars = text.length - koreanChars;
  // í•œêµ­ì–´: ~1.5 tokens per char, ì˜ì–´: ~0.25 tokens per char
  return Math.ceil(koreanChars * 1.5 + otherChars * 0.25);
}

/**
 * ë³µì¡ë„ ê¸°ë°˜ ì˜ˆìƒ ì‘ë‹µ í† í° ê³„ì‚°
 */
function estimateResponseTokens(complexity: number): number {
  const baseTokens = [256, 512, 1024, 2048, 4096];
  return baseTokens[Math.min(complexity - 1, 4)] ?? 1024;
}

// ============================================================================
// SmartRoutingEngine
// ============================================================================

export class SmartRoutingEngine {
  private static instance: SmartRoutingEngine;

  // ============================================================================
  // Circuit Breaker State (Best Practice #3)
  // ============================================================================
  private modelHealth = new Map<AIModel, ModelHealth>();
  private modelLoads = new Map<AIModel, ModelLoadState>();
  private lastSelectedModel: AIModel | null = null;
  private loadBalanceStrategy: LoadBalanceStrategy = 'adaptive';
  private scoreWeights: ScoreWeights = DEFAULT_SCORE_WEIGHTS;

  // ì§€ì› ëª¨ë¸ ëª©ë¡
  private readonly ALL_MODELS: AIModel[] = [
    'llama-3.1-8b-instant',
    'llama-3.3-70b-versatile',
    'qwen-qwq-32b',
    'gemini-2.5-flash',
    'gemini-2.5-pro',
  ];

  private constructor() {
    // Circuit Breaker ì´ˆê¸°í™”
    this.initializeModelHealth();
  }

  static getInstance(): SmartRoutingEngine {
    if (!SmartRoutingEngine.instance) {
      SmartRoutingEngine.instance = new SmartRoutingEngine();
    }
    return SmartRoutingEngine.instance;
  }

  // ============================================================================
  // Circuit Breaker ì´ˆê¸°í™” ë° ê´€ë¦¬
  // ============================================================================

  private initializeModelHealth(): void {
    for (const model of this.ALL_MODELS) {
      this.modelHealth.set(model, {
        model,
        circuitState: 'closed',
        consecutiveFailures: 0,
        lastFailureTime: null,
        lastSuccessTime: null,
        avgLatencyMs: 1000,
        recentSuccessRate: 1.0,
      });

      this.modelLoads.set(model, {
        model,
        activeRequests: 0,
        tokensUsedLastMinute: 0,
        remainingCapacity: 1.0,
        estimatedWaitMs: 0,
      });
    }
  }

  /**
   * ëª¨ë¸ ì„±ê³µ ê¸°ë¡ (Circuit Breaker)
   */
  recordSuccess(model: AIModel, latencyMs: number): void {
    const health = this.modelHealth.get(model);
    if (!health) return;

    const now = Date.now();
    const newAvgLatency = health.avgLatencyMs * 0.8 + latencyMs * 0.2; // EMA (Exponential Moving Average)

    this.modelHealth.set(model, {
      ...health,
      circuitState: 'closed',
      consecutiveFailures: 0,
      lastSuccessTime: now,
      avgLatencyMs: newAvgLatency,
      recentSuccessRate: Math.min(1.0, health.recentSuccessRate * 0.9 + 0.1),
    });
  }

  /**
   * ëª¨ë¸ ì‹¤íŒ¨ ê¸°ë¡ (Circuit Breaker)
   */
  recordFailure(model: AIModel): void {
    const health = this.modelHealth.get(model);
    if (!health) return;

    const now = Date.now();
    const newFailures = health.consecutiveFailures + 1;
    const newSuccessRate = Math.max(0, health.recentSuccessRate * 0.9);

    // Circuit Open ì¡°ê±´ í™•ì¸
    let newState: CircuitState = health.circuitState;
    if (newFailures >= DEFAULT_CIRCUIT_BREAKER_CONFIG.failureThreshold) {
      newState = 'open';
      console.warn(`âš ï¸ Circuit OPEN for ${model} after ${newFailures} failures`);
    }

    this.modelHealth.set(model, {
      ...health,
      circuitState: newState,
      consecutiveFailures: newFailures,
      lastFailureTime: now,
      recentSuccessRate: newSuccessRate,
    });
  }

  /**
   * Circuit ìƒíƒœ í™•ì¸ (Half-Open ì „í™˜ í¬í•¨)
   */
  private getCircuitState(model: AIModel): CircuitState {
    const health = this.modelHealth.get(model);
    if (!health) return 'closed';

    // Open ìƒíƒœì—ì„œ ë³µêµ¬ ì‹œê°„ ê²½ê³¼ ì‹œ Half-Open ì „í™˜
    if (health.circuitState === 'open' && health.lastFailureTime) {
      const elapsed = Date.now() - health.lastFailureTime;
      if (elapsed >= DEFAULT_CIRCUIT_BREAKER_CONFIG.recoveryTimeMs) {
        this.modelHealth.set(model, { ...health, circuitState: 'half-open' });
        console.log(`ğŸ”„ Circuit HALF-OPEN for ${model} (recovery attempt)`);
        return 'half-open';
      }
    }

    return health.circuitState;
  }

  /**
   * ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ (Circuit Breaker ê¸°ë°˜)
   */
  isModelAvailable(model: AIModel): boolean {
    const state = this.getCircuitState(model);
    return state !== 'open';
  }

  // ============================================================================
  // RouteLLM-style ìŠ¤ì½”ì–´ë§ ì‹œìŠ¤í…œ (Best Practice #1)
  // ============================================================================

  /**
   * ëª¨ë¸ë³„ ë¼ìš°íŒ… ìŠ¤ì½”ì–´ ê³„ì‚°
   */
  calculateRouteScore(
    model: AIModel,
    taskType: TaskType,
    providerStatus: AIProviderStatus
  ): RouteScore {
    const capabilities = MODEL_CAPABILITIES[model];
    const health = this.modelHealth.get(model);
    const load = this.modelLoads.get(model);

    // 1. Quality Score (ëª¨ë¸ ê³ ìœ  í’ˆì§ˆ)
    const quality = capabilities.qualityScore;

    // 2. Cost Score (ë¹„ìš© íš¨ìœ¨)
    const cost = capabilities.costScore;

    // 3. Latency Score (ì‘ë‹µ ì†ë„)
    const baseLatency = capabilities.speedScore;
    const healthLatencyPenalty = health
      ? Math.max(0, (health.avgLatencyMs - 1000) / 1000)
      : 0;
    const latency = Math.max(1, baseLatency - healthLatencyPenalty);

    // 4. Capability Score (íƒœìŠ¤í¬ ì í•©ì„±)
    const specializedModels = MODEL_SPECIALIZATION[taskType];
    const specializationRank = specializedModels.indexOf(model);
    const capability =
      specializationRank === -1 ? 5 : 10 - specializationRank * 2; // 1ìˆœìœ„=10, 2ìˆœìœ„=8, 3ìˆœìœ„=6

    // 5. Availability Score (ê°€ìš©ì„±)
    const circuitState = this.getCircuitState(model);
    let availability = 10;
    if (circuitState === 'open') availability = 0;
    else if (circuitState === 'half-open') availability = 5;
    if (health) availability *= health.recentSuccessRate;
    if (load) availability *= load.remainingCapacity;

    // Rate Limit ë°˜ì˜
    const rateStatus = this.getRateLimitForModel(model, providerStatus);
    if (!rateStatus.isAvailable) availability = 0;

    // ì´ì  (ê°€ì¤‘ í‰ê· )
    const total =
      quality * this.scoreWeights.quality +
      cost * this.scoreWeights.cost +
      latency * this.scoreWeights.latency +
      capability * this.scoreWeights.capability +
      availability * this.scoreWeights.availability;

    return {
      quality,
      cost,
      latency,
      capability,
      availability,
      total,
    };
  }

  /**
   * ëª¨ë¸ë³„ Rate Limit ìƒíƒœ ì¡°íšŒ í—¬í¼
   */
  private getRateLimitForModel(
    model: AIModel,
    status: AIProviderStatus
  ): RateLimitStatus {
    switch (model) {
      case 'llama-3.1-8b-instant':
        return status.groq['8b'];
      case 'llama-3.3-70b-versatile':
        return status.groq['70b'];
      case 'qwen-qwq-32b':
        return status.groq['32b'];
      case 'gemini-2.5-flash':
        return status.google.flash;
      case 'gemini-2.5-pro':
        return status.google.pro;
      default:
        return {
          remainingRPM: 0,
          remainingRPD: 0,
          remainingTPM: 0,
          isAvailable: false,
        };
    }
  }

  // ============================================================================
  // Task-Based ë¼ìš°íŒ… (Best Practice #2)
  // ============================================================================

  /**
   * ì¿¼ë¦¬ì—ì„œ íƒœìŠ¤í¬ íƒ€ì… ì¶”ë¡ 
   */
  detectTaskType(query: string, intent: QueryIntent | 'coding'): TaskType {
    const q = query.toLowerCase();

    // Intent ê¸°ë°˜ ë§¤í•‘
    if (intent === 'coding') return 'coding';
    if (intent === 'analysis') return 'reasoning';

    // í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ê°€ ê°ì§€
    if (
      q.includes('ì´ë¯¸ì§€') ||
      q.includes('ì‚¬ì§„') ||
      q.includes('ìŠ¤í¬ë¦°ìƒ·') ||
      q.includes('image')
    ) {
      return 'multimodal';
    }

    if (
      q.includes('ì½”ë“œ') ||
      q.includes('êµ¬í˜„') ||
      q.includes('í”„ë¡œê·¸ë˜ë°') ||
      q.includes('í•¨ìˆ˜') ||
      q.includes('í´ë˜ìŠ¤')
    ) {
      return 'coding';
    }

    if (
      q.includes('ë¶„ì„') ||
      q.includes('ì™œ') ||
      q.includes('ì´ìœ ') ||
      q.includes('ì¶”ë¡ ') ||
      q.includes('ìƒê°')
    ) {
      return 'reasoning';
    }

    if (
      q.includes('ì‘ì„±') ||
      q.includes('ì°½ì‘') ||
      q.includes('ì´ì•¼ê¸°') ||
      q.includes('ì‹œ') ||
      q.includes('ì†Œì„¤')
    ) {
      return 'creative';
    }

    if (
      q.includes('ì‚¬ì‹¤') ||
      q.includes('ì •ë³´') ||
      q.includes('ë­ì•¼') ||
      q.includes('ì•Œë ¤ì¤˜')
    ) {
      return 'factual';
    }

    if (intent === 'monitoring') return 'realtime';
    if (intent === 'guide') return 'factual';

    return 'general';
  }

  // ============================================================================
  // Load Balancing (Best Practice #4)
  // ============================================================================

  /**
   * ë¡œë“œ ë°¸ëŸ°ì‹± ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
   */
  private selectModelWithLoadBalancing(
    candidates: AIModel[],
    scores: Map<AIModel, RouteScore>
  ): AIModel {
    if (candidates.length === 0) {
      throw new Error('No available models');
    }

    if (candidates.length === 1) {
      return candidates[0]!; // Safe: already checked length === 1
    }

    switch (this.loadBalanceStrategy) {
      case 'round-robin':
        return this.selectRoundRobin(candidates);

      case 'least-loaded':
        return this.selectLeastLoaded(candidates);

      case 'weighted-random':
        return this.selectWeightedRandom(candidates, scores);
      default:
        return this.selectAdaptive(candidates, scores);
    }
  }

  private selectRoundRobin(candidates: AIModel[]): AIModel {
    if (!this.lastSelectedModel) {
      this.lastSelectedModel = candidates[0]!;
      return candidates[0]!;
    }

    const lastIndex = candidates.indexOf(this.lastSelectedModel);
    const nextIndex = (lastIndex + 1) % candidates.length;
    this.lastSelectedModel = candidates[nextIndex]!;
    return candidates[nextIndex]!;
  }

  private selectLeastLoaded(candidates: AIModel[]): AIModel {
    let leastLoaded = candidates[0]!;
    let lowestLoad = Infinity;

    for (const model of candidates) {
      const load = this.modelLoads.get(model);
      if (load && load.activeRequests < lowestLoad) {
        lowestLoad = load.activeRequests;
        leastLoaded = model;
      }
    }

    return leastLoaded;
  }

  private selectWeightedRandom(
    candidates: AIModel[],
    scores: Map<AIModel, RouteScore>
  ): AIModel {
    const weights = candidates.map((m) => scores.get(m)?.total ?? 0);
    const totalWeight = weights.reduce((a, b) => a + b, 0);

    if (totalWeight === 0) return candidates[0]!; // Safe: candidates.length >= 1

    let random = Math.random() * totalWeight;
    for (let i = 0; i < candidates.length; i++) {
      random -= weights[i]!;
      if (random <= 0) return candidates[i]!; // Safe: i < candidates.length
    }

    return candidates[candidates.length - 1]!; // Safe: candidates.length >= 1
  }

  private selectAdaptive(
    candidates: AIModel[],
    scores: Map<AIModel, RouteScore>
  ): AIModel {
    // Adaptive: ìŠ¤ì½”ì–´ ìµœê³  + ìµœê·¼ ì„±ê³µë¥  ê°€ì¤‘
    let bestModel = candidates[0]!; // Safe: candidates.length >= 1
    let bestScore = -Infinity;

    for (const model of candidates) {
      const score = scores.get(model)?.total ?? 0;
      const health = this.modelHealth.get(model);
      const adjustedScore = score * (health?.recentSuccessRate ?? 1.0);

      if (adjustedScore > bestScore) {
        bestScore = adjustedScore;
        bestModel = model;
      }
    }

    return bestModel;
  }

  /**
   * ë¡œë“œ ë°¸ëŸ°ì‹± ì „ëµ ì„¤ì •
   */
  setLoadBalanceStrategy(strategy: LoadBalanceStrategy): void {
    this.loadBalanceStrategy = strategy;
  }

  /**
   * ìŠ¤ì½”ì–´ ê°€ì¤‘ì¹˜ ì„¤ì •
   */
  setScoreWeights(weights: Partial<ScoreWeights>): void {
    this.scoreWeights = { ...this.scoreWeights, ...weights };
  }

  // ============================================================================
  // ê³µê°œ API
  // ============================================================================

  /**
   * ì¿¼ë¦¬ ë¶„ì„ - ë³µì¡ë„, ì˜ë„, í† í° ì¶”ì •
   */
  analyzeQuery(
    query: string,
    options: {
      intent?: QueryIntent | 'coding';
      complexity?: number;
      hasImages?: boolean;
      thinkingRequested?: boolean;
      contextMessages?: number;
    } = {}
  ): QueryAnalysis {
    const estimatedTokens = estimateTokenCount(query);
    const complexity = options.complexity ?? this.estimateComplexity(query);
    const intent = options.intent ?? this.inferIntent(query);

    return {
      query,
      estimatedTokens,
      complexity,
      intent,
      hasImages: options.hasImages ?? false,
      thinkingRequested: options.thinkingRequested ?? false,
      contextSize: (options.contextMessages ?? 0) * 500, // ë©”ì‹œì§€ë‹¹ í‰ê·  500 í† í°
      estimatedResponseTokens: estimateResponseTokens(complexity),
    };
  }

  /**
   * AI ì œê³µì ìƒíƒœ ì¡°íšŒ
   */
  getProviderStatus(): AIProviderStatus {
    const googleKey = getGoogleAIKey();
    const googleRateCheck = checkGoogleAIRateLimit();
    const groqAvailable = isGroqAIAvailable();

    const groq8bStatus = getGroqAIRateLimitStatus('llama-3.1-8b-instant');
    const groq70bStatus = getGroqAIRateLimitStatus('llama-3.3-70b-versatile');
    const groq32bStatus = getGroqAIRateLimitStatus('qwen-qwq-32b');

    const createGroqStatus = (
      model: GroqModel,
      status: ReturnType<typeof getGroqAIRateLimitStatus>
    ): RateLimitStatus => {
      const rateCheck = checkGroqAIRateLimit(model);
      return {
        remainingRPM: status.remainingRPM,
        remainingRPD: status.remainingRPD,
        remainingTPM:
          MODEL_CAPABILITIES[model].tpm - status.requestsLastMinute * 100,
        isAvailable: groqAvailable && rateCheck.allowed,
      };
    };

    return {
      groq: {
        '8b': createGroqStatus('llama-3.1-8b-instant', groq8bStatus),
        '70b': createGroqStatus('llama-3.3-70b-versatile', groq70bStatus),
        '32b': createGroqStatus('qwen-qwq-32b', groq32bStatus),
      },
      google: {
        flash: {
          remainingRPM: googleKey && googleRateCheck.allowed ? 60 : 0,
          remainingRPD: googleKey && googleRateCheck.allowed ? 1500 : 0,
          remainingTPM: googleKey && googleRateCheck.allowed ? 1000000 : 0,
          isAvailable: !!googleKey && googleRateCheck.allowed,
        },
        pro: {
          remainingRPM: googleKey && googleRateCheck.allowed ? 60 : 0,
          remainingRPD: googleKey && googleRateCheck.allowed ? 1500 : 0,
          remainingTPM: googleKey && googleRateCheck.allowed ? 1000000 : 0,
          isAvailable: !!googleKey && googleRateCheck.allowed,
        },
      },
    };
  }

  /**
   * ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­ ê²°ì •
   */
  determineFeatureRequirements(
    queryAnalysis: QueryAnalysis
  ): FeatureRequirements {
    const intentFeatures = INTENT_FEATURE_MAP[queryAnalysis.intent] ?? {};

    // ê¸°ë³¸ê°’
    const requirements: FeatureRequirements = {
      needsRAG: intentFeatures.needsRAG ?? false,
      ragMaxResults: intentFeatures.ragMaxResults,
      needsML: intentFeatures.needsML ?? false,
      needsNLP: intentFeatures.needsNLP ?? false,
      needsGCPProcessor: intentFeatures.needsGCPProcessor ?? false,
      needsServerMetrics: intentFeatures.needsServerMetrics ?? false,
      needsTools: intentFeatures.needsTools ?? queryAnalysis.complexity >= 3,
      needsVision: queryAnalysis.hasImages,
      needsReasoning:
        intentFeatures.needsReasoning ?? queryAnalysis.thinkingRequested,
    };

    // ë³µì¡ë„ ê¸°ë°˜ ì¡°ì •
    if (queryAnalysis.complexity >= 4) {
      requirements.needsRAG = true;
      requirements.ragMaxResults = Math.max(requirements.ragMaxResults ?? 3, 5);
      requirements.needsTools = true;
    }

    if (queryAnalysis.complexity === 5) {
      requirements.needsML = true;
      requirements.needsGCPProcessor = true;
    }

    return requirements;
  }

  /**
   * ğŸ¯ í•µì‹¬ ë©”ì„œë“œ: ë¼ìš°íŒ… ê²°ì •
   */
  determineRoute(context: RoutingContext): RoutingDecision {
    const { queryAnalysis, providerStatus, featureRequirements, preferences } =
      context;

    const reasoning: string[] = [];

    // 1. ì´ë¯¸ì§€ ì²˜ë¦¬ í•„ìš” ì‹œ Google í•„ìˆ˜
    if (featureRequirements.needsVision) {
      reasoning.push('ì´ë¯¸ì§€ ë¶„ì„ í•„ìš” â†’ Google Vision ëª¨ë¸ ì„ íƒ');
      return this.createVisionRoute(providerStatus, reasoning);
    }

    // 2. Thinking/Reasoning ëª¨ë“œ
    if (featureRequirements.needsReasoning || queryAnalysis.thinkingRequested) {
      reasoning.push('ì‹¬ì¸µ ì¶”ë¡  ìš”ì²­ â†’ Reasoning ì§€ì› ëª¨ë¸ ì„ íƒ');
      return this.createReasoningRoute(
        queryAnalysis,
        providerStatus,
        featureRequirements,
        reasoning
      );
    }

    // 3. ë³µì¡ë„ ê¸°ë°˜ ë¼ìš°íŒ…
    return this.createComplexityRoute(
      queryAnalysis,
      providerStatus,
      featureRequirements,
      preferences,
      reasoning
    );
  }

  /**
   * ê°„í¸ API - ì¿¼ë¦¬ë§Œìœ¼ë¡œ ë¼ìš°íŒ… ê²°ì •
   */
  route(
    query: string,
    options: {
      intent?: QueryIntent | 'coding';
      complexity?: number;
      hasImages?: boolean;
      thinkingRequested?: boolean;
      contextMessages?: number;
      preferences?: RoutingPreferences;
    } = {}
  ): RoutingDecision {
    const queryAnalysis = this.analyzeQuery(query, options);
    const providerStatus = this.getProviderStatus();
    const featureRequirements =
      this.determineFeatureRequirements(queryAnalysis);

    return this.determineRoute({
      queryAnalysis,
      providerStatus,
      featureRequirements,
      preferences: options.preferences,
    });
  }

  /**
   * ğŸ¯ í–¥ìƒëœ ë¼ìš°íŒ… API - Best Practices í†µí•©
   *
   * RouteLLM-style ìŠ¤ì½”ì–´ë§, Task-Based ë¼ìš°íŒ…, Circuit Breaker, Load Balancingì„
   * ëª¨ë‘ í†µí•©í•œ ê³ ê¸‰ ë¼ìš°íŒ… ê²°ì • APIì…ë‹ˆë‹¤.
   *
   * @returns EnhancedRoutingDecision - í›„ë³´ ëª¨ë¸ ìŠ¤ì½”ì–´, í´ë°± ì²´ì¸, íƒœìŠ¤í¬ íƒ€ì… í¬í•¨
   */
  routeEnhanced(
    query: string,
    options: {
      intent?: QueryIntent | 'coding';
      complexity?: number;
      hasImages?: boolean;
      thinkingRequested?: boolean;
      contextMessages?: number;
      preferences?: RoutingPreferences;
    } = {}
  ): EnhancedRoutingDecision {
    // 1. ê¸°ë³¸ ë¶„ì„
    const queryAnalysis = this.analyzeQuery(query, options);
    const providerStatus = this.getProviderStatus();
    const featureRequirements =
      this.determineFeatureRequirements(queryAnalysis);

    // 2. Task Type ê°ì§€
    const taskType = this.detectTaskType(
      query,
      options.intent ?? queryAnalysis.intent
    );

    // 3. ëª¨ë“  ëª¨ë¸ ìŠ¤ì½”ì–´ ê³„ì‚°
    const scores = new Map<AIModel, RouteScore>();
    const availableCandidates: AIModel[] = [];

    for (const model of this.ALL_MODELS) {
      // Circuit Breaker í™•ì¸
      if (!this.isModelAvailable(model)) {
        continue;
      }

      const score = this.calculateRouteScore(model, taskType, providerStatus);

      // ê°€ìš©ì„± 0ì´ë©´ ì œì™¸ (Rate Limit ì´ˆê³¼ ë“±)
      if (score.availability === 0) {
        continue;
      }

      scores.set(model, score);
      availableCandidates.push(model);
    }

    // 4. í›„ë³´ ëª¨ë¸ ì—†ìœ¼ë©´ ì—ëŸ¬
    if (availableCandidates.length === 0) {
      throw new Error(
        'No available AI models (all circuits open or rate limited)'
      );
    }

    // 5. ìŠ¤ì½”ì–´ ê¸°ë°˜ ì •ë ¬
    availableCandidates.sort((a, b) => {
      const scoreA = scores.get(a)?.total ?? 0;
      const scoreB = scores.get(b)?.total ?? 0;
      return scoreB - scoreA; // ë‚´ë¦¼ì°¨ìˆœ
    });

    // 6. ë¡œë“œ ë°¸ëŸ°ì‹± ì ìš©
    const selectedModel = this.selectModelWithLoadBalancing(
      availableCandidates,
      scores
    );

    // 7. ê¸°ë³¸ ë¼ìš°íŒ… ê²°ì • ìƒì„±
    const baseDecision = this.determineRoute({
      queryAnalysis,
      providerStatus,
      featureRequirements,
      preferences: options.preferences,
    });

    // 8. Fallback ì²´ì¸ êµ¬ì¶• (ìƒìœ„ 3ê°œ)
    const fallbackChain = availableCandidates
      .filter((m) => m !== selectedModel)
      .slice(0, 2);

    // 9. í›„ë³´ ëª¨ë¸ ìŠ¤ì½”ì–´ ë³€í™˜
    const candidateScores = availableCandidates.map((model) => ({
      model,
      score: scores.get(model)!,
      rank: availableCandidates.indexOf(model) + 1,
    }));

    // 10. EnhancedRoutingDecision ë°˜í™˜
    return {
      ...baseDecision,
      primaryModel: selectedModel,
      fallbackModel: fallbackChain[0] ?? null,
      taskType,
      candidateScores,
      fallbackChain,
      circuitBreakerState: {
        openCircuits: this.ALL_MODELS.filter(
          (m) => this.getCircuitState(m) === 'open'
        ),
        halfOpenCircuits: this.ALL_MODELS.filter(
          (m) => this.getCircuitState(m) === 'half-open'
        ),
      },
      selectionMethod: this.loadBalanceStrategy,
    };
  }

  /**
   * ëª¨ë¸ Health ìƒíƒœ ì¡°íšŒ (ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ìš©)
   */
  getModelHealth(model?: AIModel): ModelHealth | Map<AIModel, ModelHealth> {
    if (model) {
      return (
        this.modelHealth.get(model) ?? {
          model,
          circuitState: 'closed',
          consecutiveFailures: 0,
          lastFailureTime: null,
          lastSuccessTime: null,
          avgLatencyMs: 1000,
          recentSuccessRate: 1.0,
        }
      );
    }
    return new Map(this.modelHealth);
  }

  /**
   * ëª¨ë¸ Load ìƒíƒœ ì¡°íšŒ (ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ìš©)
   */
  getModelLoad(model?: AIModel): ModelLoadState | Map<AIModel, ModelLoadState> {
    if (model) {
      return (
        this.modelLoads.get(model) ?? {
          model,
          activeRequests: 0,
          tokensUsedLastMinute: 0,
          remainingCapacity: 1.0,
          estimatedWaitMs: 0,
        }
      );
    }
    return new Map(this.modelLoads);
  }

  /**
   * Active Request ì¦ê°€ (ìš”ì²­ ì‹œì‘ ì‹œ í˜¸ì¶œ)
   */
  incrementActiveRequests(model: AIModel): void {
    const load = this.modelLoads.get(model);
    if (load) {
      this.modelLoads.set(model, {
        ...load,
        activeRequests: load.activeRequests + 1,
        remainingCapacity: Math.max(
          0,
          1 - (load.activeRequests + 1) / 10 // ìµœëŒ€ 10ê°œ ë™ì‹œ ìš”ì²­ ê¸°ì¤€
        ),
      });
    }
  }

  /**
   * Active Request ê°ì†Œ (ìš”ì²­ ì™„ë£Œ ì‹œ í˜¸ì¶œ)
   */
  decrementActiveRequests(model: AIModel): void {
    const load = this.modelLoads.get(model);
    if (load && load.activeRequests > 0) {
      this.modelLoads.set(model, {
        ...load,
        activeRequests: load.activeRequests - 1,
        remainingCapacity: Math.max(0, 1 - (load.activeRequests - 1) / 10),
      });
    }
  }

  // ============================================================================
  // ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ
  // ============================================================================

  /**
   * ì¿¼ë¦¬ì—ì„œ ë³µì¡ë„ ì¶”ì • (Fallbackìš©)
   */
  private estimateComplexity(query: string): number {
    const q = query.toLowerCase();
    const tokens = estimateTokenCount(query);

    // ê¸´ ì¿¼ë¦¬ = ë†’ì€ ë³µì¡ë„
    if (tokens > 200) return 5;
    if (tokens > 100) return 4;

    // í‚¤ì›Œë“œ ê¸°ë°˜ ë³µì¡ë„
    if (
      q.includes('ë¶„ì„') ||
      q.includes('ì™œ') ||
      q.includes('ì–´ë–»ê²Œ') ||
      q.includes('ì½”ë“œ')
    ) {
      return 4;
    }

    if (
      q.includes('ìƒíƒœ') ||
      q.includes('cpu') ||
      q.includes('ë©”ëª¨ë¦¬') ||
      q.includes('ì„œë²„')
    ) {
      return 2;
    }

    if (q.includes('ì•ˆë…•') || q.includes('ë­ì•¼') || q.includes('ë„ì›€')) {
      return 1;
    }

    return 3; // ê¸°ë³¸ê°’
  }

  /**
   * ì¿¼ë¦¬ì—ì„œ ì˜ë„ ì¶”ë¡ 
   */
  private inferIntent(query: string): QueryIntent | 'coding' {
    const q = query.toLowerCase();

    if (
      q.includes('ì½”ë“œ') ||
      q.includes('ìŠ¤í¬ë¦½íŠ¸') ||
      q.includes('êµ¬í˜„') ||
      q.includes('í•¨ìˆ˜')
    ) {
      return 'coding';
    }

    if (
      q.includes('ë¶„ì„') ||
      q.includes('ì™œ') ||
      q.includes('ì›ì¸') ||
      q.includes('ë¬¸ì œ')
    ) {
      return 'analysis';
    }

    if (
      q.includes('ìƒíƒœ') ||
      q.includes('cpu') ||
      q.includes('ë©”ëª¨ë¦¬') ||
      q.includes('ì„œë²„')
    ) {
      return 'monitoring';
    }

    if (
      q.includes('ë°©ë²•') ||
      q.includes('ì–´ë–»ê²Œ') ||
      q.includes('ê°€ì´ë“œ') ||
      q.includes('ì„¤ëª…')
    ) {
      return 'guide';
    }

    return 'general';
  }

  /**
   * Vision(ì´ë¯¸ì§€) ë¼ìš°íŒ…
   */
  private createVisionRoute(
    status: AIProviderStatus,
    reasoning: string[]
  ): RoutingDecision {
    if (!status.google.flash.isAvailable) {
      throw new Error('ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•œ Google AI ëª¨ë¸ ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.');
    }

    reasoning.push('Google Gemini Flash (Vision ì§€ì›) ì„ íƒ');

    return {
      primaryModel: 'gemini-2.5-flash',
      fallbackModel: status.google.pro.isAvailable ? 'gemini-2.5-pro' : null,
      level: 'multimodal',
      provider: 'google',
      features: {
        useTools: true,
        useRAG: false,
        useML: false,
        useNLP: false,
        useGCP: false,
      },
      tokens: {
        maxOutputTokens: 4096,
        temperature: 0.5,
        ragContextBudget: 0,
      },
      reasoning,
      expectedPerformance: {
        latencyMs: 2000,
        qualityScore: 8,
      },
    };
  }

  /**
   * Reasoning ë¼ìš°íŒ…
   */
  private createReasoningRoute(
    _analysis: QueryAnalysis,
    status: AIProviderStatus,
    features: FeatureRequirements,
    reasoning: string[]
  ): RoutingDecision {
    // Google Pro ìš°ì„  (Reasoning ì§€ì›)
    if (status.google.pro.isAvailable) {
      reasoning.push('Google Gemini Pro (Reasoning ì§€ì›) ì„ íƒ');
      return {
        primaryModel: 'gemini-2.5-pro',
        fallbackModel: status.groq['70b'].isAvailable
          ? 'llama-3.3-70b-versatile'
          : null,
        level: 'thinking',
        provider: 'google',
        features: {
          useTools: true,
          useRAG: features.needsRAG,
          useML: features.needsML,
          useNLP: features.needsNLP,
          useGCP: features.needsGCPProcessor,
        },
        tokens: {
          maxOutputTokens: 8192,
          temperature: 0.7,
          ragContextBudget: COMPLEXITY_TOKEN_MAP[5].ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 5000,
          qualityScore: 10,
        },
      };
    }

    // Qwen-QWQ (Reasoning ì§€ì›)
    if (status.groq['32b'].isAvailable) {
      reasoning.push('Qwen-QWQ 32B (Reasoning ì§€ì›) ì„ íƒ');
      return {
        primaryModel: 'qwen-qwq-32b',
        fallbackModel: status.groq['70b'].isAvailable
          ? 'llama-3.3-70b-versatile'
          : null,
        level: 'thinking',
        provider: 'groq',
        features: {
          useTools: true,
          useRAG: features.needsRAG,
          useML: features.needsML,
          useNLP: features.needsNLP,
          useGCP: features.needsGCPProcessor,
        },
        tokens: {
          maxOutputTokens: 4096,
          temperature: 0.7,
          ragContextBudget: COMPLEXITY_TOKEN_MAP[5].ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 4000,
          qualityScore: 8,
        },
      };
    }

    // Fallback to 70b
    if (status.groq['70b'].isAvailable) {
      reasoning.push('Llama 70B (ê³ í’ˆì§ˆ, Reasoning ë¯¸ì§€ì›) Fallback');
      return {
        primaryModel: 'llama-3.3-70b-versatile',
        fallbackModel: null,
        level: 'thinking',
        provider: 'groq',
        features: {
          useTools: true,
          useRAG: features.needsRAG,
          useML: features.needsML,
          useNLP: features.needsNLP,
          useGCP: features.needsGCPProcessor,
        },
        tokens: {
          maxOutputTokens: 4096,
          temperature: 0.7,
          ragContextBudget: COMPLEXITY_TOKEN_MAP[4].ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 3000,
          qualityScore: 8,
        },
      };
    }

    throw new Error('Reasoning ì§€ì› ëª¨ë¸ ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.');
  }

  /**
   * ë³µì¡ë„ ê¸°ë°˜ ë¼ìš°íŒ…
   */
  private createComplexityRoute(
    analysis: QueryAnalysis,
    status: AIProviderStatus,
    features: FeatureRequirements,
    _preferences: RoutingPreferences | undefined,
    reasoning: string[]
  ): RoutingDecision {
    const complexity = Math.min(5, Math.max(1, analysis.complexity)) as
      | 1
      | 2
      | 3
      | 4
      | 5;
    const tokenConfig = COMPLEXITY_TOKEN_MAP[complexity];

    reasoning.push(`ë³µì¡ë„ ${complexity} ë¶„ì„ë¨`);

    // ë³µì¡ë„ 5: ìµœê³  í’ˆì§ˆ ëª¨ë¸
    if (complexity === 5) {
      return this.routeHighComplexity(status, features, tokenConfig, reasoning);
    }

    // ë³µì¡ë„ 4: ê³ ì„±ëŠ¥ ëª¨ë¸
    if (complexity === 4) {
      return this.routeMediumHighComplexity(
        status,
        features,
        tokenConfig,
        reasoning
      );
    }

    // ë³µì¡ë„ 2-3: ë°¸ëŸ°ìŠ¤ ëª¨ë¸
    if (complexity >= 2) {
      return this.routeMediumComplexity(
        status,
        features,
        complexity as 2 | 3,
        tokenConfig as (typeof COMPLEXITY_TOKEN_MAP)[2 | 3],
        reasoning
      );
    }

    // ë³µì¡ë„ 1: ë¹ ë¥¸ ëª¨ë¸
    return this.routeLowComplexity(status, features, tokenConfig, reasoning);
  }

  /**
   * ë³µì¡ë„ 5 ë¼ìš°íŒ… (ìµœê³  í’ˆì§ˆ)
   */
  private routeHighComplexity(
    status: AIProviderStatus,
    features: FeatureRequirements,
    tokenConfig: (typeof COMPLEXITY_TOKEN_MAP)[5],
    reasoning: string[]
  ): RoutingDecision {
    // Google Flash ìš°ì„  (ë†’ì€ ì»¨í…ìŠ¤íŠ¸, ì¢‹ì€ í’ˆì§ˆ)
    if (status.google.flash.isAvailable) {
      reasoning.push('Google Gemini Flash (ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸) ì„ íƒ');
      return {
        primaryModel: 'gemini-2.5-flash',
        fallbackModel: status.groq['70b'].isAvailable
          ? 'llama-3.3-70b-versatile'
          : null,
        level: 5,
        provider: 'google',
        features: {
          useTools: true,
          useRAG: features.needsRAG,
          useML: features.needsML,
          useNLP: features.needsNLP,
          useGCP: features.needsGCPProcessor,
        },
        tokens: {
          maxOutputTokens: tokenConfig.maxOutput,
          temperature: tokenConfig.temperature,
          ragContextBudget: tokenConfig.ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 3000,
          qualityScore: 8,
        },
      };
    }

    // Groq 70b Fallback
    if (status.groq['70b'].isAvailable) {
      reasoning.push('Llama 70B (ê³ í’ˆì§ˆ) ì„ íƒ');
      return {
        primaryModel: 'llama-3.3-70b-versatile',
        fallbackModel: status.groq['8b'].isAvailable
          ? 'llama-3.1-8b-instant'
          : null,
        level: 5,
        provider: 'groq',
        features: {
          useTools: true,
          useRAG: features.needsRAG,
          useML: features.needsML,
          useNLP: features.needsNLP,
          useGCP: features.needsGCPProcessor,
        },
        tokens: {
          maxOutputTokens: tokenConfig.maxOutput,
          temperature: tokenConfig.temperature,
          ragContextBudget: tokenConfig.ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 3000,
          qualityScore: 8,
        },
      };
    }

    throw new Error('AI APIê°€ ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤.');
  }

  /**
   * ë³µì¡ë„ 4 ë¼ìš°íŒ…
   */
  private routeMediumHighComplexity(
    status: AIProviderStatus,
    features: FeatureRequirements,
    tokenConfig: (typeof COMPLEXITY_TOKEN_MAP)[4],
    reasoning: string[]
  ): RoutingDecision {
    // 70b ìš°ì„  (ë¹„ìš© íš¨ìœ¨)
    if (status.groq['70b'].isAvailable) {
      reasoning.push('Llama 70B (ê³ ì„±ëŠ¥, ë¬´ë£Œ) ì„ íƒ');
      return {
        primaryModel: 'llama-3.3-70b-versatile',
        fallbackModel: status.google.flash.isAvailable
          ? 'gemini-2.5-flash'
          : null,
        level: 4,
        provider: 'groq',
        features: {
          useTools: true,
          useRAG: features.needsRAG,
          useML: features.needsML,
          useNLP: features.needsNLP,
          useGCP: features.needsGCPProcessor,
        },
        tokens: {
          maxOutputTokens: tokenConfig.maxOutput,
          temperature: tokenConfig.temperature,
          ragContextBudget: tokenConfig.ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 2500,
          qualityScore: 8,
        },
      };
    }

    // Google Flash Fallback
    if (status.google.flash.isAvailable) {
      reasoning.push('Google Gemini Flash Fallback');
      return {
        primaryModel: 'gemini-2.5-flash',
        fallbackModel: status.groq['8b'].isAvailable
          ? 'llama-3.1-8b-instant'
          : null,
        level: 4,
        provider: 'google',
        features: {
          useTools: true,
          useRAG: features.needsRAG,
          useML: features.needsML,
          useNLP: features.needsNLP,
          useGCP: features.needsGCPProcessor,
        },
        tokens: {
          maxOutputTokens: tokenConfig.maxOutput,
          temperature: tokenConfig.temperature,
          ragContextBudget: tokenConfig.ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 2000,
          qualityScore: 8,
        },
      };
    }

    throw new Error('AI APIê°€ ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤.');
  }

  /**
   * ë³µì¡ë„ 2-3 ë¼ìš°íŒ…
   */
  private routeMediumComplexity(
    status: AIProviderStatus,
    features: FeatureRequirements,
    complexity: 2 | 3,
    tokenConfig: (typeof COMPLEXITY_TOKEN_MAP)[2 | 3],
    reasoning: string[]
  ): RoutingDecision {
    // 8b ìš°ì„  (ë¹ ë¥´ê³  ì €ë ´)
    if (status.groq['8b'].isAvailable) {
      reasoning.push('Llama 8B (ë¹ ë¥¸ ì‘ë‹µ, ë¬´ë£Œ) ì„ íƒ');
      return {
        primaryModel: 'llama-3.1-8b-instant',
        fallbackModel: status.google.flash.isAvailable
          ? 'gemini-2.5-flash'
          : null,
        level: complexity,
        provider: 'groq',
        features: {
          useTools: features.needsTools,
          useRAG: features.needsRAG,
          useML: false, // ë³µì¡ë„ 2-3ì—ì„œëŠ” ML ë¹„í™œì„±í™”
          useNLP: false,
          useGCP: false,
        },
        tokens: {
          maxOutputTokens: tokenConfig.maxOutput,
          temperature: tokenConfig.temperature,
          ragContextBudget: tokenConfig.ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 1000,
          qualityScore: 6,
        },
      };
    }

    // Google Flash Fallback
    if (status.google.flash.isAvailable) {
      reasoning.push('Google Gemini Flash Fallback');
      return {
        primaryModel: 'gemini-2.5-flash',
        fallbackModel: null,
        level: complexity,
        provider: 'google',
        features: {
          useTools: features.needsTools,
          useRAG: features.needsRAG,
          useML: false,
          useNLP: false,
          useGCP: false,
        },
        tokens: {
          maxOutputTokens: tokenConfig.maxOutput,
          temperature: tokenConfig.temperature,
          ragContextBudget: tokenConfig.ragBudget,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 1500,
          qualityScore: 8,
        },
      };
    }

    throw new Error('AI APIê°€ ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤.');
  }

  /**
   * ë³µì¡ë„ 1 ë¼ìš°íŒ… (ê°„ë‹¨í•œ ì¿¼ë¦¬)
   */
  private routeLowComplexity(
    status: AIProviderStatus,
    _features: FeatureRequirements,
    tokenConfig: (typeof COMPLEXITY_TOKEN_MAP)[1],
    reasoning: string[]
  ): RoutingDecision {
    // 8b ìµœìš°ì„  (ê°€ì¥ ë¹ ë¦„)
    if (status.groq['8b'].isAvailable) {
      reasoning.push('Llama 8B (ìµœì†Œ ì§€ì—°, ë¬´ë£Œ) ì„ íƒ');
      return {
        primaryModel: 'llama-3.1-8b-instant',
        fallbackModel: status.google.flash.isAvailable
          ? 'gemini-2.5-flash'
          : null,
        level: 1,
        provider: 'groq',
        features: {
          useTools: false, // ë³µì¡ë„ 1ì—ì„œëŠ” Tools ë¹„í™œì„±í™”
          useRAG: false,
          useML: false,
          useNLP: false,
          useGCP: false,
        },
        tokens: {
          maxOutputTokens: tokenConfig.maxOutput,
          temperature: tokenConfig.temperature,
          ragContextBudget: 0,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 500,
          qualityScore: 6,
        },
      };
    }

    if (status.google.flash.isAvailable) {
      reasoning.push('Google Gemini Flash Fallback');
      return {
        primaryModel: 'gemini-2.5-flash',
        fallbackModel: null,
        level: 1,
        provider: 'google',
        features: {
          useTools: false,
          useRAG: false,
          useML: false,
          useNLP: false,
          useGCP: false,
        },
        tokens: {
          maxOutputTokens: tokenConfig.maxOutput,
          temperature: tokenConfig.temperature,
          ragContextBudget: 0,
        },
        reasoning,
        expectedPerformance: {
          latencyMs: 800,
          qualityScore: 8,
        },
      };
    }

    throw new Error('AI APIê°€ ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€í•©ë‹ˆë‹¤.');
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë‚´ë³´ë‚´ê¸°
export const smartRoutingEngine = SmartRoutingEngine.getInstance();

// ê°„í¸ í•¨ìˆ˜ ë‚´ë³´ë‚´ê¸°
export const routeQuery = (
  query: string,
  options?: Parameters<SmartRoutingEngine['route']>[1]
) => smartRoutingEngine.route(query, options);

export const analyzeQueryForRouting = (
  query: string,
  options?: Parameters<SmartRoutingEngine['analyzeQuery']>[1]
) => smartRoutingEngine.analyzeQuery(query, options);

export const getAIProviderStatus = () => smartRoutingEngine.getProviderStatus();

// ============================================================================
// ğŸš€ í–¥ìƒëœ ë¼ìš°íŒ… API (Best Practices í†µí•©)
// ============================================================================

/**
 * í–¥ìƒëœ ì¿¼ë¦¬ ë¼ìš°íŒ… (RouteLLM ìŠ¤ì½”ì–´ë§ + Circuit Breaker + Load Balancing)
 */
export const routeQueryEnhanced = (
  query: string,
  options?: Parameters<SmartRoutingEngine['routeEnhanced']>[1]
) => smartRoutingEngine.routeEnhanced(query, options);

/**
 * ëª¨ë¸ ì„±ê³µ ê¸°ë¡ (Circuit Breaker + ë ˆì´í„´ì‹œ ì¶”ì )
 */
export const recordModelSuccess = (model: AIModel, latencyMs: number) =>
  smartRoutingEngine.recordSuccess(model, latencyMs);

/**
 * ëª¨ë¸ ì‹¤íŒ¨ ê¸°ë¡ (Circuit Breaker ìƒíƒœ ì—…ë°ì´íŠ¸)
 */
export const recordModelFailure = (model: AIModel) =>
  smartRoutingEngine.recordFailure(model);

/**
 * ëª¨ë¸ í—¬ìŠ¤ ìƒíƒœ ì¡°íšŒ (ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ìš©)
 */
export const getModelHealthStatus = (model?: AIModel) =>
  smartRoutingEngine.getModelHealth(model);

/**
 * ëª¨ë¸ ë¡œë“œ ìƒíƒœ ì¡°íšŒ (ë””ë²„ê¹…/ëª¨ë‹ˆí„°ë§ìš©)
 */
export const getModelLoadStatus = (model?: AIModel) =>
  smartRoutingEngine.getModelLoad(model);

/**
 * í™œì„± ìš”ì²­ ì¦ê°€ (ë¡œë“œ ë°¸ëŸ°ì‹±ìš© - ìš”ì²­ ì‹œì‘ ì‹œ í˜¸ì¶œ)
 */
export const incrementModelLoad = (model: AIModel) =>
  smartRoutingEngine.incrementActiveRequests(model);

/**
 * í™œì„± ìš”ì²­ ê°ì†Œ (ë¡œë“œ ë°¸ëŸ°ì‹±ìš© - ìš”ì²­ ì™„ë£Œ ì‹œ í˜¸ì¶œ)
 */
export const decrementModelLoad = (model: AIModel) =>
  smartRoutingEngine.decrementActiveRequests(model);

// Type exports for consumers
export type {
  EnhancedRoutingDecision,
  RouteScore,
  RoutingDecision,
  TaskType,
} from '../../types/ai/routing-types';
