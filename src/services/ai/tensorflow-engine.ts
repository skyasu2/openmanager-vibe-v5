/**
 * ğŸ§  TensorFlow.js AI ì—”ì§„ v3.0
 * 
 * âœ… Vercel ì„œë²„ë¦¬ìŠ¤ ì™„ì „ í˜¸í™˜
 * âœ… ë¸Œë¼ìš°ì € + Node.js ì§€ì›  
 * âœ… ì¥ì•  ì˜ˆì¸¡ ì‹ ê²½ë§
 * âœ… ì´ìƒ íƒì§€ ì˜¤í† ì¸ì½”ë”
 * âœ… ì‹œê³„ì—´ LSTM ëª¨ë¸
 * âœ… KMeans í´ëŸ¬ìŠ¤í„°ë§ (Python ì´ì „)
 * âœ… StandardScaler (Python ì´ì „)
 * âœ… ì™„ì „ ë¡œì»¬ AI (ì™¸ë¶€ API ì—†ìŒ)
 */

import * as tf from '@tensorflow/tfjs';

interface PredictionResult {
  prediction: number[];
  confidence: number;
  model_info: string;
  processing_time: number;
}

interface AnomalyResult {
  is_anomaly: boolean;
  anomaly_score: number;
  threshold: number;
  model_info: string;
}

interface ClusterResult {
  cluster_labels: number[];
  centroids: number[][];
  inertia: number;
  model_info: string;
}

interface AIAnalysisResult {
  failure_predictions: Record<string, PredictionResult>;
  anomaly_detections: Record<string, AnomalyResult>;
  trend_predictions: Record<string, number[]>;
  clustering_analysis?: ClusterResult;
  ai_insights: string[];
  processing_stats: {
    total_time: number;
    models_used: string[];
    metrics_analyzed: number;
  };
}

/**
 * ğŸ”§ StandardScaler - Python scikit-learn ë™ë“± ê¸°ëŠ¥
 */
class StandardScaler {
  private mean: tf.Tensor | null = null;
  private std: tf.Tensor | null = null;
  private fitted = false;

  fit(data: tf.Tensor): void {
    this.mean = tf.mean(data, 0);
    const variance = tf.moments(data, 0).variance;
    this.std = variance.sqrt();
    this.fitted = true;
  }

  transform(data: tf.Tensor): tf.Tensor {
    if (!this.fitted || !this.mean || !this.std) {
      throw new Error('StandardScaler must be fitted before transform');
    }
    return data.sub(this.mean).div(this.std);
  }

  fitTransform(data: tf.Tensor): tf.Tensor {
    this.fit(data);
    return this.transform(data);
  }

  dispose(): void {
    if (this.mean) this.mean.dispose();
    if (this.std) this.std.dispose();
  }
}

/**
 * ğŸ¯ KMeans í´ëŸ¬ìŠ¤í„°ë§ - Python scikit-learn ë™ë“± ê¸°ëŠ¥
 */
class KMeans {
  private centroids: tf.Tensor | null = null;
  private nClusters: number;
  private maxIters: number;
  private tolerance: number;

  constructor(nClusters: number = 3, maxIters: number = 100, tolerance: number = 1e-4) {
    this.nClusters = nClusters;
    this.maxIters = maxIters;
    this.tolerance = tolerance;
  }

  async fit(data: tf.Tensor): Promise<void> {
    const [nSamples, nFeatures] = data.shape as [number, number];
    
    // ëœë¤ ì´ˆê¸° ì¤‘ì‹¬ì 
    this.centroids = tf.randomUniform([this.nClusters, nFeatures]);
    
    for (let iter = 0; iter < this.maxIters; iter++) {
      // ê° ì ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ì‹¬ì  ì°¾ê¸°
      const distances = this.calculateDistances(data);
      const labels = tf.argMin(distances, 1);
      
      // ìƒˆë¡œìš´ ì¤‘ì‹¬ì  ê³„ì‚°
      const newCentroids = await this.updateCentroids(data, labels);
      
      // ìˆ˜ë ´ í™•ì¸
      const centroidDiff = tf.norm(newCentroids.sub(this.centroids!));
      const diffValue = await centroidDiff.data();
      
      this.centroids!.dispose();
      this.centroids = newCentroids;
      
      if (diffValue[0] < this.tolerance) {
        console.log(`ğŸ¯ KMeans converged after ${iter + 1} iterations`);
        break;
      }
      
      labels.dispose();
      distances.dispose();
      centroidDiff.dispose();
    }
  }

  async predict(data: tf.Tensor): Promise<number[]> {
    if (!this.centroids) {
      throw new Error('KMeans must be fitted before prediction');
    }
    
    const distances = this.calculateDistances(data);
    const labels = tf.argMin(distances, 1);
    const labelsArray = await labels.data();
    
    distances.dispose();
    labels.dispose();
    
    return Array.from(labelsArray);
  }

  async fitPredict(data: tf.Tensor): Promise<ClusterResult> {
    await this.fit(data);
    const labels = await this.predict(data);
    const inertia = await this.calculateInertia(data, labels);
    const centroids = await this.centroids!.array() as number[][];
    
    return {
      cluster_labels: labels,
      centroids: centroids,
      inertia: inertia,
      model_info: `KMeans (k=${this.nClusters}, iter=${this.maxIters})`
    };
  }

  private calculateDistances(data: tf.Tensor): tf.Tensor {
    // ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°: ||x - c||Â²
    const expanded = data.expandDims(1); // [n_samples, 1, n_features]
    const centroidsExpanded = this.centroids!.expandDims(0); // [1, n_clusters, n_features]
    const diff = expanded.sub(centroidsExpanded);
    return tf.sum(tf.square(diff), 2); // [n_samples, n_clusters]
  }

  private async updateCentroids(data: tf.Tensor, labels: tf.Tensor): Promise<tf.Tensor> {
    const [nSamples, nFeatures] = data.shape as [number, number];
    const newCentroids = [];
    
    // ë¼ë²¨ê³¼ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ ë³€í™˜
    const labelsArray = await labels.data();
    const dataArray = await data.array() as number[][];
    
    for (let k = 0; k < this.nClusters; k++) {
      // í´ëŸ¬ìŠ¤í„° kì— ì†í•˜ëŠ” í¬ì¸íŠ¸ë“¤ ì°¾ê¸°
      const clusterPoints = [];
      for (let i = 0; i < labelsArray.length; i++) {
        if (labelsArray[i] === k) {
          clusterPoints.push(dataArray[i]);
        }
      }
      
      if (clusterPoints.length > 0) {
        // í´ëŸ¬ìŠ¤í„° í¬ì¸íŠ¸ë“¤ì˜ í‰ê·  ê³„ì‚°
        const clusterTensor = tf.tensor2d(clusterPoints);
        const centroid = tf.mean(clusterTensor, 0);
        newCentroids.push(centroid);
        clusterTensor.dispose();
      } else {
        // ë¹ˆ í´ëŸ¬ìŠ¤í„°ì˜ ê²½ìš° ëœë¤ ì ìœ¼ë¡œ ì¬ì´ˆê¸°í™”
        const randomCentroid = tf.randomUniform([nFeatures]);
        newCentroids.push(randomCentroid);
      }
    }
    
    return tf.stack(newCentroids);
  }

  private async calculateInertia(data: tf.Tensor, labels: number[]): Promise<number> {
    let totalInertia = 0;
    const dataArray = await data.array() as number[][];
    const centroidsArray = await this.centroids!.array() as number[][];
    
    for (let i = 0; i < labels.length; i++) {
      const clusterIdx = labels[i];
      const point = dataArray[i];
      const centroid = centroidsArray[clusterIdx];
      
      // ìœ í´ë¦¬ë“œ ê±°ë¦¬ì˜ ì œê³±
      const distance = point.reduce((sum, val, idx) => {
        const diff = val - centroid[idx];
        return sum + diff * diff;
      }, 0);
      
      totalInertia += distance;
    }
    
    return totalInertia;
  }

  dispose(): void {
    if (this.centroids) {
      this.centroids.dispose();
    }
  }
}

export class TensorFlowAIEngine {
  private models: Map<string, tf.LayersModel> = new Map();
  private initialized = false;
  private modelSpecs: Map<string, any> = new Map();
  private scaler: StandardScaler = new StandardScaler();
  private kmeans: KMeans = new KMeans();

  constructor() {
    this.initializeModelSpecs();
  }

  private initializeModelSpecs(): void {
    // ğŸ¯ ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸ ìŠ¤í™
    this.modelSpecs.set('failure_prediction', {
      input_shape: [10],
      layers: [
        { type: 'dense', units: 64, activation: 'relu' },
        { type: 'dropout', rate: 0.2 },
        { type: 'dense', units: 32, activation: 'relu' },
        { type: 'dropout', rate: 0.2 },
        { type: 'dense', units: 16, activation: 'relu' },
        { type: 'dense', units: 1, activation: 'sigmoid' }
      ],
      optimizer: 'adam',
      loss: 'binaryCrossentropy',
      description: 'ì¥ì•  í™•ë¥  ì˜ˆì¸¡ ì‹ ê²½ë§'
    });

    // ğŸ” ì´ìƒ íƒì§€ ëª¨ë¸ ìŠ¤í™ (ì˜¤í† ì¸ì½”ë”)
    this.modelSpecs.set('anomaly_detection', {
      input_shape: [20],
      encoder_layers: [
        { type: 'dense', units: 16, activation: 'relu' },
        { type: 'dense', units: 8, activation: 'relu' },
        { type: 'dense', units: 4, activation: 'relu' }
      ],
      decoder_layers: [
        { type: 'dense', units: 8, activation: 'relu' },
        { type: 'dense', units: 16, activation: 'relu' },
        { type: 'dense', units: 20, activation: 'linear' }
      ],
      optimizer: 'adam',
      loss: 'meanSquaredError',
      description: 'ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ì´ìƒ íƒì§€'
    });

    // ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ ìŠ¤í™ (LSTM)
    this.modelSpecs.set('timeseries', {
      input_shape: [10, 1],
      layers: [
        { type: 'lstm', units: 50, return_sequences: true },
        { type: 'dropout', rate: 0.2 },
        { type: 'lstm', units: 50, return_sequences: false },
        { type: 'dropout', rate: 0.2 },
        { type: 'dense', units: 25 },
        { type: 'dense', units: 1 }
      ],
      optimizer: 'adam',
      loss: 'meanSquaredError',
      description: 'LSTM ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡'
    });
  }

  async initialize(): Promise<void> {
    if (this.initialized) return;

    console.log('ğŸ§  TensorFlow.js AI ì—”ì§„ ì´ˆê¸°í™” ì¤‘...');
    
    try {
      // ê¸°ì¡´ ëª¨ë¸ë“¤ ì™„ì „ ì •ë¦¬ (ì¤‘ë³µ ë°©ì§€)
      this.dispose();
      
      // TensorFlow.js ì „ì—­ ë³€ìˆ˜ ì •ë¦¬
      await this.cleanupTensorFlowGlobals();
      
      // TensorFlow.js ë°±ì—”ë“œ ì„¤ì •
      await this.setupTensorFlowBackend();
      
      // ëª¨ë¸ ìŠ¤í™ ì´ˆê¸°í™”
      this.initializeModelSpecs();
      
      // ëª¨ë¸ë“¤ ì´ˆê¸°í™”
      await this.initializeAllModels();
      
      this.initialized = true;
      console.log('âœ… TensorFlow.js AI ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ');
      console.log(`ğŸ”§ ë°±ì—”ë“œ: ${tf.getBackend()}`);
      console.log(`ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©: ${JSON.stringify(tf.memory())}`);
      
    } catch (error: any) {
      console.error('âŒ TensorFlow.js ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      // ì´ˆê¸°í™” ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰ (fallback ëª¨ë“œ)
      this.initialized = false;
    }
  }
  private async cleanupTensorFlowGlobals(): Promise<void> {
    try {
      // ê¸°ì¡´ ëª¨ë¸ë“¤ ì™„ì „ dispose
      if (this.models.size > 0) {
        console.log(`ğŸ§¹ ${this.models.size}ê°œ ê¸°ì¡´ ëª¨ë¸ ì •ë¦¬ ì¤‘...`);
        for (const [name, model] of this.models.entries()) {
          try {
            model.dispose();
            console.log(`âœ… ëª¨ë¸ ì •ë¦¬: ${name}`);
          } catch (error) {
            console.warn(`âš ï¸ ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨: ${name}`, error);
          }
        }
        this.models.clear();
      }
      
      // ëª¨ë“  TensorFlow.js í…ì„œì™€ ë³€ìˆ˜ ì •ë¦¬
      tf.disposeVariables();
      
      // ë©”ëª¨ë¦¬ ê°•ì œ ì •ë¦¬
      const memoryInfo = tf.memory();
      if (memoryInfo.numTensors > 0) {
        console.log(`ğŸ§¹ ${memoryInfo.numTensors}ê°œ í…ì„œ ì •ë¦¬ ì¤‘...`);
        
        // ë°±ì—”ë“œ ì™„ì „ ì¬ì„¤ì •
        const currentBackend = tf.getBackend();
        try {
          await tf.removeBackend(currentBackend);
          console.log(`ğŸ”„ ë°±ì—”ë“œ ${currentBackend} ì œê±°ë¨`);
        } catch (error) {
          console.warn('âš ï¸ ë°±ì—”ë“œ ì œê±° ì‹¤íŒ¨ (ê³„ì† ì§„í–‰):', error);
        }
        
        // ë°±ì—”ë“œ ì¬ì„¤ì •
        await tf.setBackend(currentBackend);
        await tf.ready();
        console.log(`ğŸ”„ ë°±ì—”ë“œ ${currentBackend} ì¬ì„¤ì • ì™„ë£Œ`);
      }
      
      console.log('âœ… TensorFlow.js ì „ì—­ ìƒíƒœ ì •ë¦¬ ì™„ë£Œ');
    } catch (error) {
      console.warn('âš ï¸ TensorFlow.js ì „ì—­ ì •ë¦¬ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰):', error);
    }
  }

  private async setupTensorFlowBackend(): Promise<void> {
    try {
      if (typeof window !== 'undefined') {
        // ë¸Œë¼ìš°ì € í™˜ê²½
        await tf.setBackend('webgl');
      } else {
        // Node.js í™˜ê²½ (Vercel)
        await tf.setBackend('cpu');
      }
      await tf.ready();
    } catch (error) {
      console.warn('âš ï¸ WebGL ë°±ì—”ë“œ ì‹¤íŒ¨, CPU ë°±ì—”ë“œë¡œ ì „í™˜');
      await tf.setBackend('cpu');
      await tf.ready();
    }
  }

  private async initializeAllModels(): Promise<void> {
    const modelPromises = [
      this.initializeFailurePredictionModel(),
      this.initializeAnomalyDetectionModel(),
      this.initializeTimeSeriesModel()
    ];

    await Promise.all(modelPromises);
    console.log(`ğŸ“Š ${this.models.size}ê°œ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ`);
  }

  private async initializeFailurePredictionModel(): Promise<void> {
    const spec = this.modelSpecs.get('failure_prediction')!;
    const timestamp = Date.now();
    
    const model = tf.sequential({
      layers: [
        tf.layers.dense({ 
          inputShape: spec.input_shape, 
          units: 64, 
          activation: 'relu',
          name: `failure_hidden1_${timestamp}`
        }),
        tf.layers.dropout({ rate: 0.2 }),
        tf.layers.dense({ 
          units: 32, 
          activation: 'relu',
          name: `failure_hidden2_${timestamp}`
        }),
        tf.layers.dropout({ rate: 0.2 }),
        tf.layers.dense({ 
          units: 16, 
          activation: 'relu',
          name: `failure_hidden3_${timestamp}`
        }),
        tf.layers.dense({ 
          units: 1, 
          activation: 'sigmoid',
          name: `failure_output_${timestamp}`
        })
      ]
    });

    model.compile({
      optimizer: tf.train.adam(0.001),
      loss: 'binaryCrossentropy',
      metrics: ['accuracy']
    });

    this.models.set('failure_prediction', model);
    console.log('ğŸ¯ ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ');
  }

  private async initializeAnomalyDetectionModel(): Promise<void> {
    const timestamp = Date.now();
    
    // ì˜¤í† ì¸ì½”ë” ì¸ì½”ë”
    const encoder = tf.sequential({
      layers: [
        tf.layers.dense({ 
          inputShape: [20], 
          units: 16, 
          activation: 'relu',
          name: `encoder_1_${timestamp}`
        }),
        tf.layers.dense({ 
          units: 8, 
          activation: 'relu',
          name: `encoder_2_${timestamp}`
        }),
        tf.layers.dense({ 
          units: 4, 
          activation: 'relu',
          name: `bottleneck_${timestamp}`
        })
      ]
    });

    // ì˜¤í† ì¸ì½”ë” ë””ì½”ë”
    const decoder = tf.sequential({
      layers: [
        tf.layers.dense({ 
          inputShape: [4], 
          units: 8, 
          activation: 'relu',
          name: `decoder_1_${timestamp}`
        }),
        tf.layers.dense({ 
          units: 16, 
          activation: 'relu',
          name: `decoder_2_${timestamp}`
        }),
        tf.layers.dense({ 
          units: 20, 
          activation: 'linear',
          name: `decoder_output_${timestamp}`
        })
      ]
    });

    // ì „ì²´ ì˜¤í† ì¸ì½”ë”
    const autoencoder = tf.sequential();
    autoencoder.add(encoder);
    autoencoder.add(decoder);

    autoencoder.compile({
      optimizer: 'adam',
      loss: 'meanSquaredError'
    });

    this.models.set('anomaly_detection', autoencoder);
    console.log('ğŸ” ì´ìƒ íƒì§€ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ');
  }

  private async initializeTimeSeriesModel(): Promise<void> {
    const timestamp = Date.now();
    
    const model = tf.sequential({
      layers: [
        tf.layers.lstm({ 
          units: 50, 
          returnSequences: true, 
          inputShape: [10, 1],
          name: `lstm_1_${timestamp}`
        }),
        tf.layers.dropout({ rate: 0.2 }),
        tf.layers.lstm({ 
          units: 50, 
          returnSequences: false,
          name: `lstm_2_${timestamp}`
        }),
        tf.layers.dropout({ rate: 0.2 }),
        tf.layers.dense({ 
          units: 25,
          name: `dense_1_${timestamp}`
        }),
        tf.layers.dense({ 
          units: 1,
          name: `output_${timestamp}`
        })
      ]
    });

    model.compile({
      optimizer: 'adam',
      loss: 'meanSquaredError',
      metrics: ['mae']
    });

    this.models.set('timeseries', model);
    console.log('ğŸ“ˆ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ');
  }

  async predictFailure(metrics: number[]): Promise<PredictionResult> {
    await this.initialize();
    
    const startTime = Date.now();
    const model = this.models.get('failure_prediction');
    if (!model) throw new Error('ì¥ì•  ì˜ˆì¸¡ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ');

    // ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬
    const processedMetrics = this.preprocessMetrics(metrics, 10);
    const inputTensor = tf.tensor2d([processedMetrics]);

    try {
      const prediction = model.predict(inputTensor) as tf.Tensor;
      const predictionArray = await prediction.data();
      
      const processingTime = Date.now() - startTime;
      
      return {
        prediction: Array.from(predictionArray),
        confidence: predictionArray[0] > 0.5 ? predictionArray[0] : 1 - predictionArray[0],
        model_info: 'TensorFlow.js ì‹ ê²½ë§ (4ì¸µ, ReLU+Sigmoid)',
        processing_time: processingTime
      };
    } finally {
      inputTensor.dispose();
    }
  }

  async detectAnomalies(timeSeries: number[]): Promise<AnomalyResult> {
    await this.initialize();
    
    const model = this.models.get('anomaly_detection');
    if (!model) throw new Error('ì´ìƒ íƒì§€ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ');

    // ë°ì´í„° ì „ì²˜ë¦¬
    const processedData = this.preprocessMetrics(timeSeries, 20);
    const inputTensor = tf.tensor2d([processedData]);

    try {
      const reconstruction = model.predict(inputTensor) as tf.Tensor;
      const reconstructionData = await reconstruction.data();
      
      // ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚° (MSE)
      const originalData = await inputTensor.data();
      const mse = this.calculateMSE(Array.from(originalData), Array.from(reconstructionData));
      
      // ë™ì  ì„ê³„ê°’ (ë°ì´í„°ì˜ í‘œì¤€í¸ì°¨ ê¸°ë°˜)
      const threshold = this.calculateDynamicThreshold(processedData);
      const isAnomaly = mse > threshold;
      
      return {
        is_anomaly: isAnomaly,
        anomaly_score: mse,
        threshold: threshold,
        model_info: 'TensorFlow.js ì˜¤í† ì¸ì½”ë” (20â†’4â†’20)'
      };
    } finally {
      inputTensor.dispose();
    }
  }

  async predictTimeSeries(historicalData: number[], steps: number = 5): Promise<number[]> {
    await this.initialize();
    
    const model = this.models.get('timeseries');
    if (!model) throw new Error('ì‹œê³„ì—´ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ');

    // ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬
    const sequences = this.createSequences(historicalData, 10);
    if (sequences.length === 0) return [];

    const lastSequence = sequences[sequences.length - 1];
    const currentSequence = [...lastSequence];

    try {
      const predictions = [];

      for (let i = 0; i < steps; i++) {
        const sequenceTensor = tf.tensor3d([currentSequence]);
        const prediction = model.predict(sequenceTensor) as tf.Tensor;
        const predictionValue = (await prediction.data())[0];
        
        predictions.push(predictionValue);
        
        // ë‹¤ìŒ ì˜ˆì¸¡ì„ ìœ„í•´ ì‹œí€€ìŠ¤ ì—…ë°ì´íŠ¸
        currentSequence.shift();
        currentSequence.push([predictionValue]);
        
        sequenceTensor.dispose();
        prediction.dispose();
      }

      return predictions;
    } catch (error: any) {
      console.error('ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹¤íŒ¨:', error);
      return [];
    }
  }

  async clusterAnalysis(data: number[][]): Promise<ClusterResult> {
    await this.initialize();
    
    if (data.length < 3) {
      throw new Error('í´ëŸ¬ìŠ¤í„°ë§ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 3ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤');
    }
    
    const startTime = Date.now();
    
    try {
      // ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
      const dataTensor = tf.tensor2d(data);
      
      // ë°ì´í„° ì •ê·œí™”
      const scaledData = this.scaler.fitTransform(dataTensor);
      
      // KMeans í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
      const result = await this.kmeans.fitPredict(scaledData);
      
      // ë¦¬ì†ŒìŠ¤ ì •ë¦¬
      dataTensor.dispose();
      scaledData.dispose();
      
      const processingTime = Date.now() - startTime;
      console.log(`ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: ${processingTime}ms`);
      
      return {
        ...result,
        model_info: `${result.model_info} (${processingTime}ms)`
      };
      
    } catch (error: any) {
      console.error('âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  async analyzeMetricsWithAI(metrics: Record<string, number[]>): Promise<AIAnalysisResult> {
    await this.initialize();
    
    const startTime = Date.now();
    const analysis: AIAnalysisResult = {
      failure_predictions: {},
      anomaly_detections: {},
      trend_predictions: {},
      ai_insights: [],
      processing_stats: {
        total_time: 0,
        models_used: [],
        metrics_analyzed: Object.keys(metrics).length
      }
    };

    try {
      // ê¸°ì¡´ ë¶„ì„ë“¤
      for (const [metricName, values] of Object.entries(metrics)) {
        if (values.length === 0) continue;

        // ì¥ì•  ì˜ˆì¸¡
        try {
          const failurePred = await this.predictFailure(values);
          analysis.failure_predictions[metricName] = failurePred;
          analysis.processing_stats.models_used.push('failure_prediction');
        } catch (error: any) {
          console.warn(`âš ï¸ ${metricName} ì¥ì•  ì˜ˆì¸¡ ì‹¤íŒ¨:`, error.message);
        }

        // ì´ìƒ íƒì§€
        try {
          const anomalyResult = await this.detectAnomalies(values);
          analysis.anomaly_detections[metricName] = anomalyResult;
          analysis.processing_stats.models_used.push('anomaly_detection');
        } catch (error: any) {
          console.warn(`âš ï¸ ${metricName} ì´ìƒ íƒì§€ ì‹¤íŒ¨:`, error.message);
        }

        // ì‹œê³„ì—´ ì˜ˆì¸¡
        try {
          const trendPred = await this.predictTimeSeries(values, 5);
          analysis.trend_predictions[metricName] = trendPred;
          analysis.processing_stats.models_used.push('timeseries');
        } catch (error: any) {
          console.warn(`âš ï¸ ${metricName} íŠ¸ë Œë“œ ì˜ˆì¸¡ ì‹¤íŒ¨:`, error.message);
        }
      }

      // ğŸ†• í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì¶”ê°€
      try {
        const allMetricsData = Object.values(metrics).filter(values => values.length > 0);
        if (allMetricsData.length >= 3) {
          // ë©”íŠ¸ë¦­ë“¤ì„ í–‰ë ¬ë¡œ ë³€í™˜ (ê° í–‰ì€ ì‹œê°„ì , ê° ì—´ì€ ë©”íŠ¸ë¦­)
          const maxLength = Math.max(...allMetricsData.map(arr => arr.length));
          const matrixData = [];
          
          for (let i = 0; i < Math.min(maxLength, 100); i++) { // ìµœëŒ€ 100ê°œ í¬ì¸íŠ¸
            const row = allMetricsData.map(arr => arr[i] || 0);
            matrixData.push(row);
          }
          
          const clusterResult = await this.clusterAnalysis(matrixData);
          analysis.clustering_analysis = clusterResult;
          analysis.processing_stats.models_used.push('kmeans_clustering');
          
          // í´ëŸ¬ìŠ¤í„°ë§ ì¸ì‚¬ì´íŠ¸ ì¶”ê°€
          const uniqueClusters = new Set(clusterResult.cluster_labels).size;
          analysis.ai_insights.push(`ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ${uniqueClusters}ê°œ íŒ¨í„´ìœ¼ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤`);
          analysis.ai_insights.push(`í´ëŸ¬ìŠ¤í„° ë‚´ ì‘ì§‘ë„: ${clusterResult.inertia.toFixed(2)}`);
        }
      } catch (error: any) {
        console.warn('âš ï¸ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨:', error.message);
      }

      // AI ì¸ì‚¬ì´íŠ¸ ìƒì„±
      this.generateAIInsights(analysis);

    } catch (error: any) {
      console.error('âŒ AI ë¶„ì„ ì‹¤íŒ¨:', error);
      analysis.ai_insights.push(`ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ${error.message}`);
    }

    analysis.processing_stats.total_time = Date.now() - startTime;
    analysis.processing_stats.models_used = [...new Set(analysis.processing_stats.models_used)];

    console.log(`ğŸ§  AI ë¶„ì„ ì™„ë£Œ: ${analysis.processing_stats.total_time}ms`);
    return analysis;
  }

  private generateAIInsights(analysis: AIAnalysisResult): void {
    // AI ì¸ì‚¬ì´íŠ¸ ìƒì„± ë¡œì§ì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
    // í˜„ì¬ëŠ” ì¸ì‚¬ì´íŠ¸ ìƒì„± ë¡œì§ì´ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
  }

  private preprocessMetrics(metrics: number[], targetLength: number): number[] {
    if (!Array.isArray(metrics) || metrics.length === 0) {
      return new Array(targetLength).fill(0);
    }

    // ì •ê·œí™” (0-1 ë²”ìœ„)
    const min = Math.min(...metrics);
    const max = Math.max(...metrics);
    const range = max - min || 1;
    
    const normalized = metrics.map(val => (val - min) / range);

    // ê¸¸ì´ ì¡°ì •
    if (normalized.length === targetLength) return normalized;
    
    if (normalized.length > targetLength) {
      // ìµœê·¼ ë°ì´í„°ë§Œ ì‚¬ìš©
      return normalized.slice(-targetLength);
    } else {
      // í‰ê· ê°’ìœ¼ë¡œ íŒ¨ë”©
      const mean = normalized.reduce((sum, val) => sum + val, 0) / normalized.length;
      const padded = [...normalized];
      while (padded.length < targetLength) {
        padded.unshift(mean);
      }
      return padded;
    }
  }

  private createSequences(data: number[], sequenceLength: number): number[][][] {
    if (data.length < sequenceLength) return [];
    
    const sequences = [];
    for (let i = 0; i <= data.length - sequenceLength; i++) {
      const sequence = data.slice(i, i + sequenceLength).map(val => [val]);
      sequences.push(sequence);
    }
    return sequences;
  }

  private calculateMSE(original: number[], reconstructed: number[]): number {
    if (original.length !== reconstructed.length) return Infinity;
    
    const mse = original.reduce((sum, val, i) => {
      const error = val - reconstructed[i];
      return sum + error * error;
    }, 0) / original.length;
    
    return mse;
  }

  private calculateDynamicThreshold(data: number[]): number {
    // í‘œì¤€í¸ì°¨ ê¸°ë°˜ ë™ì  ì„ê³„ê°’
    const mean = data.reduce((sum, val) => sum + val, 0) / data.length;
    const variance = data.reduce((sum, val) => sum + Math.pow(val - mean, 2), 0) / data.length;
    const stdDev = Math.sqrt(variance);
    
    // 2-sigma ê·œì¹™ ì ìš©
    return Math.max(0.01, stdDev * 2);
  }

  async getModelInfo(): Promise<any> {
    return {
      framework: 'TensorFlow.js',
      version: tf.version_core,
      backend: tf.getBackend(),
      models: Array.from(this.models.keys()),
      memory_usage: tf.memory(),
      initialized: this.initialized,
      model_specs: Object.fromEntries(this.modelSpecs),
      supported_features: [
        'ì¥ì•  ì˜ˆì¸¡',
        'ì´ìƒ íƒì§€',
        'ì‹œê³„ì—´ ì˜ˆì¸¡',
        'ì‹¤ì‹œê°„ ë¶„ì„',
        'Vercel ì„œë²„ë¦¬ìŠ¤ í˜¸í™˜'
      ]
    };
  }

  dispose(): void {
    console.log('ğŸ—‘ï¸ TensorFlow.js ëª¨ë¸ ì •ë¦¬ ì¤‘...');
    
    this.models.forEach((model, name) => {
      try {
        model.dispose();
        console.log(`âœ… ${name} ëª¨ë¸ ì •ë¦¬ ì™„ë£Œ`);
      } catch (error: any) {
        console.error(`âŒ ${name} ëª¨ë¸ ì •ë¦¬ ì‹¤íŒ¨:`, error);
      }
    });
    
    this.models.clear();
    this.initialized = false;
    
    console.log(`ğŸ“Š ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: ${JSON.stringify(tf.memory())}`);
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const tensorFlowAIEngine = new TensorFlowAIEngine(); 