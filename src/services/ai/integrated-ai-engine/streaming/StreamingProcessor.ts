/**
 * ğŸ”„ Streaming Processor
 * 
 * ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
 * - ì‹¤ì‹œê°„ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
 * - ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬
 * - ì§„í–‰ ìƒí™© ì¶”ì 
 * - ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬
 */

import { aiEngineUtils } from '../utils/AIEngineUtils';
import { nlpProcessor } from '../nlp/NLPProcessor';
import { intentHandlerManager } from '../intents/IntentHandlerManager';
import { responseGeneratorManager } from '../generators/ResponseGeneratorManager';
import {
    StreamingProcessor as IStreamingProcessor,
    StreamingChunk,
    AIQueryRequest,
    AIQueryResponse,
    IntentType
} from '../types/AIEngineTypes';

export class StreamingProcessor implements IStreamingProcessor {
    private static instance: StreamingProcessor;
    private chunkCounter = 0;

    /**
     * ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ
     */
    static getInstance(): StreamingProcessor {
        if (!StreamingProcessor.instance) {
            StreamingProcessor.instance = new StreamingProcessor();
        }
        return StreamingProcessor.instance;
    }

    /**
     * ì¿¼ë¦¬ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
     */
    async *processQueryStream(
        request: AIQueryRequest
    ): AsyncGenerator<StreamingChunk, void, unknown> {
        const queryId = aiEngineUtils.generateQueryId();
        const startTime = performance.now();

        try {
            // 1. ì´ˆê¸°í™” ì²­í¬
            yield this.createChunk('progress', {
                step: 'initialization',
                message: 'AI ì—”ì§„ ì´ˆê¸°í™” ì¤‘...',
                progress: 0
            });

            // 2. NLP ì²˜ë¦¬ ì²­í¬
            yield this.createChunk('progress', {
                step: 'nlp_processing',
                message: 'ìì—°ì–´ ì²˜ë¦¬ ì¤‘...',
                progress: 20
            });

            const nlpResult = await nlpProcessor.processQuery(request.query);

            yield this.createChunk('partial_result', {
                step: 'nlp_complete',
                data: {
                    intent: nlpResult.intent,
                    confidence: nlpResult.confidence,
                    language: nlpResult.language
                },
                progress: 40
            });

            // 3. Intent ì²˜ë¦¬ ì²­í¬
            yield this.createChunk('progress', {
                step: 'intent_processing',
                message: `${nlpResult.intent} ì²˜ë¦¬ ì¤‘...`,
                progress: 60
            });

            // ì´ˆê¸° ì‘ë‹µ êµ¬ì¡° ìƒì„±
            const response: AIQueryResponse = {
                success: true,
                query_id: queryId,
                intent: nlpResult.intent,
                confidence: nlpResult.confidence,
                answer: '',
                analysis_results: {
                    nlp_analysis: nlpResult,
                },
                recommendations: [],
                processing_stats: {
                    total_time: 0,
                    components_used: [],
                    models_executed: [],
                    data_sources: []
                },
                metadata: {
                    timestamp: new Date().toISOString(),
                    language: nlpResult.language,
                    session_id: request.context?.session_id
                }
            };

            // Intent ì²˜ë¦¬
            const intentResult = await intentHandlerManager.handleIntent(
                nlpResult,
                request,
                response
            );

            if (!intentResult.success) {
                yield this.createChunk('error', {
                    message: intentResult.error || 'Intent ì²˜ë¦¬ ì‹¤íŒ¨',
                    code: 'INTENT_PROCESSING_ERROR'
                });
                return;
            }

            // ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸
            response.processing_stats.components_used.push(...intentResult.components_used);
            response.processing_stats.models_executed.push(...intentResult.models_executed);

            yield this.createChunk('partial_result', {
                step: 'intent_complete',
                data: {
                    components_used: intentResult.components_used,
                    models_executed: intentResult.models_executed
                },
                progress: 80
            });

            // 4. ë‹µë³€ ìƒì„± ì²­í¬
            yield this.createChunk('progress', {
                step: 'response_generation',
                message: 'ë‹µë³€ ìƒì„± ì¤‘...',
                progress: 90
            });

            // ë‹µë³€ ìƒì„±
            const generatedAnswer = responseGeneratorManager.generateResponse(
                nlpResult.intent as IntentType,
                response,
                {
                    language: nlpResult.language as 'ko' | 'en',
                    include_details: true,
                    include_recommendations: true,
                    format: 'detailed'
                }
            );

            response.answer = generatedAnswer;

            // ê¶Œì¥ì‚¬í•­ ìƒì„±
            this.generateRecommendations(nlpResult, response);

            // ìµœì¢… í†µê³„ ê³„ì‚°
            response.processing_stats.total_time = performance.now() - startTime;

            // 5. ìµœì¢… ê²°ê³¼ ì²­í¬
            yield this.createChunk('final_result', {
                step: 'complete',
                data: response,
                progress: 100
            });

        } catch (error: any) {
            console.error('ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:', error);

            yield this.createChunk('error', {
                message: error.message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                code: 'STREAMING_ERROR',
                stack: error.stack
            });
        }
    }

    /**
     * ì •ì  ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë³€í™˜
     */
    async *createStreamingResponse(
        response: AIQueryResponse,
        chunkSize: number = 100
    ): AsyncGenerator<StreamingChunk, void, unknown> {
        try {
            const answer = response.answer;
            const chunks = this.splitIntoChunks(answer, chunkSize);

            for (let i = 0; i < chunks.length; i++) {
                const isLast = i === chunks.length - 1;
                const progress = Math.round(((i + 1) / chunks.length) * 100);

                yield this.createChunk(isLast ? 'final_result' : 'partial_result', {
                    chunk_index: i,
                    chunk_total: chunks.length,
                    content: chunks[i],
                    progress,
                    is_complete: isLast,
                    metadata: isLast ? response.metadata : undefined
                });

                // ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ ìœ„í•œ ì§€ì—°
                await this.delay(50);
            }

        } catch (error: any) {
            yield this.createChunk('error', {
                message: error.message || 'ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì‹¤íŒ¨',
                code: 'STREAMING_RESPONSE_ERROR'
            });
        }
    }

    /**
     * ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìƒì„±
     */
    private createChunk(
        type: StreamingChunk['type'],
        data: any
    ): StreamingChunk {
        return {
            type,
            data,
            timestamp: Date.now(),
            chunk_id: `chunk_${++this.chunkCounter}_${Date.now()}`
        };
    }

    /**
     * í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
     */
    private splitIntoChunks(text: string, chunkSize: number): string[] {
        const chunks: string[] = [];
        let currentChunk = '';
        const words = text.split(' ');

        for (const word of words) {
            if (currentChunk.length + word.length + 1 <= chunkSize) {
                currentChunk += (currentChunk ? ' ' : '') + word;
            } else {
                if (currentChunk) {
                    chunks.push(currentChunk);
                    currentChunk = word;
                } else {
                    // ë‹¨ì–´ ìì²´ê°€ ì²­í¬ í¬ê¸°ë³´ë‹¤ í° ê²½ìš°
                    chunks.push(word);
                }
            }
        }

        if (currentChunk) {
            chunks.push(currentChunk);
        }

        return chunks;
    }

    /**
     * ê¶Œì¥ì‚¬í•­ ìƒì„±
     */
    private generateRecommendations(nlpResult: any, response: AIQueryResponse): void {
        const recommendations: string[] = [];
        const lang = nlpResult.language;

        // Intentë³„ ê¶Œì¥ì‚¬í•­
        switch (nlpResult.intent) {
            case 'troubleshooting':
                if (lang === 'ko') {
                    recommendations.push('ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ ì •ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”.');
                    recommendations.push('ëª¨ë‹ˆí„°ë§ ì•Œë¦¼ì„ ì„¤ì •í•˜ì—¬ ì¡°ê¸° íƒì§€í•˜ì„¸ìš”.');
                    recommendations.push('ë°±ì—… ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.');
                } else {
                    recommendations.push('Check system logs regularly.');
                    recommendations.push('Set up monitoring alerts for early detection.');
                    recommendations.push('Verify backup systems are functioning properly.');
                }
                break;

            case 'prediction':
                if (lang === 'ko') {
                    recommendations.push('ì˜ˆì¸¡ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ê²€ì¦í•˜ì„¸ìš”.');
                    recommendations.push('ì„ê³„ê°’ ì„¤ì •ì„ í˜„ì¬ ì‹œìŠ¤í…œì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”.');
                    recommendations.push('ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆë°© ì¡°ì¹˜ë¥¼ ìˆ˜ë¦½í•˜ì„¸ìš”.');
                } else {
                    recommendations.push('Regularly validate prediction model accuracy.');
                    recommendations.push('Adjust threshold settings for current system.');
                    recommendations.push('Establish preventive measures based on predictions.');
                }
                break;

            case 'performance':
                if (lang === 'ko') {
                    recommendations.push('ì„±ëŠ¥ ê¸°ì¤€ì¹˜ë¥¼ ì •ì˜í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.');
                    recommendations.push('ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ê³  ìµœì í™”í•˜ì„¸ìš”.');
                    recommendations.push('ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ ì„ ì •ê¸°ì ìœ¼ë¡œ ê²€í† í•˜ì„¸ìš”.');
                } else {
                    recommendations.push('Define and monitor performance benchmarks.');
                    recommendations.push('Identify and optimize bottlenecks.');
                    recommendations.push('Review resource utilization regularly.');
                }
                break;

            default:
                if (lang === 'ko') {
                    recommendations.push('ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì •ê¸°ì ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”.');
                    recommendations.push('ë¬¸ì„œí™”ë¥¼ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€í•˜ì„¸ìš”.');
                } else {
                    recommendations.push('Check system status regularly.');
                    recommendations.push('Keep documentation up to date.');
                }
        }

        // ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        if (response.analysis_results.active_alerts?.length > 0) {
            if (lang === 'ko') {
                recommendations.push('í™œì„± ì•Œë¦¼ì— ëŒ€í•œ ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.');
            } else {
                recommendations.push('Immediate action required for active alerts.');
            }
        }

        response.recommendations = recommendations;
    }

    /**
     * ì§€ì—° í•¨ìˆ˜
     */
    private delay(ms: number): Promise<void> {
        return new Promise(resolve => setTimeout(resolve, ms));
    }

    /**
     * ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ í™•ì¸
     */
    isStreamingSupported(): boolean {
        return typeof AsyncGenerator !== 'undefined';
    }

    /**
     * ì²­í¬ ìœ íš¨ì„± ê²€ì‚¬
     */
    validateChunk(chunk: StreamingChunk): boolean {
        return !!(
            chunk.type &&
            chunk.data !== undefined &&
            chunk.timestamp &&
            chunk.chunk_id
        );
    }

    /**
     * ìŠ¤íŠ¸ë¦¬ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
     */
    collectStreamingMetrics(): {
        total_chunks_sent: number;
        last_chunk_timestamp: number;
        streaming_active: boolean;
    } {
        return {
            total_chunks_sent: this.chunkCounter,
            last_chunk_timestamp: Date.now(),
            streaming_active: true
        };
    }

    /**
     * ìŠ¤íŠ¸ë¦¬ë° ë¦¬ì…‹
     */
    reset(): void {
        this.chunkCounter = 0;
    }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìµìŠ¤í¬íŠ¸
export const streamingProcessor = StreamingProcessor.getInstance(); 