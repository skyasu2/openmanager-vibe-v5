/**
 * ğŸ¤– Integrated AI Engine v3.0
 * 
 * í†µí•© AI ì—”ì§„ ë©”ì¸ í´ë˜ìŠ¤
 * - ëª¨ë“  ëª¨ë“ˆ í†µí•© ê´€ë¦¬
 * - ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
 * - ì˜ì¡´ì„± ì£¼ì… ë° ì„¤ì • ê´€ë¦¬
 * - ìƒëª…ì£¼ê¸° ê´€ë¦¬
 */

import { autoReportGenerator } from '../report-generator';
import {
    SystemManager
} from './system/SystemManager';
import {
    aiEngineUtils
} from './utils/AIEngineUtils';
import {
    nlpProcessor
} from './nlp/NLPProcessor';
import {
    intentHandlerManager
} from './intents/IntentHandlerManager';
import {
    responseGeneratorManager
} from './generators/ResponseGeneratorManager';
import {
    streamingProcessor
} from './streaming/StreamingProcessor';
import {
    AIQueryRequest,
    AIQueryResponse,
    AIEngineConfig,
    AIEngineStatus,
    StreamingChunk,
    IntentType,
    ReportConfig
} from './types/AIEngineTypes';

export class IntegratedAIEngine {
    private systemManager: SystemManager;
    private lastAnalysisCache: Map<string, any> = new Map();

    constructor(config?: Partial<AIEngineConfig>) {
        this.systemManager = new SystemManager(config);
        console.log('ğŸ¤– í†µí•© AI ì—”ì§„ v3.0 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±');
    }

    /**
     * ì—”ì§„ ì´ˆê¸°í™”
     */
    async initialize(): Promise<void> {
        console.log('ğŸš€ í†µí•© AI ì—”ì§„ v3.0 ì´ˆê¸°í™” ì‹œì‘...');

        try {
            await this.systemManager.initialize();
            console.log('âœ… í†µí•© AI ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ');
        } catch (error: any) {
            console.error('âŒ í†µí•© AI ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
            throw error;
        }
    }

    /**
     * ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬
     */
    async processQuery(request: AIQueryRequest): Promise<AIQueryResponse> {
        const startTime = performance.now();
        const queryId = aiEngineUtils.generateQueryId();

        console.log(`ğŸ“ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: ${queryId}`);
        console.log(`ğŸ” ì¿¼ë¦¬: "${request.query}"`);

        try {
            // 1. NLP ì²˜ë¦¬
            const nlpResult = await nlpProcessor.processQuery(request.query);
            console.log(`ğŸ§  Intent ê°ì§€: ${nlpResult.intent} (ì‹ ë¢°ë„: ${(nlpResult.confidence * 100).toFixed(1)}%)`);

            // 2. ì´ˆê¸° ì‘ë‹µ êµ¬ì¡° ìƒì„±
            const response: AIQueryResponse = {
                success: true,
                query_id: queryId,
                intent: nlpResult.intent,
                confidence: nlpResult.confidence,
                answer: '',
                analysis_results: {
                    nlp_analysis: nlpResult,
                },
                recommendations: [],
                processing_stats: {
                    total_time: 0,
                    components_used: ['nlp-processor'],
                    models_executed: [],
                    data_sources: []
                },
                metadata: {
                    timestamp: new Date().toISOString(),
                    language: nlpResult.language,
                    session_id: request.context?.session_id
                }
            };

            // ì„¸ì…˜ ì¶”ì 
            if (request.context?.session_id) {
                this.systemManager.addSession(request.context.session_id);
            }

            // 3. Intentë³„ ì²˜ë¦¬
            const intentResult = await intentHandlerManager.handleIntent(
                nlpResult,
                request,
                response
            );

            if (!intentResult.success) {
                throw new Error(intentResult.error || 'Intent ì²˜ë¦¬ ì‹¤íŒ¨');
            }

            // ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸
            response.processing_stats.components_used.push(...intentResult.components_used);
            response.processing_stats.models_executed.push(...intentResult.models_executed);

            // 4. ë‹µë³€ ìƒì„±
            await this.generateComprehensiveAnswer(nlpResult, request, response);

            // 5. ë³´ê³ ì„œ ìƒì„± (í•„ìš”ì‹œ)
            if (this.shouldGenerateReport(nlpResult, request)) {
                await this.generateReport(response, request);
            }

            // 6. ìµœì¢… í†µê³„ ê³„ì‚°
            response.processing_stats.total_time = performance.now() - startTime;

            console.log(`âœ… ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: ${queryId} (${response.processing_stats.total_time.toFixed(2)}ms)`);
            console.log(`ğŸ“Š ì‚¬ìš©ëœ ì»´í¬ë„ŒíŠ¸: ${response.processing_stats.components_used.join(', ')}`);

            // ìºì‹œ ì €ì¥
            this.lastAnalysisCache.set(queryId, {
                request,
                response,
                timestamp: Date.now()
            });

            return response;

        } catch (error: any) {
            console.error(`âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: ${queryId}`, error);

            return {
                success: false,
                query_id: queryId,
                intent: 'general',
                confidence: 0,
                answer: this.generateErrorAnswer(request.context?.language || 'ko', error.message),
                analysis_results: {
                    nlp_analysis: null,
                },
                recommendations: [],
                processing_stats: {
                    total_time: performance.now() - startTime,
                    components_used: [],
                    models_executed: [],
                    data_sources: []
                },
                metadata: {
                    timestamp: new Date().toISOString(),
                    language: request.context?.language || 'ko',
                    session_id: request.context?.session_id,
                    debug_info: {
                        error: error.message,
                        stack: error.stack
                    }
                }
            };
        }
    }

    /**
     * ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬
     */
    async *processQueryStream(
        request: AIQueryRequest
    ): AsyncGenerator<StreamingChunk, void, unknown> {
        console.log('ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘');

        try {
            yield* streamingProcessor.processQueryStream(request);
        } catch (error: any) {
            console.error('âŒ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹¤íŒ¨:', error);
            yield {
                type: 'error',
                data: {
                    message: error.message || 'ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                    code: 'STREAMING_ERROR'
                },
                timestamp: Date.now(),
                chunk_id: `error_${Date.now()}`
            };
        }
    }

    /**
     * ì¢…í•© ë‹µë³€ ìƒì„±
     */
    private async generateComprehensiveAnswer(
        nlpResult: any,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<void> {
        console.log('ğŸ“ ë‹µë³€ ìƒì„± ì‹œì‘...');

        try {
            // ë‹µë³€ ìƒì„±
            const generatedAnswer = responseGeneratorManager.generateResponse(
                nlpResult.intent as IntentType,
                response,
                {
                    language: nlpResult.language as 'ko' | 'en',
                    include_details: true,
                    include_recommendations: true,
                    format: 'detailed'
                }
            );

            response.answer = generatedAnswer;

            // ê¶Œì¥ì‚¬í•­ì´ ì•„ì§ ì—†ëŠ” ê²½ìš° ìƒì„±
            if (response.recommendations.length === 0) {
                this.generateRecommendations(nlpResult, response);
            }

            console.log('âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ');
        } catch (error: any) {
            console.error('âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨:', error);
            response.answer = this.generateErrorAnswer(
                nlpResult.language,
                'ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            );
        }
    }

    /**
     * ê¶Œì¥ì‚¬í•­ ìƒì„±
     */
    private generateRecommendations(nlpResult: any, response: AIQueryResponse): void {
        const recommendations: string[] = [];
        const lang = nlpResult.language;

        // Intentë³„ ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        switch (nlpResult.intent) {
            case 'troubleshooting':
                if (lang === 'ko') {
                    recommendations.push('ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.');
                    recommendations.push('ê´€ë ¨ ì„œë¹„ìŠ¤ì˜ ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”.');
                    recommendations.push('í•„ìš”ì‹œ ì¬ì‹œì‘ì„ ê³ ë ¤í•˜ì„¸ìš”.');
                } else {
                    recommendations.push('Check system logs to identify root cause.');
                    recommendations.push('Verify the status of related services.');
                    recommendations.push('Consider restart if necessary.');
                }
                break;

            case 'prediction':
                if (lang === 'ko') {
                    recommendations.push('ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆë°© ì¡°ì¹˜ë¥¼ ê³„íší•˜ì„¸ìš”.');
                    recommendations.push('ì„ê³„ê°’ì„ ì¬ê²€í† í•˜ê³  í•„ìš”ì‹œ ì¡°ì •í•˜ì„¸ìš”.');
                } else {
                    recommendations.push('Plan preventive actions based on predictions.');
                    recommendations.push('Review and adjust thresholds if needed.');
                }
                break;

            case 'performance':
                if (lang === 'ko') {
                    recommendations.push('ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”.');
                    recommendations.push('ë³‘ëª© ì§€ì ì„ ì‹ë³„í•˜ê³  ìµœì í™”í•˜ì„¸ìš”.');
                } else {
                    recommendations.push('Monitor resource utilization.');
                    recommendations.push('Identify and optimize bottlenecks.');
                }
                break;

            default:
                if (lang === 'ko') {
                    recommendations.push('ì •ê¸°ì ì¸ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤.');
                } else {
                    recommendations.push('Regular system health checks are recommended.');
                }
        }

        response.recommendations = recommendations;
    }

    /**
     * ë³´ê³ ì„œ ìƒì„± í•„ìš” ì—¬ë¶€ í™•ì¸
     */
    private shouldGenerateReport(nlpResult: any, request: AIQueryRequest): boolean {
        return (
            request.context?.include_charts === true ||
            nlpResult.intent === 'reporting' ||
            (nlpResult.intent === 'analysis' && nlpResult.confidence > 0.8)
        );
    }

    /**
     * ë³´ê³ ì„œ ìƒì„±
     */
    private async generateReport(
        response: AIQueryResponse,
        request: AIQueryRequest
    ): Promise<void> {
        try {
            console.log('ğŸ“„ ë³´ê³ ì„œ ìƒì„± ì¤‘...');

            const reportData = {
                timestamp: new Date().toISOString(),
                summary: response.metadata.language === 'ko'
                    ? 'ì‹œìŠ¤í…œ ë¶„ì„ ë³´ê³ ì„œì…ë‹ˆë‹¤.'
                    : 'System analysis report.',
                failure_analysis: response.analysis_results.anomaly_detection || {},
                prediction_results: response.analysis_results.ai_predictions || {},
                ai_insights: response.recommendations,
                recommendations: response.recommendations,
                metrics_data: {},
                charts: [],
                system_status: { overall: 'healthy' },
                time_range: {
                    start: new Date(Date.now() - 24 * 60 * 60 * 1000).toISOString(),
                    end: new Date().toISOString(),
                    duration: '24h',
                },
            };

            const reportConfig: ReportConfig = {
                format: 'markdown',
                include_charts: request.context?.include_charts || false,
                include_raw_data: false,
                template: 'technical',
                language: response.metadata.language as 'ko' | 'en',
                time_range: reportData.time_range
            };

            const generatedReport = await autoReportGenerator.generateReport(
                reportData,
                reportConfig
            );

            if (generatedReport.success && generatedReport.report) {
                response.generated_report = generatedReport.report;
                response.processing_stats.components_used.push('report-generator');
                console.log('âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ');
            }
        } catch (error: any) {
            console.error('âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨:', error);
        }
    }

    /**
     * ì—ëŸ¬ ë‹µë³€ ìƒì„±
     */
    private generateErrorAnswer(language: string, error: string): string {
        if (language === 'ko') {
            return `ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error}`;
        } else {
            return `Sorry, an error occurred during processing: ${error}`;
        }
    }

    /**
     * ì—”ì§„ ìƒíƒœ ì¡°íšŒ
     */
    async getEngineStatus(): Promise<AIEngineStatus> {
        return await this.systemManager.getEngineStatus();
    }

    /**
     * Render ìƒíƒœ ì¡°íšŒ
     */
    getRenderStatus(): 'active' | 'sleeping' | 'error' {
        return this.systemManager.getRenderStatus();
    }

    /**
     * ìºì‹œëœ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
     */
    getCachedAnalysis(queryId?: string): any[] {
        if (queryId) {
            const cached = this.lastAnalysisCache.get(queryId);
            return cached ? [cached] : [];
        }

        return Array.from(this.lastAnalysisCache.values());
    }

    /**
     * ìºì‹œ ì •ë¦¬
     */
    clearCache(): void {
        this.lastAnalysisCache.clear();
        console.log('ğŸ§¹ ë¶„ì„ ìºì‹œ ì •ë¦¬ ì™„ë£Œ');
    }

    /**
     * ì„¤ì • ì—…ë°ì´íŠ¸
     */
    updateConfig(config: Partial<AIEngineConfig>): void {
        this.systemManager.updateConfig(config);
    }

    /**
     * í˜„ì¬ ì„¤ì • ì¡°íšŒ
     */
    getConfig(): AIEngineConfig {
        return this.systemManager.getConfig();
    }

    /**
     * ìƒíƒœ ì²´í¬
     */
    async healthCheck(): Promise<any> {
        return await this.systemManager.healthCheck();
    }

    /**
     * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
     */
    dispose(): void {
        console.log('ğŸ§¹ í†µí•© AI ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...');

        this.systemManager.dispose();
        this.clearCache();
        streamingProcessor.reset();

        console.log('âœ… í†µí•© AI ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ');
    }

    /**
     * ì¬ì‹œì‘
     */
    async restart(): Promise<void> {
        console.log('ğŸ”„ í†µí•© AI ì—”ì§„ ì¬ì‹œì‘ ì¤‘...');

        this.dispose();
        await this.systemManager.restart();

        console.log('âœ… í†µí•© AI ì—”ì§„ ì¬ì‹œì‘ ì™„ë£Œ');
    }
}

// ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ìµìŠ¤í¬íŠ¸
export const integratedAIEngine = new IntegratedAIEngine(); 