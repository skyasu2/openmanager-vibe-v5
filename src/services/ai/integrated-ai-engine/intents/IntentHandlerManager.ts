/**
 * ğŸ¯ Intent Handler Manager
 * 
 * Intent í•¸ë“¤ëŸ¬ í†µí•© ê´€ë¦¬
 * - 7ê°€ì§€ Intent íƒ€ì… ì²˜ë¦¬
 * - í•¸ë“¤ëŸ¬ ë“±ë¡ ë° ì‹¤í–‰
 * - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
 * - ì—ëŸ¬ ì²˜ë¦¬
 */

import { realMCPClient } from '../../../mcp/real-mcp-client';
import { tensorFlowAIEngine } from '../../tensorflow-engine';
import { aiEngineUtils } from '../utils/AIEngineUtils';
import {
    IntentHandler,
    IntentHandlerResult,
    NLPResult,
    AIQueryRequest,
    AIQueryResponse,
    IntentType,
    SystemMetrics
} from '../types/AIEngineTypes';

export class IntentHandlerManager {
    private static instance: IntentHandlerManager;
    private handlers = new Map<IntentType, IntentHandler>();

    /**
     * ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ
     */
    static getInstance(): IntentHandlerManager {
        if (!IntentHandlerManager.instance) {
            IntentHandlerManager.instance = new IntentHandlerManager();
            IntentHandlerManager.instance.initializeHandlers();
        }
        return IntentHandlerManager.instance;
    }

    /**
     * í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
     */
    private initializeHandlers(): void {
        this.registerHandler('troubleshooting', new TroubleshootingHandler());
        this.registerHandler('prediction', new PredictionHandler());
        this.registerHandler('analysis', new AnalysisHandler());
        this.registerHandler('monitoring', new MonitoringHandler());
        this.registerHandler('reporting', new ReportingHandler());
        this.registerHandler('performance', new PerformanceHandler());
        this.registerHandler('general', new GeneralHandler());
    }

    /**
     * í•¸ë“¤ëŸ¬ ë“±ë¡
     */
    registerHandler(intent: IntentType, handler: IntentHandler): void {
        this.handlers.set(intent, handler);
    }

    /**
     * Intent ì²˜ë¦¬
     */
    async handleIntent(
        nlpResult: NLPResult,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<IntentHandlerResult> {
        const intent = nlpResult.intent as IntentType;
        const handler = this.handlers.get(intent) || this.handlers.get('general');

        if (!handler) {
            return {
                success: false,
                error: `No handler found for intent: ${intent}`,
                processing_time: 0,
                components_used: [],
                models_executed: []
            };
        }

        const startTime = performance.now();

        try {
            const result = await handler.handle(nlpResult, request, response);
            result.processing_time = performance.now() - startTime;
            return result;
        } catch (error: any) {
            return {
                success: false,
                error: error.message || 'Unknown error',
                processing_time: performance.now() - startTime,
                components_used: [],
                models_executed: []
            };
        }
    }

    /**
     * ë“±ë¡ëœ í•¸ë“¤ëŸ¬ ëª©ë¡ ì¡°íšŒ
     */
    getRegisteredHandlers(): IntentType[] {
        return Array.from(this.handlers.keys());
    }
}

/**
 * ë¬¸ì œ í•´ê²° í•¸ë“¤ëŸ¬
 */
class TroubleshootingHandler implements IntentHandler {
    canHandle(intent: IntentType): boolean {
        return intent === 'troubleshooting';
    }

    async handle(
        nlpResult: NLPResult,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<IntentHandlerResult> {
        console.log('ğŸ”§ ë¬¸ì œ í•´ê²° ëª¨ë“œ ì‹¤í–‰');

        try {
            // ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            const systemMetrics = await aiEngineUtils.collectSystemMetrics(
                request.context?.server_ids
            );

            // AI ë¶„ì„ ìˆ˜í–‰ (ë©”íŠ¸ë¦­ì´ ìˆëŠ” ê²½ìš°)
            if (systemMetrics.servers && Object.keys(systemMetrics.servers).length > 0) {
                const flattenedMetrics = this.flattenMetrics(systemMetrics.servers);
                const aiAnalysis = await tensorFlowAIEngine.analyzeMetricsWithAI(flattenedMetrics);

                response.analysis_results.ai_predictions = aiAnalysis.failure_predictions;
                response.analysis_results.anomaly_detection = aiAnalysis.anomaly_detections;
            }

            return {
                success: true,
                data: systemMetrics,
                processing_time: 0,
                components_used: ['mcp-client', 'tensorflow-engine'],
                models_executed: ['failure-prediction', 'anomaly-detection']
            };
        } catch (error: any) {
            return {
                success: false,
                error: error.message,
                processing_time: 0,
                components_used: [],
                models_executed: []
            };
        }
    }

    private flattenMetrics(servers: Record<string, Record<string, number[]>>): Record<string, number[]> {
        const flattened: Record<string, number[]> = {};
        for (const [serverId, serverMetrics] of Object.entries(servers)) {
            for (const [metricName, values] of Object.entries(serverMetrics)) {
                flattened[`${serverId}_${metricName}`] = values;
            }
        }
        return flattened;
    }
}

/**
 * ì˜ˆì¸¡ í•¸ë“¤ëŸ¬
 */
class PredictionHandler implements IntentHandler {
    canHandle(intent: IntentType): boolean {
        return intent === 'prediction';
    }

    async handle(
        nlpResult: NLPResult,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<IntentHandlerResult> {
        console.log('ğŸ”® ì˜ˆì¸¡ ëª¨ë“œ ì‹¤í–‰');

        try {
            const systemMetrics = await aiEngineUtils.collectSystemMetrics(
                request.context?.server_ids
            );

            if (systemMetrics.servers && Object.keys(systemMetrics.servers).length > 0) {
                const flattenedMetrics = this.flattenMetrics(systemMetrics.servers);
                const aiAnalysis = await tensorFlowAIEngine.analyzeMetricsWithAI(flattenedMetrics);

                response.analysis_results.ai_predictions = aiAnalysis.failure_predictions;
                response.analysis_results.trend_forecasts = aiAnalysis.trend_predictions;

                // ë†’ì€ ì‹ ë¢°ë„ ì˜ˆì¸¡ì— ëŒ€í•œ ì•Œë¦¼ ìƒì„±
                for (const [metric, prediction] of Object.entries(aiAnalysis.failure_predictions)) {
                    if (prediction.confidence > 0.8 && prediction.prediction[0] > 0.6) {
                        systemMetrics.alerts.push({
                            type: 'prediction',
                            severity: 'high',
                            metric: metric,
                            message: `ë†’ì€ í™•ë¥ ì˜ ì¥ì•  ì˜ˆì¸¡: ${(prediction.prediction[0] * 100).toFixed(1)}%`,
                            confidence: prediction.confidence,
                        });
                    }
                }
            }

            response.analysis_results.active_alerts = systemMetrics.alerts;

            return {
                success: true,
                data: systemMetrics,
                processing_time: 0,
                components_used: ['tensorflow-engine', 'prediction-model'],
                models_executed: ['failure-prediction', 'trend-prediction']
            };
        } catch (error: any) {
            return {
                success: false,
                error: error.message,
                processing_time: 0,
                components_used: [],
                models_executed: []
            };
        }
    }

    private flattenMetrics(servers: Record<string, Record<string, number[]>>): Record<string, number[]> {
        const flattened: Record<string, number[]> = {};
        for (const [serverId, serverMetrics] of Object.entries(servers)) {
            for (const [metricName, values] of Object.entries(serverMetrics)) {
                flattened[`${serverId}_${metricName}`] = values;
            }
        }
        return flattened;
    }
}

/**
 * ë¶„ì„ í•¸ë“¤ëŸ¬
 */
class AnalysisHandler implements IntentHandler {
    canHandle(intent: IntentType): boolean {
        return intent === 'analysis';
    }

    async handle(
        nlpResult: NLPResult,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<IntentHandlerResult> {
        console.log('ğŸ“Š ë¶„ì„ ëª¨ë“œ ì‹¤í–‰');

        try {
            const systemMetrics = await aiEngineUtils.collectSystemMetrics(
                request.context?.server_ids
            );

            if (systemMetrics.servers && Object.keys(systemMetrics.servers).length > 0) {
                const flattenedMetrics = this.flattenMetrics(systemMetrics.servers);
                const aiAnalysis = await tensorFlowAIEngine.analyzeMetricsWithAI(flattenedMetrics);

                response.analysis_results.ai_predictions = aiAnalysis.failure_predictions;
                response.analysis_results.anomaly_detection = aiAnalysis.anomaly_detections;
            }

            // MCPë¥¼ í†µí•œ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            if (request.context?.session_id) {
                const sessionContext = await realMCPClient.retrieveContext(
                    request.context.session_id
                );
                response.analysis_results.session_context = sessionContext;
            }

            return {
                success: true,
                data: systemMetrics,
                processing_time: 0,
                components_used: ['tensorflow-engine', 'mcp-client'],
                models_executed: ['failure-prediction', 'anomaly-detection']
            };
        } catch (error: any) {
            return {
                success: false,
                error: error.message,
                processing_time: 0,
                components_used: [],
                models_executed: []
            };
        }
    }

    private flattenMetrics(servers: Record<string, Record<string, number[]>>): Record<string, number[]> {
        const flattened: Record<string, number[]> = {};
        for (const [serverId, serverMetrics] of Object.entries(servers)) {
            for (const [metricName, values] of Object.entries(serverMetrics)) {
                flattened[`${serverId}_${metricName}`] = values;
            }
        }
        return flattened;
    }
}

/**
 * ëª¨ë‹ˆí„°ë§ í•¸ë“¤ëŸ¬
 */
class MonitoringHandler implements IntentHandler {
    canHandle(intent: IntentType): boolean {
        return intent === 'monitoring';
    }

    async handle(
        nlpResult: NLPResult,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<IntentHandlerResult> {
        console.log('ğŸ“Š ëª¨ë‹ˆí„°ë§ ëª¨ë“œ ì‹¤í–‰');

        try {
            const systemMetrics = await aiEngineUtils.collectSystemMetrics(
                request.context?.server_ids
            );

            // ì‹¤ì‹œê°„ ìƒíƒœ ë¶„ì„
            if (systemMetrics.alerts && systemMetrics.alerts.length > 0) {
                response.analysis_results.active_alerts = systemMetrics.alerts;
            }

            return {
                success: true,
                data: systemMetrics,
                processing_time: 0,
                components_used: ['system_metrics', 'real_time_monitoring'],
                models_executed: []
            };
        } catch (error: any) {
            return {
                success: false,
                error: error.message,
                processing_time: 0,
                components_used: [],
                models_executed: []
            };
        }
    }
}

/**
 * ë³´ê³ ì„œ í•¸ë“¤ëŸ¬
 */
class ReportingHandler implements IntentHandler {
    canHandle(intent: IntentType): boolean {
        return intent === 'reporting';
    }

    async handle(
        nlpResult: NLPResult,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<IntentHandlerResult> {
        console.log('ğŸ“„ ë³´ê³ ì„œ ìƒì„± ëª¨ë“œ ì‹¤í–‰');

        try {
            const systemMetrics = await aiEngineUtils.collectSystemMetrics(
                request.context?.server_ids
            );

            // ë³´ê³ ì„œ ìƒì„± í”Œë˜ê·¸ ì„¤ì •
            response.metadata.debug_info = {
                ...response.metadata.debug_info,
                should_generate_report: true,
                report_type: 'system_analysis'
            };

            return {
                success: true,
                data: systemMetrics,
                processing_time: 0,
                components_used: ['report-generator', 'system_metrics'],
                models_executed: []
            };
        } catch (error: any) {
            return {
                success: false,
                error: error.message,
                processing_time: 0,
                components_used: [],
                models_executed: []
            };
        }
    }
}

/**
 * ì„±ëŠ¥ í•¸ë“¤ëŸ¬
 */
class PerformanceHandler implements IntentHandler {
    canHandle(intent: IntentType): boolean {
        return intent === 'performance';
    }

    async handle(
        nlpResult: NLPResult,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<IntentHandlerResult> {
        console.log('âš¡ ì„±ëŠ¥ ë¶„ì„ ëª¨ë“œ ì‹¤í–‰');

        try {
            const systemMetrics = await aiEngineUtils.collectSystemMetrics(
                request.context?.server_ids
            );

            // ì„±ëŠ¥ ê´€ë ¨ ë©”íŠ¸ë¦­ ê°•ì¡°
            response.analysis_results.performance_metrics = {
                response_times: systemMetrics.servers,
                throughput: systemMetrics.global_stats,
                resource_utilization: systemMetrics.servers
            };

            return {
                success: true,
                data: systemMetrics,
                processing_time: 0,
                components_used: ['performance-analyzer', 'system_metrics'],
                models_executed: ['performance-prediction']
            };
        } catch (error: any) {
            return {
                success: false,
                error: error.message,
                processing_time: 0,
                components_used: [],
                models_executed: []
            };
        }
    }
}

/**
 * ì¼ë°˜ í•¸ë“¤ëŸ¬
 */
class GeneralHandler implements IntentHandler {
    canHandle(intent: IntentType): boolean {
        return intent === 'general';
    }

    async handle(
        nlpResult: NLPResult,
        request: AIQueryRequest,
        response: AIQueryResponse
    ): Promise<IntentHandlerResult> {
        console.log('ğŸ’¬ ì¼ë°˜ ì§ˆì˜ ëª¨ë“œ ì‹¤í–‰');

        try {
            // í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
            const searchResults = await aiEngineUtils.searchDocumentsByKeywords(
                nlpResult.keywords
            );

            response.analysis_results.document_results = searchResults;

            // ê¸°ë³¸ ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
            const systemMetrics = await aiEngineUtils.collectSystemMetrics();
            response.analysis_results.system_overview = {
                server_count: Object.keys(systemMetrics.servers).length,
                alert_count: systemMetrics.alerts.length,
                last_update: systemMetrics.timestamp
            };

            return {
                success: true,
                data: { searchResults, systemMetrics },
                processing_time: 0,
                components_used: ['document-search', 'system_metrics'],
                models_executed: []
            };
        } catch (error: any) {
            return {
                success: false,
                error: error.message,
                processing_time: 0,
                components_used: [],
                models_executed: []
            };
        }
    }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìµìŠ¤í¬íŠ¸
export const intentHandlerManager = IntentHandlerManager.getInstance(); 