/**
 * ğŸ¤– Google AI Studio (Gemini) ì„œë¹„ìŠ¤
 *
 * âœ… Gemini 1.5 Flash/Pro ì§€ì›
 * âœ… ì„œë²„ ëª¨ë‹ˆí„°ë§ íŠ¹í™” ë¶„ì„
 * âœ… ë² íƒ€ ëª¨ë“œ ì „ìš© ê³ ê¸‰ ê¸°ëŠ¥
 * âœ… ë¬´ë£Œ í• ë‹¹ëŸ‰ ìµœì í™”
 * âœ… í´ë°± ì‹œìŠ¤í…œ ë‚´ì¥
 * âœ… ë³´ì•ˆ ê°•í™”ëœ API í‚¤ ê´€ë¦¬
 */

import { getGoogleAIKey, isGoogleAIAvailable } from '@/lib/google-ai-manager';

interface GoogleAIConfig {
  apiKey: string;
  model: 'gemini-1.5-flash' | 'gemini-1.5-pro';
  enabled: boolean;
  rateLimits: {
    rpm: number;
    daily: number;
  };
}

interface GoogleAIResponse {
  success: boolean;
  content: string;
  model: string;
  tokensUsed?: number;
  cached?: boolean;
  processingTime: number;
  confidence: number;
}

interface ServerMetrics {
  name: string;
  cpu_usage: number;
  memory_usage: number;
  disk_usage: number;
  response_time: number;
  status: string;
  timestamp: string;
}

interface AdvancedAnalysisRequest {
  query: string;
  serverMetrics?: ServerMetrics[];
  context?: any;
  analysisType:
    | 'monitoring'
    | 'prediction'
    | 'troubleshooting'
    | 'optimization';
  priority: 'low' | 'medium' | 'high' | 'critical';
}

export class GoogleAIService {
  private config: GoogleAIConfig;
  private baseUrl = 'https://generativelanguage.googleapis.com/v1beta';
  private requestCache = new Map<
    string,
    { response: string; timestamp: number }
  >();
  private requestCount = { minute: 0, day: 0, lastReset: Date.now() };
  private isInitialized = false;

  constructor() {
    // ğŸ” ë³´ì•ˆ ê°•í™”ëœ API í‚¤ ê´€ë¦¬ ì‚¬ìš©
    const apiKey = getGoogleAIKey();

    this.config = {
      apiKey: apiKey || '',
      model: (process.env.GOOGLE_AI_MODEL as any) || 'gemini-1.5-flash',
      enabled:
        process.env.GOOGLE_AI_ENABLED === 'true' && isGoogleAIAvailable(),
      rateLimits: {
        rpm: this.getRateLimit('rpm'),
        daily: this.getRateLimit('daily'),
      },
    };
  }

  /**
   * ğŸ”§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
   */
  async initialize(): Promise<boolean> {
    if (this.isInitialized) return true;

    try {
      if (!this.config.enabled || !this.config.apiKey) {
        console.log('ğŸ¤– Google AI ë² íƒ€ ëª¨ë“œ: ë¹„í™œì„±í™”ë¨');
        return false;
      }

      // ì—°ê²° í…ŒìŠ¤íŠ¸
      const testResponse = await this.generateContent(
        'Hello, this is a connection test.',
        {
          skipCache: true,
          timeout: 5000,
        }
      );

      if (testResponse.success) {
        this.isInitialized = true;
        console.log('âœ… Google AI Studio ë² íƒ€ ëª¨ë“œ ì´ˆê¸°í™” ì™„ë£Œ');
        console.log(`ğŸ¯ ëª¨ë¸: ${this.config.model}`);
        console.log(
          `âš¡ í• ë‹¹ëŸ‰: ${this.config.rateLimits.rpm}RPM, ${this.config.rateLimits.daily}/ì¼`
        );
        return true;
      }

      throw new Error('ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨');
    } catch (error) {
      console.warn('âš ï¸ Google AI ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      this.config.enabled = false;
      return false;
    }
  }

  /**
   * ğŸ§  ê³ ê¸‰ ì„œë²„ ëª¨ë‹ˆí„°ë§ ë¶„ì„ (ë² íƒ€ ê¸°ëŠ¥)
   */
  async analyzeAdvanced(
    request: AdvancedAnalysisRequest
  ): Promise<GoogleAIResponse> {
    if (!this.isAvailable()) {
      throw new Error('Google AI ë² íƒ€ ëª¨ë“œê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.');
    }

    const startTime = Date.now();

    try {
      const prompt = this.buildAdvancedPrompt(request);
      const cacheKey = this.generateCacheKey(prompt, request.analysisType);

      // ìºì‹œ í™•ì¸ (ê³ ê¸‰ ë¶„ì„ì€ 5ë¶„ ìºì‹œ)
      const cached = this.getCachedResponse(cacheKey, 300000);
      if (cached) {
        return {
          success: true,
          content: cached,
          model: this.config.model,
          cached: true,
          processingTime: Date.now() - startTime,
          confidence: 0.95,
        };
      }

      const response = await this.generateContent(prompt);

      if (response.success) {
        // ìºì‹œ ì €ì¥
        this.setCachedResponse(cacheKey, response.content);

        return {
          success: true,
          content: this.enhanceResponse(response.content, request),
          model: this.config.model,
          cached: false,
          processingTime: Date.now() - startTime,
          confidence: this.calculateConfidence(response.content, request),
        };
      }

      throw new Error('Google AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨');
    } catch (error) {
      console.error('âŒ Google AI ê³ ê¸‰ ë¶„ì„ ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ğŸ¯ ê¸°ë³¸ ì»¨í…ì¸  ìƒì„±
   */
  async generateContent(
    prompt: string,
    options: { skipCache?: boolean; timeout?: number } = {}
  ): Promise<GoogleAIResponse> {
    if (!this.isAvailable()) {
      throw new Error('Google AI ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }

    // ğŸ” ì‹¤ì‹œê°„ìœ¼ë¡œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    const currentApiKey = getGoogleAIKey();
    if (!currentApiKey) {
      throw new Error(
        'Google AI API í‚¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.'
      );
    }

    // í• ë‹¹ëŸ‰ í™•ì¸
    if (!this.checkRateLimit()) {
      throw new Error(
        'Google AI í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.'
      );
    }

    const startTime = Date.now();

    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(
        () => controller.abort(),
        options.timeout || 10000
      );

      const response = await fetch(
        `${this.baseUrl}/models/${this.config.model}:generateContent?key=${currentApiKey}`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          signal: controller.signal,
          body: JSON.stringify({
            contents: [
              {
                parts: [{ text: prompt }],
              },
            ],
            generationConfig: {
              temperature: 0.1,
              topK: 32,
              topP: 0.95,
              maxOutputTokens: 4096,
              stopSequences: [],
            },
            safetySettings: [
              {
                category: 'HARM_CATEGORY_HARASSMENT',
                threshold: 'BLOCK_MEDIUM_AND_ABOVE',
              },
              {
                category: 'HARM_CATEGORY_HATE_SPEECH',
                threshold: 'BLOCK_MEDIUM_AND_ABOVE',
              },
            ],
          }),
        }
      );

      clearTimeout(timeoutId);

      if (!response.ok) {
        const errorData = await response.json();
        throw new Error(
          `Google AI API Error: ${errorData.error?.message || response.statusText}`
        );
      }

      const data = await response.json();
      const content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';

      if (!content) {
        throw new Error('Google AIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.');
      }

      // ìš”ì²­ ì¹´ìš´íŠ¸ ì¦ê°€
      this.incrementRequestCount();

      return {
        success: true,
        content,
        model: this.config.model,
        tokensUsed: data.usageMetadata?.totalTokenCount,
        cached: false,
        processingTime: Date.now() - startTime,
        confidence: 0.95,
      };
    } catch (error: any) {
      console.error('âŒ Google AI ìš”ì²­ ì‹¤íŒ¨:', error);

      if (error.name === 'AbortError') {
        throw new Error('Google AI ìš”ì²­ ì‹œê°„ ì´ˆê³¼');
      }

      if (
        error.message.includes('quota') ||
        error.message.includes('rate limit')
      ) {
        throw new Error('Google AI í• ë‹¹ëŸ‰ ì´ˆê³¼. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      }

      throw error;
    }
  }

  /**
   * ğŸ¯ ì„œë²„ ë©”íŠ¸ë¦­ ë¶„ì„ (ê¸°ì¡´ í˜¸í™˜ì„±)
   */
  async analyzeServerMetrics(metrics: ServerMetrics[]): Promise<string> {
    const prompt = `
ì„œë²„ ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

${metrics
  .map(
    server => `
ì„œë²„: ${server.name}
CPU: ${server.cpu_usage}%
ë©”ëª¨ë¦¬: ${server.memory_usage}%
ë””ìŠ¤í¬: ${server.disk_usage}%
ì‘ë‹µì‹œê°„: ${server.response_time}ms
ìƒíƒœ: ${server.status}
`
  )
  .join('\n')}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½
2. ì£¼ì˜ê°€ í•„ìš”í•œ ì„œë²„ ì‹ë³„
3. ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­
4. ì˜ˆìƒë˜ëŠ” ë¬¸ì œì ê³¼ ëŒ€ì‘ë°©ì•ˆ

ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        `;

    const response = await this.generateContent(prompt);
    return response.content;
  }

  /**
   * ğŸš€ ê³ ê¸‰ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  private buildAdvancedPrompt(request: AdvancedAnalysisRequest): string {
    const basePrompt = `
ë‹¹ì‹ ì€ OpenManager ì„œë²„ ëª¨ë‹ˆí„°ë§ ì „ë¬¸ AIì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ${request.analysisType} ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

ìš°ì„ ìˆœìœ„: ${request.priority.toUpperCase()}
ë¶„ì„ ìœ í˜•: ${request.analysisType}
ì‚¬ìš©ì ì§ˆì˜: ${request.query}
`;

    let contextPrompt = '';

    if (request.serverMetrics && request.serverMetrics.length > 0) {
      contextPrompt += '\nğŸ“Š **ì„œë²„ ë©”íŠ¸ë¦­ ë°ì´í„°:**\n';
      request.serverMetrics.forEach((server, index) => {
        contextPrompt += `
${index + 1}. ì„œë²„: ${server.name}
   - CPU ì‚¬ìš©ë¥ : ${server.cpu_usage}%
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : ${server.memory_usage}%
   - ë””ìŠ¤í¬ ì‚¬ìš©ë¥ : ${server.disk_usage}%
   - í‰ê·  ì‘ë‹µì‹œê°„: ${server.response_time}ms
   - ìƒíƒœ: ${server.status}
   - ì¸¡ì •ì‹œê°„: ${server.timestamp}
`;
      });
    }

    const analysisGuideline = this.getAnalysisGuideline(
      request.analysisType,
      request.priority
    );

    return `${basePrompt}${contextPrompt}\n${analysisGuideline}

ì‘ë‹µ í˜•ì‹:
1. ğŸ¯ **í•µì‹¬ ìš”ì•½** (2-3ì¤„)
2. ğŸ“Š **ìƒì„¸ ë¶„ì„**
3. âš ï¸ **ì£¼ì˜ì‚¬í•­** (ìˆëŠ” ê²½ìš°)
4. ğŸ’¡ **ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­**
5. ğŸ”® **ì˜ˆì¸¡ ë° íŠ¸ë Œë“œ** (í•´ë‹¹í•˜ëŠ” ê²½ìš°)

ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.`;
  }

  /**
   * ğŸ“‹ ë¶„ì„ ìœ í˜•ë³„ ê°€ì´ë“œë¼ì¸
   */
  private getAnalysisGuideline(type: string, priority: string): string {
    const guidelines = {
      monitoring: `
ğŸ” **ëª¨ë‹ˆí„°ë§ ë¶„ì„ ì§€ì¹¨:**
- í˜„ì¬ ì‹œìŠ¤í…œ ì „ë°˜ì  ìƒíƒœ í‰ê°€
- ë¹„ì •ìƒì ì¸ ë©”íŠ¸ë¦­ íŒ¨í„´ ì‹ë³„
- ë¦¬ì†ŒìŠ¤ ì‚¬ìš© íš¨ìœ¨ì„± í‰ê°€
- ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„`,

      prediction: `
ğŸ”® **ì˜ˆì¸¡ ë¶„ì„ ì§€ì¹¨:**
- í–¥í›„ 1-24ì‹œê°„ íŠ¸ë Œë“œ ì˜ˆì¸¡
- ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ ì‹œì  ì˜ˆì¸¡
- ì ì¬ì  ì¥ì•  ë°œìƒ ê°€ëŠ¥ì„±
- ìš©ëŸ‰ ê³„íš ê¶Œì¥ì‚¬í•­`,

      troubleshooting: `
ğŸ”§ **ë¬¸ì œ í•´ê²° ë¶„ì„ ì§€ì¹¨:**
- ê·¼ë³¸ ì›ì¸ ë¶„ì„ (Root Cause Analysis)
- ì¦‰ì‹œ ì¡°ì¹˜ ê°€ëŠ¥í•œ í•´ê²°ë°©ì•ˆ
- ë‹¨ê³„ë³„ ë¬¸ì œ í•´ê²° ì ˆì°¨
- ì¬ë°œ ë°©ì§€ ëŒ€ì±…`,

      optimization: `
âš¡ **ìµœì í™” ë¶„ì„ ì§€ì¹¨:**
- ì„±ëŠ¥ ê°œì„  ê¸°íšŒ ì‹ë³„
- ë¦¬ì†ŒìŠ¤ í™œìš© ìµœì í™” ë°©ì•ˆ
- ë¹„ìš© íš¨ìœ¨ì„± ê°œì„  ì œì•ˆ
- ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê¶Œì¥ì‚¬í•­`,
    };

    let guideline = guidelines[type] || guidelines.monitoring;

    if (priority === 'critical') {
      guideline +=
        '\n\nğŸš¨ **ê¸´ê¸‰ ìƒí™©**: ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•œ ì‚¬í•­ì„ ìµœìš°ì„ ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.';
    }

    return guideline;
  }

  /**
   * âœ¨ ì‘ë‹µ í–¥ìƒ (ë² íƒ€ ê¸°ëŠ¥)
   */
  private enhanceResponse(
    content: string,
    request: AdvancedAnalysisRequest
  ): string {
    // ì‘ë‹µì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    const enhanced = `${content}

---
ğŸ¤– **AI ë¶„ì„ ì •ë³´**
- ì—”ì§„: Google AI Studio (Gemini ${this.config.model})
- ë¶„ì„ ìœ í˜•: ${request.analysisType}
- ìš°ì„ ìˆœìœ„: ${request.priority}
- ìƒì„± ì‹œê°„: ${new Date().toLocaleString('ko-KR')}
- ë² íƒ€ ê¸°ëŠ¥: ê³ ê¸‰ ë¶„ì„ í™œì„±í™” âœ¨`;

    return enhanced;
  }

  /**
   * ğŸ“Š ì‹ ë¢°ë„ ê³„ì‚°
   */
  private calculateConfidence(
    content: string,
    request: AdvancedAnalysisRequest
  ): number {
    let confidence = 0.8; // ê¸°ë³¸ ì‹ ë¢°ë„

    // ì‘ë‹µ ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
    if (content.length > 500) confidence += 0.1;
    if (content.length > 1000) confidence += 0.05;

    // ë¶„ì„ ìœ í˜•ë³„ ì¡°ì •
    if (request.analysisType === 'monitoring') confidence += 0.05;
    if (request.analysisType === 'prediction') confidence -= 0.1;

    // ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì¡°ì •
    if (request.serverMetrics && request.serverMetrics.length > 5)
      confidence += 0.05;

    return Math.min(0.98, Math.max(0.7, confidence));
  }

  /**
   * ğŸ”„ í• ë‹¹ëŸ‰ ê´€ë¦¬
   */
  private getRateLimit(type: 'rpm' | 'daily'): number {
    const limits = {
      'gemini-1.5-flash': { rpm: 15, daily: 1500 },
      'gemini-1.5-pro': { rpm: 2, daily: 50 },
    };

    return (
      limits[this.config.model]?.[type] || limits['gemini-1.5-flash'][type]
    );
  }

  private checkRateLimit(): boolean {
    const now = Date.now();

    // ë¶„ë‹¹ ë¦¬ì…‹
    if (now - this.requestCount.lastReset > 60000) {
      this.requestCount.minute = 0;
      this.requestCount.lastReset = now;
    }

    // ì¼ì¼ ë¦¬ì…‹
    if (now - this.requestCount.lastReset > 86400000) {
      this.requestCount.day = 0;
    }

    return (
      this.requestCount.minute < this.config.rateLimits.rpm &&
      this.requestCount.day < this.config.rateLimits.daily
    );
  }

  private incrementRequestCount(): void {
    this.requestCount.minute++;
    this.requestCount.day++;
  }

  /**
   * ğŸ’¾ ìºì‹œ ê´€ë¦¬
   */
  private generateCacheKey(prompt: string, type: string): string {
    const hash = this.simpleHash(prompt);
    return `${type}-${hash}`;
  }

  private getCachedResponse(key: string, maxAge: number): string | null {
    const cached = this.requestCache.get(key);
    if (cached && Date.now() - cached.timestamp < maxAge) {
      return cached.response;
    }
    return null;
  }

  private setCachedResponse(key: string, response: string): void {
    this.requestCache.set(key, {
      response,
      timestamp: Date.now(),
    });

    // ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
    if (this.requestCache.size > 100) {
      const oldestKey = this.requestCache.keys().next().value;
      this.requestCache.delete(oldestKey);
    }
  }

  private simpleHash(str: string): string {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = (hash << 5) - hash + char;
      hash = hash & hash; // 32bit integer ë³€í™˜
    }
    return Math.abs(hash).toString(36);
  }

  /**
   * ğŸ” ìƒíƒœ ì²´í¬
   */
  isAvailable(): boolean {
    // ğŸ” ì‹¤ì‹œê°„ìœ¼ë¡œ API í‚¤ ê°€ìš©ì„± í™•ì¸
    const currentApiKey = getGoogleAIKey();
    return (
      this.config.enabled &&
      currentApiKey &&
      this.isInitialized &&
      isGoogleAIAvailable()
    );
  }

  getStatus(): any {
    return {
      enabled: this.config.enabled,
      initialized: this.isInitialized,
      model: this.config.model,
      rateLimits: this.config.rateLimits,
      currentUsage: {
        minute: this.requestCount.minute,
        day: this.requestCount.day,
      },
      cacheSize: this.requestCache.size,
    };
  }

  /**
   * ğŸ§ª ì—°ê²° í…ŒìŠ¤íŠ¸
   */
  async testConnection(): Promise<{
    success: boolean;
    message: string;
    latency?: number;
  }> {
    if (!this.config.apiKey) {
      return { success: false, message: 'API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.' };
    }

    try {
      const startTime = Date.now();
      const response = await this.generateContent('Hello from OpenManager!', {
        skipCache: true,
        timeout: 5000,
      });
      const latency = Date.now() - startTime;

      if (response.success) {
        return {
          success: true,
          message: `ì—°ê²° ì„±ê³µ! (${latency}ms)`,
          latency,
        };
      }

      return { success: false, message: 'ì—°ê²° ì‹¤íŒ¨' };
    } catch (error: any) {
      return {
        success: false,
        message: `ì—°ê²° ì˜¤ë¥˜: ${error.message}`,
      };
    }
  }
}
