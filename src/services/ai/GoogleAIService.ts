/**
 * ğŸ¤– Google AI Service v5.44.3
 * Edge Runtime ì™„ì „ í˜¸í™˜ ë²„ì „
 */

import { getVercelConfig } from '@/config/vercel-edge-config';
import { edgeRuntimeService } from '@/lib/edge-runtime-utils';
import { AIRequest, AIResponse } from '@/types/ai-types';
import { GoogleGenerativeAI } from '@google/generative-ai';

// Edge Runtime ì„¤ì •
const vercelConfig = getVercelConfig();
const logger = edgeRuntimeService.logger;
const cache = edgeRuntimeService.cache;
const performance = edgeRuntimeService.performance;

interface GoogleAIConfig {
  apiKey: string;
  model: 'gemini-1.5-flash' | 'gemini-1.5-pro';
  enabled: boolean;
  rateLimits: {
    rpm: number;
    daily: number;
  };
}

export interface GoogleAIResponse {
  success: boolean;
  content: string;
  model: string;
  tokensUsed?: number;
  cached?: boolean;
  processingTime: number;
  confidence: number;
  error?: {
    code: string;
    message: string;
    details: string;
    timestamp: string;
    retryable: boolean;
  };
}

interface ServerMetrics {
  name: string;
  cpu_usage: number;
  memory_usage: number;
  disk_usage: number;
  response_time: number;
  status: string;
  timestamp: string;
}

/**
 * ğŸ¤– Google AI ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
 * GOOGLE_ONLY ëª¨ë“œ ì „ìš©, ìì—°ì–´ ì§ˆì˜ ì²˜ë¦¬
 */
export class GoogleAIService {
  private static instance: GoogleAIService | null = null;
  private genAI: GoogleGenerativeAI | null = null;
  private model: any = null;
  private initialized = false;
  private requestCount = 0;
  private lastError: string | null = null;

  constructor() {
    logger.info('ğŸ¤– Google AI Service ì´ˆê¸°í™” ì‹œì‘ - Edge Runtime ëª¨ë“œ');
  }

  /**
   * ğŸ¯ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
   */
  public static getInstance(): GoogleAIService {
    if (!GoogleAIService.instance) {
      GoogleAIService.instance = new GoogleAIService();
    }
    return GoogleAIService.instance;
  }

  /**
   * ğŸ”§ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
   */
  async initialize(): Promise<void> {
    const timer = performance.startTimer('google-ai-initialization');

    try {
      // Vercel í™˜ê²½ì—ì„œ Google AI ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
      if (!vercelConfig.enableGoogleAI) {
        throw new Error('Google AIëŠ” Pro í”Œëœì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤');
      }

      const apiKey = process.env.GOOGLE_AI_API_KEY;
      if (!apiKey) {
        throw new Error('GOOGLE_AI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
      }

      this.genAI = new GoogleGenerativeAI(apiKey);
      this.model = this.genAI.getGenerativeModel({
        model: process.env.GOOGLE_AI_MODEL || 'gemini-1.5-flash',
      });

      this.initialized = true;
      this.lastError = null;

      const duration = timer();
      logger.info(`âœ… Google AI Service ì´ˆê¸°í™” ì™„ë£Œ: ${duration.toFixed(2)}ms`);
    } catch (error) {
      const duration = timer();
      this.lastError = error instanceof Error ? error.message : 'Unknown error';
      logger.error(
        `âŒ Google AI Service ì´ˆê¸°í™” ì‹¤íŒ¨: ${duration.toFixed(2)}ms`,
        error
      );
      throw error;
    }
  }

  /**
   * ğŸ¯ AI ì¿¼ë¦¬ ì²˜ë¦¬ (ìì—°ì–´ ì§ˆì˜ ì „ìš©)
   */
  async processQuery(request: AIRequest): Promise<AIResponse> {
    const requestId = `google_${++this.requestCount}_${Date.now()}`;
    const timer = performance.startTimer('google-ai-query');

    logger.info(`ğŸ”„ Google AI ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: ${requestId}`, {
      query: request.query?.substring(0, 100),
      mode: request.mode,
    });

    try {
      if (!this.initialized || !this.model) {
        throw new Error('Google AI Serviceê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
      }

      // ìì—°ì–´ ì§ˆì˜ í™•ì¸
      if (!this.isNaturalLanguageQuery(request.query || '')) {
        throw new Error('Google AIëŠ” ìì—°ì–´ ì§ˆì˜ì—ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤');
      }

      // ìºì‹œ í™•ì¸
      const cacheKey = `google_ai_${this.generateQueryHash(request.query || '')}`;
      const cached = cache.get(cacheKey);
      if (cached) {
        logger.info(`ğŸ“‹ Google AI ìºì‹œ ì‘ë‹µ: ${requestId}`);
        return cached;
      }

      // Google AI ìš”ì²­
      const googleTimer = performance.startTimer('google-ai-api-call');

      const result = await Promise.race([
        this.model.generateContent(this.buildPrompt(request)),
        new Promise((_, reject) =>
          setTimeout(
            () => reject(new Error('Google AI API timeout')),
            vercelConfig.maxExecutionTime - 2000
          )
        ),
      ]);

      const googleDuration = googleTimer();

      if (!result.response) {
        throw new Error('Google AIì—ì„œ ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤');
      }

      const text = result.response.text();
      const confidence = this.calculateConfidence(text, request.query || '');

      const response: AIResponse = {
        success: true,
        response: text,
        confidence,
        mode: request.mode || 'GOOGLE_ONLY',
        enginePath: ['google-ai-service'],
        processingTime: googleDuration,
        fallbacksUsed: 0,
        metadata: {
          mainEngine: 'Google AI (Gemini)',
          supportEngines: ['Google AI (Gemini)'],
          ragUsed: false,
          googleAIUsed: true,
          mcpContextUsed: false,
          subEnginesUsed: [],
          cacheUsed: false,
          processingTime: googleDuration,
          enginePath: ['google-ai-service'],
        },
      };

      // ì„±ê³µí•œ ì‘ë‹µ ìºì‹œ ì €ì¥
      cache.set(cacheKey, response, vercelConfig.cacheTTL);

      const duration = timer();
      logger.info(
        `âœ… Google AI ì¿¼ë¦¬ ì™„ë£Œ: ${requestId} (${duration.toFixed(2)}ms)`
      );

      return response;
    } catch (error) {
      const duration = timer();
      this.lastError = error instanceof Error ? error.message : 'Unknown error';

      logger.error(
        `âŒ Google AI ì¿¼ë¦¬ ì‹¤íŒ¨: ${requestId} (${duration.toFixed(2)}ms)`,
        error
      );

      return {
        success: false,
        response: '',
        confidence: 0,
        mode: request.mode || 'GOOGLE_ONLY',
        enginePath: ['google-ai-error'],
        processingTime: duration,
        fallbacksUsed: 0,
        error: this.lastError,
        metadata: {
          mainEngine: 'Google AI (Error)',
          supportEngines: [],
          ragUsed: false,
          googleAIUsed: false,
          mcpContextUsed: false,
          subEnginesUsed: [],
          error: this.lastError,
        },
      };
    }
  }

  /**
   * ğŸ” ìì—°ì–´ ì§ˆì˜ íŒë³„
   */
  private isNaturalLanguageQuery(query: string): boolean {
    // í•œêµ­ì–´ ìì—°ì–´ íŒ¨í„´
    const koreanNaturalPatterns = [
      /^(ì–´ë–»ê²Œ|ì™œ|ë¬´ì—‡|ì–¸ì œ|ì–´ë””ì„œ|ëˆ„ê°€|ë¬´ì—‡ì„|ì–´ë–¤|ì–´ëŠ)/,
      /\?$/,
      /(ì„¤ëª…|ë¶„ì„|ìš”ì•½|ì •ë¦¬|ì•Œë ¤|ì¶”ì²œ|ë¹„êµ|ì°¨ì´|ë°©ë²•)/,
      /(í•  ìˆ˜ ìˆë‚˜ìš”|ê°€ëŠ¥í•œê°€ìš”|í•´ì£¼ì„¸ìš”|ë„ì™€ì£¼ì„¸ìš”)/,
      /(ë¬¸ì œ|í•´ê²°|ê°œì„ |ìµœì í™”|ì„±ëŠ¥|ìƒíƒœ)/,
    ];

    // ì˜ì–´ ìì—°ì–´ íŒ¨í„´
    const englishNaturalPatterns = [
      /^(how|why|what|when|where|who|which|can you|could you)/i,
      /\?$/,
      /(explain|analyze|summarize|tell me|recommend|compare|difference)/i,
      /(help|assist|show|provide|give)/i,
    ];

    const text = query.trim();
    return (
      koreanNaturalPatterns.some(pattern => pattern.test(text)) ||
      englishNaturalPatterns.some(pattern => pattern.test(text))
    );
  }

  /**
   * ğŸ“ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  private buildPrompt(request: AIRequest): string {
    const basePrompt = `
ë‹¹ì‹ ì€ ì„œë²„ ì‹œìŠ¤í…œ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: ${request.query}

ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
1. ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•œ ì •ë³´ ì œê³µ
2. ì‹¤ë¬´ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ì†”ë£¨ì…˜ ì œì‹œ
3. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…
4. í•„ìš”ì‹œ ë‹¨ê³„ë³„ ê°€ì´ë“œ ì œê³µ
5. ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì œí•œì‚¬í•­ë„ í•¨ê»˜ ì•ˆë‚´

ë‹µë³€:`;

    return basePrompt.trim();
  }

  /**
   * ğŸ“Š ì‹ ë¢°ë„ ê³„ì‚°
   */
  private calculateConfidence(response: string, query: string): number {
    let confidence = 0.7; // Google AI ê¸°ë³¸ ì‹ ë¢°ë„

    // ì‘ë‹µ ê¸¸ì´ í‰ê°€
    if (response.length > 50) confidence += 0.1;
    if (response.length > 200) confidence += 0.1;

    // ì „ë¬¸ ìš©ì–´ í¬í•¨ ì—¬ë¶€
    const technicalTerms = [
      'CPU',
      'ë©”ëª¨ë¦¬',
      'ë””ìŠ¤í¬',
      'ë„¤íŠ¸ì›Œí¬',
      'ì„œë²„',
      'ë°ì´í„°ë² ì´ìŠ¤',
      'API',
      'ë¡œë“œë°¸ëŸ°ì„œ',
      'ìºì‹œ',
      'ëª¨ë‹ˆí„°ë§',
      'ë¡œê·¸',
      'ë°±ì—…',
    ];

    const termsFound = technicalTerms.filter(term =>
      response.toLowerCase().includes(term.toLowerCase())
    ).length;

    confidence += Math.min(termsFound * 0.02, 0.1);

    // êµ¬ì²´ì ì¸ í•´ê²°ì±… ì œì‹œ ì—¬ë¶€
    const solutionPatterns = [
      /ë‹¨ê³„/,
      /ë°©ë²•/,
      /í•´ê²°/,
      /ì„¤ì¹˜/,
      /ì„¤ì •/,
      /êµ¬ì„±/,
      /ìµœì í™”/,
    ];

    const solutionsFound = solutionPatterns.filter(pattern =>
      pattern.test(response)
    ).length;

    confidence += Math.min(solutionsFound * 0.03, 0.1);

    return Math.min(confidence, 0.95);
  }

  /**
   * ğŸ”‘ ì¿¼ë¦¬ í•´ì‹œ ìƒì„±
   */
  private generateQueryHash(query: string): string {
    let hash = 0;
    for (let i = 0; i < query.length; i++) {
      const char = query.charCodeAt(i);
      hash = (hash << 5) - hash + char;
      hash = hash & hash; // 32bit integer ë³€í™˜
    }
    return Math.abs(hash).toString(36);
  }

  /**
   * ğŸ¯ ì‘ë‹µ ìƒì„± (UnifiedAIEngine í˜¸í™˜)
   */
  async generateResponse(prompt: string): Promise<GoogleAIResponse> {
    try {
      const request: AIRequest = {
        query: prompt,
        mode: 'GOOGLE_ONLY',
      };

      const response = await this.processQuery(request);

      return {
        success: response.success,
        content: response.response || '',
        model: process.env.GOOGLE_AI_MODEL || 'gemini-1.5-flash',
        tokensUsed: 0,
        cached: false,
        processingTime: response.processingTime || 0,
        confidence: response.confidence || 0.7,
        error: response.error
          ? {
              code: 'GOOGLE_AI_ERROR',
              message: response.error,
              details: response.error,
              timestamp: new Date().toISOString(),
              retryable: false,
            }
          : undefined,
      };
    } catch (error: any) {
      logger.error('âŒ Google AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨:', error);

      return {
        success: false,
        content: '',
        model: process.env.GOOGLE_AI_MODEL || 'gemini-1.5-flash',
        tokensUsed: 0,
        cached: false,
        processingTime: 0,
        confidence: 0,
        error: {
          code: 'GOOGLE_AI_ERROR',
          message: error.message || 'Google AI API ì‹¤íŒ¨',
          details: error.stack || error.toString(),
          timestamp: new Date().toISOString(),
          retryable: false,
        },
      };
    }
  }

  /**
   * ğŸ¯ ì„œë²„ ë©”íŠ¸ë¦­ ë¶„ì„ (ê¸°ì¡´ í˜¸í™˜ì„±)
   */
  async analyzeServerMetrics(metrics: ServerMetrics[]): Promise<string> {
    const prompt = `
ì„œë²„ ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

${metrics
  .map(
    server => `
ì„œë²„: ${server.name}
CPU: ${server.cpu_usage}%
ë©”ëª¨ë¦¬: ${server.memory_usage}%
ë””ìŠ¤í¬: ${server.disk_usage}%
ì‘ë‹µì‹œê°„: ${server.response_time}ms
ìƒíƒœ: ${server.status}
`
  )
  .join('\n')}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½
2. ì£¼ì˜ê°€ í•„ìš”í•œ ì„œë²„ ì‹ë³„
3. ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­
4. ì˜ˆìƒë˜ëŠ” ë¬¸ì œì ê³¼ ëŒ€ì‘ë°©ì•ˆ

ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        `;

    const response = await this.generateResponse(prompt);
    return response.content;
  }

  /**
   * ğŸ¥ í—¬ìŠ¤ì²´í¬
   */
  async healthCheck(): Promise<boolean> {
    try {
      if (!this.initialized || !this.model) {
        return false;
      }

      // ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
      const testResult = await Promise.race([
        this.model.generateContent('ì•ˆë…•í•˜ì„¸ìš”'),
        new Promise((_, reject) =>
          setTimeout(() => reject(new Error('Health check timeout')), 5000)
        ),
      ]);

      return !!testResult.response;
    } catch (error) {
      logger.warn('Google AI í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨:', error);
      return false;
    }
  }

  /**
   * ğŸ“Š ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ
   */
  getStatus() {
    return {
      initialized: this.initialized,
      requestCount: this.requestCount,
      lastError: this.lastError,
      model: process.env.GOOGLE_AI_MODEL || 'gemini-1.5-flash',
      enabledInPlan: vercelConfig.enableGoogleAI,
      vercelPlan: vercelConfig.environment.isPro ? 'pro' : 'hobby',
      timestamp: new Date().toISOString(),
    };
  }

  /**
   * âœ… ê°€ìš©ì„± í™•ì¸
   */
  isAvailable(): boolean {
    return this.initialized && vercelConfig.enableGoogleAI;
  }

  /**
   * âœ… ì¤€ë¹„ ìƒíƒœ í™•ì¸
   */
  isReady(): boolean {
    return this.isAvailable();
  }

  /**
   * ğŸ”— ì—°ê²° í…ŒìŠ¤íŠ¸
   */
  async testConnection(): Promise<{
    success: boolean;
    message: string;
    latency?: number;
  }> {
    try {
      const isHealthy = await this.healthCheck();
      return {
        success: isHealthy,
        message: isHealthy ? 'Google AI ì—°ê²° ì„±ê³µ' : 'Google AI ì—°ê²° ì‹¤íŒ¨',
        latency: isHealthy ? 100 : 0,
      };
    } catch (error) {
      return {
        success: false,
        message: `Google AI ì—°ê²° ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'Unknown error'}`,
        latency: 0,
      };
    }
  }

  /**
   * ğŸ§¹ ì •ë¦¬ ì‘ì—…
   */
  cleanup(): void {
    this.genAI = null;
    this.model = null;
    this.initialized = false;
    this.requestCount = 0;
    this.lastError = null;

    logger.info('ğŸ§¹ Google AI Service ì •ë¦¬ ì™„ë£Œ');
  }
}
