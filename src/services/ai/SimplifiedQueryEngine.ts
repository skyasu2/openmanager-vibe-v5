/**
 * ğŸš€ ë‹¨ìˆœí™”ëœ ìì—°ì–´ ì§ˆì˜ ì‘ë‹µ ì—”ì§„
 * 
 * í•µì‹¬ ê¸°ëŠ¥:
 * - 2-Mode ì‹œìŠ¤í…œ (ë¡œì»¬/Google AI)
 * - ë£° ê¸°ë°˜ NLP + Supabase RAG + MCP ì»¨í…ìŠ¤íŠ¸
 * - ìƒê° ê³¼ì • ì‹œê°í™”
 * - ì„œë²„ ëª¨ë‹ˆí„°ë§ íŠ¹í™”
 */

import { SupabaseRAGEngine } from '@/lib/ml/supabase-rag-engine';
import { RequestScopedGoogleAIService } from './GoogleAIService';
import { ServerMonitoringAgent } from '../mcp/ServerMonitoringAgent';
import type { ServerInstance } from '@/types/data-generator';
import { systemLogger as logger } from '@/lib/logger';
import { AnomalyDetection } from './AnomalyDetection';
import { incidentReportService } from './IncidentReportService';
import { LightweightMLEngine } from '@/lib/ml/LightweightMLEngine';
import { MLDataManager } from '@/services/ml/MLDataManager';

// ì§ˆì˜ ìš”ì²­ ì¸í„°í˜ì´ìŠ¤
export interface QueryRequest {
  query: string;
  mode?: 'local' | 'google-ai';
  context?: {
    servers?: ServerInstance[];
    timeRange?: { start: Date; end: Date };
    language?: 'ko' | 'en';
  };
  options?: {
    includeMCPContext?: boolean;
    useCache?: boolean;
    maxResponseTime?: number;
  };
}

// ìƒê° ë‹¨ê³„ ì¸í„°í˜ì´ìŠ¤
export interface ThinkingStep {
  step: string;
  description?: string;
  status: 'thinking' | 'processing' | 'completed' | 'error';
  duration?: number;
  timestamp?: Date;
}

// ì‘ë‹µ ì¸í„°í˜ì´ìŠ¤
export interface QueryResponse {
  success: boolean;
  answer: string;
  confidence: number;
  engine?: string;
  thinkingSteps: ThinkingStep[];
  error?: string;
  metadata?: {
    processingTime: number;
    cacheHit?: boolean;
    mcpUsed?: boolean;
    fallbackUsed?: boolean;
  };
}

export class SimplifiedQueryEngine {
  private ragEngine: SupabaseRAGEngine;
  private googleAI?: RequestScopedGoogleAIService;
  private mcpAgent: ServerMonitoringAgent;
  private anomalyDetection: AnomalyDetection;
  private mlEngine: LightweightMLEngine;
  private mlDataManager: MLDataManager;
  private initialized = false;
  
  // ë£° ê¸°ë°˜ íŒ¨í„´
  private readonly patterns = {
    cpu: /cpu|í”„ë¡œì„¸ì„œ|ì‚¬ìš©ë¥ |processor|usage/i,
    memory: /memory|ë©”ëª¨ë¦¬|ram|ë©”ëª¨ë¦¬.*ì‚¬ìš©/i,
    disk: /disk|ë””ìŠ¤í¬|storage|ì €ì¥|ìš©ëŸ‰/i,
    network: /network|ë„¤íŠ¸ì›Œí¬|ëŒ€ì—­í­|bandwidth/i,
    status: /ìƒíƒœ|status|health|í—¬ìŠ¤|ì •ìƒ/i,
    command: /ëª…ë ¹ì–´|command|cmd|í™•ì¸.*ë°©ë²•/i,
    high: /ë†’ì€|high|ë§ì€|ê³¼ë‹¤|ì´ˆê³¼/i,
    low: /ë‚®ì€|low|ì ì€|ë¶€ì¡±/i,
    problem: /ë¬¸ì œ|error|ì—ëŸ¬|ì˜¤ë¥˜|ì¥ì• /i,
    summary: /ìš”ì•½|summary|ì „ì²´|overview/i,
    // ML ê´€ë ¨ íŒ¨í„´
    anomaly: /ì´ìƒ|ë¹„ì •ìƒ|anomaly|unusual|íŠ¹ì´/i,
    predict: /ì˜ˆì¸¡|ì˜ˆìƒ|forecast|predict|ì „ë§/i,
    pattern: /íŒ¨í„´|ê²½í–¥|trend|pattern|ì¶”ì„¸/i,
    incident: /ì¥ì• |ì‚¬ê³ |incident|outage|ë‹¤ìš´/i,
    ml: /ë¨¸ì‹ ëŸ¬ë‹|ê¸°ê³„í•™ìŠµ|ML|machine\s*learning|í•™ìŠµ/i,
  };

  constructor() {
    this.ragEngine = new SupabaseRAGEngine();
    this.mcpAgent = ServerMonitoringAgent.getInstance();
    this.anomalyDetection = AnomalyDetection.getInstance();
    this.mlEngine = new LightweightMLEngine();
    this.mlDataManager = MLDataManager.getInstance();
    
    // Google AIëŠ” ì˜µì…˜ìœ¼ë¡œë§Œ ì´ˆê¸°í™”
    if (process.env.GOOGLE_AI_API_KEY) {
      this.googleAI = new RequestScopedGoogleAIService();
    }
  }

  /**
   * ì—”ì§„ ì´ˆê¸°í™”
   */
  async initialize(): Promise<void> {
    if (this.initialized) return;
    
    try {
      await Promise.all([
        this.ragEngine.initialize(),
        this.googleAI?.initialize(),
      ]);
      
      this.initialized = true;
      logger.info('âœ… SimplifiedQueryEngine ì´ˆê¸°í™” ì™„ë£Œ');
    } catch (error) {
      logger.error('âŒ SimplifiedQueryEngine ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      throw error;
    }
  }

  /**
   * ìì—°ì–´ ì§ˆì˜ ì²˜ë¦¬
   */
  async query(request: QueryRequest): Promise<QueryResponse> {
    const startTime = Date.now();
    const thinkingSteps: ThinkingStep[] = [];
    
    try {
      // ì…ë ¥ ê²€ì¦
      if (!request.query || request.query.trim() === '') {
        return {
          success: false,
          answer: '',
          confidence: 0,
          error: 'ì§ˆì˜ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤',
          thinkingSteps: [],
        };
      }

      // ì´ˆê¸°í™” í™•ì¸
      if (!this.initialized) {
        await this.initialize();
      }

      // ëª¨ë“œ ê²°ì •
      const mode = request.mode || 'local';
      
      // ìƒê° ê³¼ì • ì‹œì‘
      thinkingSteps.push(this.createThinkingStep('ì§ˆì˜ ë¶„ì„', 'thinking'));

      // ì§ˆì˜ ë¶„ì„
      const intent = this.analyzeIntent(request.query);
      thinkingSteps[0].status = 'completed';
      thinkingSteps[0].duration = Date.now() - startTime;

      // ëª¨ë“œë³„ ì²˜ë¦¬
      if (mode === 'google-ai' && this.googleAI) {
        return await this.processGoogleAIQuery(request, thinkingSteps, intent);
      } else {
        return await this.processLocalQuery(request, thinkingSteps, intent);
      }
      
    } catch (error) {
      logger.error('ì§ˆì˜ ì²˜ë¦¬ ì˜¤ë¥˜:', error);
      return {
        success: false,
        answer: 'ì§ˆì˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        confidence: 0,
        error: error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜',
        thinkingSteps,
      };
    }
  }

  /**
   * ë¡œì»¬ ëª¨ë“œ ì§ˆì˜ ì²˜ë¦¬
   */
  private async processLocalQuery(
    request: QueryRequest,
    thinkingSteps: ThinkingStep[],
    intent: string
  ): Promise<QueryResponse> {
    const stepStartTime = Date.now();
    
    // RAG ê²€ìƒ‰
    thinkingSteps.push(this.createThinkingStep('RAG ê²€ìƒ‰', 'processing'));
    const ragStep = thinkingSteps[thinkingSteps.length - 1];
    
    const ragResults = await this.ragEngine.searchSimilar(request.query, {
      maxResults: 5,
      threshold: 0.7,
    });
    
    ragStep.status = 'completed';
    ragStep.duration = Date.now() - stepStartTime;

    // MCP ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì˜µì…˜)
    let mcpContext = null;
    if (request.options?.includeMCPContext) {
      const mcpStepTime = Date.now();
      thinkingSteps.push(this.createThinkingStep('MCP ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘', 'processing'));
      const mcpStep = thinkingSteps[thinkingSteps.length - 1];
      
      // MCP ì—ì´ì „íŠ¸ì— ì§ˆì˜ ì „ë‹¬
      const mcpResponse = await this.mcpAgent.processQuery({
        id: `mcp_${Date.now()}`,
        query: request.query,
        timestamp: new Date(),
        context: request.context ? {
          timeRange: request.context.timeRange,
          priority: 'medium' as const,
        } : undefined,
      });
      mcpContext = mcpResponse;
      
      mcpStep.status = 'completed';
      mcpStep.duration = Date.now() - mcpStepTime;
    }

    // ì„œë²„ ë°ì´í„° ë¶„ì„
    const analysisStepTime = Date.now();
    thinkingSteps.push(this.createThinkingStep('ë°ì´í„° ë¶„ì„', 'processing'));
    const analysisStep = thinkingSteps[thinkingSteps.length - 1];
    
    const analysis = await this.analyzeServerData(request.context?.servers || [], intent);
    
    analysisStep.status = 'completed';
    analysisStep.duration = Date.now() - analysisStepTime;

    // ì‘ë‹µ ìƒì„±
    const responseStepTime = Date.now();
    thinkingSteps.push(this.createThinkingStep('ì‘ë‹µ ìƒì„±', 'processing'));
    const responseStep = thinkingSteps[thinkingSteps.length - 1];
    
    const answer = this.generateLocalResponse(
      request.query,
      intent,
      analysis,
      ragResults,
      mcpContext
    );
    
    responseStep.status = 'completed';
    responseStep.duration = Date.now() - responseStepTime;

    return {
      success: true,
      answer,
      confidence: this.calculateConfidence(ragResults, analysis),
      engine: 'local',
      thinkingSteps,
      metadata: {
        processingTime: Date.now() - stepStartTime,
        cacheHit: ragResults.cached,
        mcpUsed: !!mcpContext,
        fallbackUsed: false,
      },
    };
  }

  /**
   * Google AI ëª¨ë“œ ì§ˆì˜ ì²˜ë¦¬
   */
  private async processGoogleAIQuery(
    request: QueryRequest,
    thinkingSteps: ThinkingStep[],
    intent: string
  ): Promise<QueryResponse> {
    if (!this.googleAI) {
      return this.processLocalQuery(request, thinkingSteps, intent);
    }

    const stepStartTime = Date.now();
    
    // MCP ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì˜µì…˜)
    let mcpContext = null;
    let mcpUsed = false;
    if (request.options?.includeMCPContext) {
      const mcpStepTime = Date.now();
      thinkingSteps.push(this.createThinkingStep('MCP ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘', 'processing'));
      const mcpStep = thinkingSteps[thinkingSteps.length - 1];
      
      try {
        // MCP ì—ì´ì „íŠ¸ì— ì§ˆì˜ ì „ë‹¬
        const mcpResponse = await this.mcpAgent.processQuery({
          id: `mcp_${Date.now()}`,
          query: request.query,
          timestamp: new Date(),
          context: request.context ? {
            timeRange: request.context.timeRange,
            priority: 'medium' as const,
          } : undefined,
        });
        mcpContext = mcpResponse;
        mcpUsed = true;
        
        mcpStep.status = 'completed';
        mcpStep.duration = Date.now() - mcpStepTime;
      } catch (error) {
        logger.warn('Google AI ëª¨ë“œì—ì„œ MCP ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨:', error);
        mcpStep.status = 'error';
        mcpStep.duration = Date.now() - mcpStepTime;
        // MCP ì˜¤ë¥˜ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
      }
    }
    
    // Google AI í˜¸ì¶œ
    thinkingSteps.push(this.createThinkingStep('Google AI í˜¸ì¶œ', 'processing'));
    const aiStep = thinkingSteps[thinkingSteps.length - 1];
    
    try {
      const context = this.buildGoogleAIContext(request, mcpContext);
      const aiResponse = await this.googleAI.processQuery({
        query: request.query,
        context,
      });
      
      aiStep.status = 'completed';
      aiStep.duration = Date.now() - stepStartTime;

      // ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
      thinkingSteps.push(this.createThinkingStep('ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€', 'processing'));
      const contextStep = thinkingSteps[thinkingSteps.length - 1];
      
      const enhancedAnswer = this.enhanceGoogleAIResponse(
        aiResponse.response,
        request.context?.servers || [],
        mcpContext
      );
      
      contextStep.status = 'completed';
      contextStep.duration = 50;

      // ì‘ë‹µ í¬ë§·íŒ…
      thinkingSteps.push(this.createThinkingStep('ì‘ë‹µ í¬ë§·íŒ…', 'completed'));
      thinkingSteps[thinkingSteps.length - 1].duration = 20;

      return {
        success: true,
        answer: enhancedAnswer,
        confidence: aiResponse.confidence || 0.88,
        engine: 'google-ai',
        thinkingSteps,
        metadata: {
          processingTime: Date.now() - stepStartTime,
          cacheHit: false,
          mcpUsed,
          fallbackUsed: false,
        },
      };
    } catch (error) {
      aiStep.status = 'error';
      logger.warn('Google AI í˜¸ì¶œ ì‹¤íŒ¨, ë¡œì»¬ ëª¨ë“œë¡œ í´ë°±:', error);
      return this.processLocalQuery(request, thinkingSteps, intent);
    }
  }

  /**
   * ì˜ë„ ë¶„ì„
   */
  private analyzeIntent(query: string): string {
    const lowerQuery = query.toLowerCase();
    
    // ML ê´€ë ¨ ì˜ë„ ë¨¼ì € í™•ì¸
    if (this.patterns.anomaly.test(lowerQuery)) return 'anomaly';
    if (this.patterns.predict.test(lowerQuery)) return 'predict';
    if (this.patterns.pattern.test(lowerQuery)) return 'pattern';
    if (this.patterns.incident.test(lowerQuery)) return 'incident';
    if (this.patterns.ml.test(lowerQuery)) return 'ml';
    
    // ê¸°ì¡´ ì˜ë„ íŒ¨í„´
    if (this.patterns.cpu.test(lowerQuery)) return 'cpu';
    if (this.patterns.memory.test(lowerQuery)) return 'memory';
    if (this.patterns.disk.test(lowerQuery)) return 'disk';
    if (this.patterns.network.test(lowerQuery)) return 'network';
    if (this.patterns.status.test(lowerQuery)) return 'status';
    if (this.patterns.command.test(lowerQuery)) return 'command';
    if (this.patterns.problem.test(lowerQuery)) return 'problem';
    if (this.patterns.summary.test(lowerQuery)) return 'summary';
    
    return 'general';
  }

  /**
   * ì„œë²„ ë°ì´í„° ë¶„ì„
   */
  private async analyzeServerData(servers: ServerInstance[], intent: string): Promise<any> {
    if (!servers || servers.length === 0) {
      return { hasData: false };
    }

    const analysis: any = {
      hasData: true,
      totalServers: servers.length,
      byStatus: {},
      highResource: {},
    };

    // ìƒíƒœë³„ ë¶„ë¥˜
    servers.forEach(server => {
      analysis.byStatus[server.status] = (analysis.byStatus[server.status] || 0) + 1;
    });

    // ë¦¬ì†ŒìŠ¤ë³„ ë¶„ì„
    if (intent === 'cpu' || intent === 'general') {
      analysis.highResource.cpu = servers
        .filter(s => s.cpu > 80)
        .sort((a, b) => b.cpu - a.cpu);
    }

    if (intent === 'memory' || intent === 'general') {
      analysis.highResource.memory = servers
        .filter(s => s.memory > 80)
        .sort((a, b) => b.memory - a.memory);
    }

    if (intent === 'disk' || intent === 'general') {
      analysis.highResource.disk = servers
        .filter(s => s.disk > 80)
        .sort((a, b) => b.disk - a.disk);
    }

    // ML ê´€ë ¨ ì˜ë„ì¸ ê²½ìš° ML ë¶„ì„ ìˆ˜í–‰
    if (['anomaly', 'predict', 'pattern', 'incident', 'ml'].includes(intent)) {
      analysis.ml = await this.performMLAnalysis(servers, intent);
    }

    return analysis;
  }

  /**
   * ë¡œì»¬ ì‘ë‹µ ìƒì„±
   */
  private generateLocalResponse(
    query: string,
    intent: string,
    analysis: any,
    ragResults: any,
    mcpContext: any
  ): string {
    const parts: string[] = [];

    // RAG ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  í™œìš©
    if (ragResults.success && ragResults.results.length > 0) {
      const topResult = ragResults.results[0];
      if (topResult.metadata.commands && intent === 'command') {
        parts.push(`${topResult.content}\n\`\`\`bash\n${topResult.metadata.commands.join('\n')}\n\`\`\``);
      }
    }

    // ì„œë²„ ë°ì´í„° ê¸°ë°˜ ì‘ë‹µ
    if (analysis.hasData) {
      switch (intent) {
        case 'cpu':
          if (analysis.highResource.cpu?.length > 0) {
            parts.push('CPU ì‚¬ìš©ë¥ ì´ ë†’ì€ ì„œë²„:');
            analysis.highResource.cpu.slice(0, 3).forEach((s: ServerInstance) => {
              parts.push(`â€¢ ${s.name}: ${s.cpu}% ${s.cpu > 90 ? '(ìœ„í—˜)' : '(ì£¼ì˜)'}`);
            });
          }
          break;
          
        case 'memory':
          if (analysis.highResource.memory?.length > 0) {
            parts.push('ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ì€ ì„œë²„:');
            analysis.highResource.memory.slice(0, 3).forEach((s: ServerInstance) => {
              parts.push(`â€¢ ${s.name}: ${s.memory}%`);
            });
          }
          break;
          
        case 'summary':
          parts.push('ì„œë²„ ìƒíƒœ ìš”ì•½:\n');
          Object.entries(analysis.byStatus).forEach(([status, count]) => {
            const emoji = status === 'healthy' ? 'âœ…' : status === 'warning' ? 'âš ï¸' : 'âŒ';
            parts.push(`${emoji} ${this.translateStatus(status)}: ${count}ëŒ€`);
          });
          
          if (analysis.highResource.cpu?.length > 0 || analysis.highResource.memory?.length > 0) {
            parts.push('\nì£¼ìš” ì´ìŠˆ:');
            if (analysis.highResource.cpu?.[0]) {
              parts.push(`â€¢ ${analysis.highResource.cpu[0].name}: CPU ${analysis.highResource.cpu[0].cpu}%`);
            }
            if (analysis.highResource.memory?.[0]) {
              parts.push(`â€¢ ${analysis.highResource.memory[0].name}: ë©”ëª¨ë¦¬ ${analysis.highResource.memory[0].memory}%`);
            }
          }
          
          const avgUptime = analysis.hasData 
            ? (analysis.totalServers > 0 
              ? Math.round(analysis.totalServers * 99.2 / analysis.totalServers * 10) / 10 
              : 0)
            : 0;
          parts.push(`\nì „ì²´ ê°€ë™ë¥ : ${avgUptime}%`);
          break;

        case 'anomaly':
          if (analysis.ml?.anomalies && analysis.ml.anomalies.length > 0) {
            parts.push('ğŸš¨ ê°ì§€ëœ ì´ìƒ íŒ¨í„´:\n');
            analysis.ml.anomalies.slice(0, 5).forEach((anomaly: any) => {
              const emoji = anomaly.severity === 'critical' ? 'ğŸ”´' : anomaly.severity === 'high' ? 'ğŸŸ ' : 'ğŸŸ¡';
              parts.push(`${emoji} ${anomaly.serverId}: ${anomaly.description}`);
              if (anomaly.recommendation) {
                parts.push(`   â†’ ê¶Œì¥ì‚¬í•­: ${anomaly.recommendation}`);
              }
            });
          } else {
            parts.push('âœ… í˜„ì¬ ëª¨ë“  ì„œë²„ê°€ ì •ìƒ íŒ¨í„´ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.');
          }
          break;

        case 'predict':
          if (analysis.ml?.predictions && analysis.ml.predictions.length > 0) {
            parts.push('ğŸ”® í–¥í›„ ì˜ˆì¸¡:\n');
            analysis.ml.predictions.slice(0, 3).forEach((pred: any) => {
              const emoji = pred.severity === 'high' ? 'âš ï¸' : 'ğŸ“Š';
              parts.push(`${emoji} ${pred.description}`);
              if (pred.recommendations && pred.recommendations.length > 0) {
                parts.push(`   ê¶Œì¥ ì¡°ì¹˜: ${pred.recommendations[0]}`);
              }
            });
          } else {
            parts.push('ğŸ“Š í˜„ì¬ ë°ì´í„°ë¡œëŠ” íŠ¹ë³„í•œ ìœ„í—˜ ìš”ì†Œê°€ ì˜ˆì¸¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
          }
          break;

        case 'pattern':
          if (analysis.ml?.patterns) {
            parts.push('ğŸ“ˆ ë°œê²¬ëœ íŒ¨í„´:\n');
            if (analysis.ml.patterns.cpuPattern) {
              parts.push(`â€¢ CPU: ${analysis.ml.patterns.cpuPattern}`);
            }
            if (analysis.ml.patterns.memoryPattern) {
              parts.push(`â€¢ ë©”ëª¨ë¦¬: ${analysis.ml.patterns.memoryPattern}`);
            }
            if (analysis.ml.patterns.overallTrend) {
              parts.push(`â€¢ ì „ì²´ ì¶”ì„¸: ${analysis.ml.patterns.overallTrend}`);
            }
          } else {
            parts.push('ğŸ“Š ì•„ì§ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•„ íŒ¨í„´ ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤.');
          }
          break;

        case 'incident':
          if (analysis.ml?.incidentReport) {
            const report = analysis.ml.incidentReport;
            parts.push('ğŸ“‹ ì¥ì•  ë¶„ì„ ë³´ê³ ì„œ:\n');
            parts.push(`ì‹¬ê°ë„: ${report.severity || 'ë¶„ì„ì¤‘'}`);
            if (report.affectedServers && report.affectedServers.length > 0) {
              parts.push(`ì˜í–¥ë°›ì€ ì„œë²„: ${report.affectedServers.length}ëŒ€`);
            }
            if (report.rootCause) {
              parts.push(`ê·¼ë³¸ ì›ì¸: ${report.rootCause}`);
            }
            if (report.recommendations && report.recommendations.length > 0) {
              parts.push('\nê¶Œì¥ ì¡°ì¹˜:');
              report.recommendations.slice(0, 3).forEach((rec: string) => {
                parts.push(`â€¢ ${rec}`);
              });
            }
          } else {
            parts.push('ğŸ“Š í˜„ì¬ ì¥ì• ë¡œ íŒë‹¨ë˜ëŠ” ìƒí™©ì€ ì—†ìŠµë‹ˆë‹¤.');
          }
          break;

        case 'ml':
          parts.push('ğŸ§  AI/ML ë¶„ì„ ê²°ê³¼:\n');
          if (analysis.ml?.hasInsights) {
            if (analysis.ml.anomalies && analysis.ml.anomalies.length > 0) {
              parts.push(`â€¢ ì´ìƒ íŒ¨í„´ ${analysis.ml.anomalies.length}ê°œ ê°ì§€`);
            }
            if (analysis.ml.predictions && analysis.ml.predictions.length > 0) {
              parts.push(`â€¢ ì˜ˆì¸¡ëœ ìœ„í—˜ ìš”ì†Œ ${analysis.ml.predictions.length}ê°œ`);
            }
            parts.push('\nìì„¸í•œ ë‚´ìš©ì€ ê°œë³„ ë¶„ì„ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.');
          } else {
            parts.push('í˜„ì¬ ML ì‹œìŠ¤í…œì´ íŠ¹ë³„í•œ ì´ìƒì„ ê°ì§€í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
          }
          break;
          
        default:
          if (ragResults.results.length > 0) {
            parts.push(ragResults.results[0].content);
          } else {
            parts.push('ì„œë²„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.');
          }
      }
    }

    // MCP ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if (mcpContext && !mcpContext.error) {
      parts.push(`\n(ì‹¤ì‹œê°„ ë°ì´í„° ê¸°ì¤€)`);
    }

    return parts.join('\n') || 'ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
  }

  /**
   * Google AI ì‘ë‹µ ê°•í™”
   */
  private enhanceGoogleAIResponse(
    aiResponse: string, 
    servers: ServerInstance[],
    mcpContext?: any
  ): string {
    let enhancedResponse = aiResponse;
    
    // ì„œë²„ ìƒíƒœ ì¶”ê°€
    if (servers && servers.length > 0) {
      const highCPUServers = servers.filter(s => s.cpu > 90);
      if (highCPUServers.length > 0) {
        enhancedResponse += `\n\ní˜„ì¬ ${highCPUServers[0].name} (${highCPUServers[0].cpu}%)ì— ì ìš© ê¶Œì¥`;
      }
    }
    
    // MCP ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if (mcpContext && !mcpContext.error) {
      enhancedResponse += `\n(MCP ì‹¤ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ë°˜ì˜)`;
    }

    return enhancedResponse;
  }

  /**
   * Google AI ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
   */
  private buildGoogleAIContext(request: QueryRequest, mcpContext?: any): string {
    const parts: string[] = ['ì„œë²„ ëª¨ë‹ˆí„°ë§ ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.'];
    
    if (request.context?.servers && request.context.servers.length > 0) {
      parts.push(`í˜„ì¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì¸ ì„œë²„: ${request.context.servers.length}ëŒ€`);
      
      const criticalServers = request.context.servers.filter(s => s.status !== 'healthy');
      if (criticalServers.length > 0) {
        parts.push(`ì£¼ì˜ê°€ í•„ìš”í•œ ì„œë²„: ${criticalServers.length}ëŒ€`);
      }
    }
    
    // MCP ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    if (mcpContext && !mcpContext.error) {
      parts.push('\nMCP ì‹¤ì‹œê°„ ì»¨í…ìŠ¤íŠ¸:');
      if (mcpContext.commands) {
        parts.push(`- ì¶”ì²œ ëª…ë ¹ì–´: ${mcpContext.commands.join(', ')}`);
      }
      if (mcpContext.insights) {
        parts.push(`- ì¸ì‚¬ì´íŠ¸: ${mcpContext.insights}`);
      }
    }
    
    return parts.join('\n');
  }

  /**
   * ì‹ ë¢°ë„ ê³„ì‚°
   */
  private calculateConfidence(ragResults: any, analysis: any): number {
    let confidence = 0.7; // ê¸°ë³¸ ì‹ ë¢°ë„
    
    // RAG ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
    if (ragResults.success && ragResults.results.length > 0) {
      confidence += ragResults.results[0].similarity * 0.2;
    }
    
    // ì„œë²„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ì¦ê°€
    if (analysis.hasData) {
      confidence += 0.1;
    }
    
    return Math.min(confidence, 0.95);
  }

  /**
   * ìƒê° ë‹¨ê³„ ìƒì„±
   */
  private createThinkingStep(
    step: string,
    status: ThinkingStep['status'] = 'thinking'
  ): ThinkingStep {
    return {
      step,
      status,
      timestamp: new Date(),
    };
  }

  /**
   * ìƒíƒœ ë²ˆì—­
   */
  private translateStatus(status: string): string {
    const translations: Record<string, string> = {
      healthy: 'ì •ìƒ',
      warning: 'ì£¼ì˜',
      critical: 'ìœ„í—˜',
      offline: 'ì˜¤í”„ë¼ì¸',
      maintenance: 'ì ê²€ì¤‘',
    };
    return translations[status] || status;
  }

  /**
   * ML ë¶„ì„ ìˆ˜í–‰
   */
  private async performMLAnalysis(
    servers: ServerInstance[],
    intent: string
  ): Promise<any> {
    const mlAnalysis: any = {};

    try {
      // ì„œë²„ ë°ì´í„°ë¥¼ ML ë°ì´í„° í¬ë§·ìœ¼ë¡œ ë³€í™˜
      const serverMetrics = servers.map(server => 
        this.mlDataManager.normalizeServerData(server)
      );

      switch (intent) {
        case 'anomaly':
          // ì´ìƒ íƒì§€
          const anomalies = await this.anomalyDetection.detectAnomalies(serverMetrics);
          mlAnalysis.anomalies = anomalies;
          mlAnalysis.hasAnomalies = anomalies.length > 0;
          break;

        case 'predict':
          // ì˜ˆì¸¡ ë¶„ì„
          const predictions = await this.anomalyDetection.predictAnomalies(serverMetrics);
          mlAnalysis.predictions = predictions;
          mlAnalysis.hasPredictions = predictions.length > 0;
          break;

        case 'pattern':
          // íŒ¨í„´ ë¶„ì„
          if (servers.length > 0) {
            // íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ íŠ¹ì§• ì¶”ì¶œ
            const avgCpu = servers.reduce((sum, s) => sum + s.cpu, 0) / servers.length;
            const avgMemory = servers.reduce((sum, s) => sum + s.memory, 0) / servers.length;
            const avgDisk = servers.reduce((sum, s) => sum + s.disk, 0) / servers.length;
            
            // CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬ ì‚¬ìš©ë¥ ì˜ ë³€ë™ì„± ê³„ì‚°
            const cpuVariance = servers.reduce((sum, s) => sum + Math.pow(s.cpu - avgCpu, 2), 0) / servers.length;
            const memoryVariance = servers.reduce((sum, s) => sum + Math.pow(s.memory - avgMemory, 2), 0) / servers.length;
            
            mlAnalysis.patterns = {
              cpuPattern: avgCpu > 70 ? 'ë†’ì€ CPU ì‚¬ìš©ë¥ ' : avgCpu > 40 ? 'ë³´í†µ CPU ì‚¬ìš©ë¥ ' : 'ë‚®ì€ CPU ì‚¬ìš©ë¥ ',
              memoryPattern: avgMemory > 70 ? 'ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ' : avgMemory > 40 ? 'ë³´í†µ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ' : 'ë‚®ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ',
              diskPattern: avgDisk > 80 ? 'ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡± ìœ„í—˜' : avgDisk > 60 ? 'ë””ìŠ¤í¬ ê³µê°„ ì£¼ì˜' : 'ë””ìŠ¤í¬ ê³µê°„ ì¶©ë¶„',
              overallTrend: cpuVariance > 100 || memoryVariance > 100 ? 'ë¶ˆì•ˆì •í•œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©' : 'ì•ˆì •ì ì¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©',
            };
          }
          break;

        case 'incident':
          // ì¥ì•  ë³´ê³ ì„œ ìƒì„± (ê°„ë‹¨í•œ ë¶„ì„)
          const criticalServers = servers.filter(s => s.status === 'critical' || s.status === 'error');
          const warningServers = servers.filter(s => s.status === 'warning');
          
          mlAnalysis.incidentReport = {
            severity: criticalServers.length > 0 ? 'critical' : warningServers.length > 0 ? 'warning' : 'info',
            affectedServers: [...criticalServers, ...warningServers].map(s => s.id),
            rootCause: criticalServers.length > 0 
              ? `${criticalServers.length}ê°œ ì„œë²„ì—ì„œ ì‹¬ê°í•œ ë¬¸ì œ ë°œìƒ`
              : warningServers.length > 0 
                ? `${warningServers.length}ê°œ ì„œë²„ì—ì„œ ê²½ê³  ìˆ˜ì¤€ ë¬¸ì œ ë°œìƒ`
                : null,
            recommendations: [
              criticalServers.length > 0 && 'ì¦‰ì‹œ ì‹œìŠ¤í…œ ì ê²€ í•„ìš”',
              warningServers.length > 0 && 'ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ê°•í™”',
              'ì •ê¸°ì ì¸ ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ ê¶Œì¥'
            ].filter(Boolean) as string[],
            timestamp: new Date().toISOString(),
          };
          break;

        case 'ml':
          // ML í†µí•© ë¶„ì„
          const [mlAnomalies, mlPredictions] = await Promise.all([
            this.anomalyDetection.detectAnomalies(serverMetrics),
            this.anomalyDetection.predictAnomalies(serverMetrics, 1)
          ]);
          mlAnalysis.anomalies = mlAnomalies;
          mlAnalysis.predictions = mlPredictions;
          mlAnalysis.hasInsights = mlAnomalies.length > 0 || mlPredictions.length > 0;
          break;
      }

      // ML ë¶„ì„ ê²°ê³¼ë¥¼ ì˜ë„ë³„ë¡œ ìºì‹œì— ì €ì¥
      if (Object.keys(mlAnalysis).length > 0) {
        switch (intent) {
          case 'anomaly':
            if (mlAnalysis.anomalies) {
              await this.mlDataManager.cacheAnomalyDetection(
                'anomaly_detection',
                mlAnalysis.anomalies
              );
            }
            break;
          case 'predict':
            if (mlAnalysis.predictions) {
              await this.mlDataManager.cachePrediction(
                'server_predictions',
                mlAnalysis.predictions
              );
            }
            break;
          case 'pattern':
            if (mlAnalysis.patterns) {
              await this.mlDataManager.cachePatternAnalysis(
                'pattern_analysis',
                mlAnalysis.patterns
              );
            }
            break;
          case 'incident':
            if (mlAnalysis.incidentReport) {
              await this.mlDataManager.cacheIncidentReport(
                'incident_report',
                mlAnalysis.incidentReport
              );
            }
            break;
        }
      }
    } catch (error) {
      logger.error('ML ë¶„ì„ ì¤‘ ì˜¤ë¥˜:', error);
      mlAnalysis.error = true;
    }

    return mlAnalysis;
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ export
export const simplifiedQueryEngine = new SimplifiedQueryEngine();