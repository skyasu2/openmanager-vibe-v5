/**
 * ğŸ¤– Google AI Mode Processor - SimplifiedQueryEngine
 *
 * Handles Google AI mode query processing:
 * - Google AI API activation
 * - AI Assistant MCP activation (CloudContextLoader)
 * - Korean NLP processing
 * - GCP VM MCP server integration
 * - All advanced features included
 */

import type { SupabaseRAGEngine } from './supabase-rag-engine';
import { CloudContextLoader } from '../mcp/CloudContextLoader';
import { MockContextLoader } from './MockContextLoader';
import type {
  AIQueryContext,
  MCPContext,
  AIMetadata,
} from '../../types/ai-service-types';
import type {
  QueryRequest,
  QueryResponse,
} from './SimplifiedQueryEngine.types';
import { SimplifiedQueryEngineUtils } from './SimplifiedQueryEngine.utils';
import { SimplifiedQueryEngineHelpers } from './SimplifiedQueryEngine.processors.helpers';
import type { GoogleAIModel } from './QueryDifficultyAnalyzer';
import { getGoogleAIUsageTracker } from './GoogleAIUsageTracker';
// ğŸ”§ íƒ€ì„ì•„ì›ƒ ì„¤ì • (í†µí•© ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
import { getEnvironmentTimeouts } from '@/utils/timeout-config';
// ğŸš€ ì•„í‚¤í…ì²˜ ê°œì„ : ì§ì ‘ Google AI SDK í†µí•©
import { getDirectGoogleAIService } from './DirectGoogleAIService';

/**
 * ğŸ¤– êµ¬ê¸€ AI ëª¨ë“œ í”„ë¡œì„¸ì„œ
 */
export class GoogleAIModeProcessor {
  private utils: SimplifiedQueryEngineUtils;
  private contextLoader: CloudContextLoader;
  private mockContextLoader: MockContextLoader;
  private helpers: SimplifiedQueryEngineHelpers;
  private ragEngine: SupabaseRAGEngine;

  constructor(
    utils: SimplifiedQueryEngineUtils,
    contextLoader: CloudContextLoader,
    mockContextLoader: MockContextLoader,
    helpers: SimplifiedQueryEngineHelpers,
    ragEngine: SupabaseRAGEngine
  ) {
    this.utils = utils;
    this.contextLoader = contextLoader;
    this.mockContextLoader = mockContextLoader;
    this.helpers = helpers;
    this.ragEngine = ragEngine;
  }

  /**
   * êµ¬ê¸€ AI ëª¨ë“œ ì¿¼ë¦¬ ì²˜ë¦¬
   * - Google AI API í™œì„±í™”
   * - AI ì–´ì‹œìŠ¤í„´íŠ¸ MCP í™œì„±í™” (CloudContextLoader)
   * - í•œêµ­ì–´ NLP ì²˜ë¦¬
   * - ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
   */
  async processUnifiedQuery(
    query: string,
    context: AIQueryContext | undefined,
    options: QueryRequest['options'],
    mcpContext: MCPContext | null,
    thinkingSteps: QueryResponse['thinkingSteps'],
    startTime: number
  ): Promise<QueryResponse> {
    const enableKoreanNLP = true;
    const enableAIAssistantMCP = !!mcpContext;

    // 1ë‹¨ê³„: í•œêµ­ì–´ NLP ì²˜ë¦¬ (í™œì„±í™”ëœ ê²½ìš°)
    if (enableKoreanNLP) {
      const nlpStepStart = Date.now();
      thinkingSteps.push({
        step: 'í•œêµ­ì–´ NLP ì²˜ë¦¬',
        description: 'í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ ë° ì˜ë„ ë¶„ì„',
        status: 'pending',
        timestamp: nlpStepStart,
      });

      try {
        const koreanRatio = this.utils.calculateKoreanRatio(query);

        if (koreanRatio > 0.3) {
          // Korean NLP ì—”ì§„ (Google AI modeì—ì„œëŠ” Gemini APIê°€ ìì²´ ì²˜ë¦¬)
          const nlpStep = thinkingSteps[thinkingSteps.length - 1];
          if (nlpStep) {
            nlpStep.status = 'completed';
            nlpStep.description = `í•œêµ­ì–´ ë¹„ìœ¨ ${Math.round(koreanRatio * 100)}% - NLP ì²˜ë¦¬ ì™„ë£Œ`;
          }
        } else {
          const nlpEnglishStep = thinkingSteps[thinkingSteps.length - 1];
          if (nlpEnglishStep) {
            nlpEnglishStep.status = 'completed';
            nlpEnglishStep.description = `ì˜ì–´ ì¿¼ë¦¬ ê°ì§€ - NLP ê±´ë„ˆë›°ê¸°`;
          }
        }

        const nlpDurationStep = thinkingSteps[thinkingSteps.length - 1];
        if (nlpDurationStep) {
          nlpDurationStep.duration = Date.now() - nlpStepStart;
        }
      } catch (error) {
        console.warn('í•œêµ­ì–´ NLP ì²˜ë¦¬ ì‹¤íŒ¨:', error);
        const nlpFailedStep = thinkingSteps[thinkingSteps.length - 1];
        if (nlpFailedStep) {
          nlpFailedStep.status = 'failed';
          nlpFailedStep.duration = Date.now() - nlpStepStart;
        }
      }
    }

    // 2ë‹¨ê³„: RAG ê²€ìƒ‰ (Supabase pgvector)
    const ragStepStart = Date.now();
    thinkingSteps.push({
      step: 'Supabase RAG ê²€ìƒ‰',
      description: 'pgvector ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰',
      status: 'pending',
      timestamp: ragStepStart,
    });

    let ragResult;
    try {
      ragResult = await this.ragEngine.searchSimilar(query, {
        maxResults: 5,
        threshold: 0.5,
        category: options?.category,
        enableMCP: false,
        useLocalEmbeddings: true,
        enableKeywordFallback: true,
      });

      const ragStep = thinkingSteps[thinkingSteps.length - 1];
      if (ragStep) {
        ragStep.status = 'completed';
        ragStep.description = `${ragResult.totalResults}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬`;
        ragStep.duration = Date.now() - ragStepStart;
      }
    } catch (ragError) {
      console.error('RAG ê²€ìƒ‰ ì‹¤íŒ¨:', ragError);
      const ragFailedStep = thinkingSteps[thinkingSteps.length - 1];
      if (ragFailedStep) {
        ragFailedStep.status = 'failed';
        ragFailedStep.description = 'RAG ê²€ìƒ‰ ì‹¤íŒ¨';
        ragFailedStep.duration = Date.now() - ragStepStart;
      }
      // RAG ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•ŠìŒ. ê³„ì† ì§„í–‰.
      ragResult = {
        success: false,
        results: [],
        totalResults: 0,
        cached: false,
        processingTime: Date.now() - ragStepStart,
      };
    }

    // 3ë‹¨ê³„: Cloud Functions ê¸°ë°˜ í†µí•© ë¶„ì„
    const unifiedStepStart = Date.now();
    thinkingSteps.push({
      step: 'Cloud Functions ë¶„ì„',
      description: 'Korean NLP + ML Analytics + Server Analyzer ì‹¤í–‰',
      status: 'pending',
      timestamp: unifiedStepStart,
    });

    const unifiedInsights = await this.helpers.fetchUnifiedInsights(
      query,
      context,
      ragResult
    );

    const unifiedStep = thinkingSteps[thinkingSteps.length - 1];
    if (unifiedStep) {
      unifiedStep.status = unifiedInsights.summary ? 'completed' : 'failed';
      unifiedStep.description = unifiedInsights.summary
        ? 'Cloud Functions ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ'
        : 'Cloud Functions ê²°ê³¼ ì—†ìŒ';
      unifiedStep.duration = Date.now() - unifiedStepStart;
    }

    // 4ë‹¨ê³„: ê¸°ë³¸ ëª¨ë¸ ê³ ì • (ë¬´ë£Œ í‹°ì–´ ì•ˆì •ì„± ìš°ì„ )
    const modelStepStart = Date.now();
    thinkingSteps.push({
      step: 'ëª¨ë¸ ì„ íƒ',
      description: 'Flash-Lite ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (ë¬´ë£Œ í‹°ì–´ ìµœì í™”)',
      status: 'pending',
      timestamp: modelStepStart,
    });

    // ğŸ¯ ë¬´ë£Œ í‹°ì–´ ì•ˆì •ì„± ìš°ì„ : Flash-Lite ê³ ì • ì‚¬ìš© (í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ëŒ€ì²´ ëª¨ë¸ ì „í™˜ ê°€ëŠ¥)
    let selectedModel: GoogleAIModel = 'gemini-2.5-flash-lite';

    const modelStep = thinkingSteps[thinkingSteps.length - 1];
    if (modelStep) {
      modelStep.status = 'completed';
      modelStep.description = `Flash-Lite ëª¨ë¸ ì„ íƒ (RPD 1,000ê°œ, ì•ˆì •ì„± ìš°ì„ )`;
      modelStep.duration = Date.now() - modelStepStart;
    }

    console.log(`ğŸ¯ ëª¨ë¸ ê³ ì •: ${selectedModel} (ë¬´ë£Œ í‹°ì–´ ìµœì í™”)`);

    // ğŸ¯ ê¸°ë³¸ ëª¨ë¸ ê³ ì •: í‘œì¤€ íŒŒë¼ë¯¸í„° ì‚¬ìš© (ìŠ¤ì½”í”„ ì™¸ë¶€ì—ì„œ ì •ì˜)
    const standardTemperature = 0.7; // ê· í˜•ì¡íŒ ì°½ì˜ì„±
    const standardMaxTokens = 1000; // ì¶©ë¶„í•œ ì‘ë‹µ ê¸¸ì´

    // 5ë‹¨ê³„: Google AI API ì²˜ë¦¬ (ì„ íƒëœ ëª¨ë¸ ì‚¬ìš©)
    const googleStepStart = Date.now();
    thinkingSteps.push({
      step: 'Google AI ì²˜ë¦¬',
      description: `${selectedModel} ëª¨ë¸ë¡œ API í˜¸ì¶œ`,
      status: 'pending',
      timestamp: googleStepStart,
    });

    // ğŸ›¡ï¸ í• ë‹¹ëŸ‰ ë³´í˜¸: API í˜¸ì¶œ ì „ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    // ğŸ”„ ì‚¬ìš©ëŸ‰ ì¶”ì ê¸°: í• ë‹¹ëŸ‰ ì²´í¬ + ì„±ê³µ/ì‹¤íŒ¨ ê¸°ë¡ìš© (try/catch ë¸”ë¡ ì™¸ë¶€ì—ì„œ ì„ ì–¸í•˜ì—¬ ì¬ì‚¬ìš©)
    const usageTracker = getGoogleAIUsageTracker();

    try {
      // 1. ì„œë²„ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
      const serverContext = await this.helpers.getFormattedServerContext(query);

      // 2. ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ë¹Œë“œ
      const basePrompt = this.helpers.buildGoogleAIPrompt(
        query,
        context,
        mcpContext,
        ragResult,
        unifiedInsights.summary
      );

      // 3. ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ (ì„œë²„ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
      const prompt = serverContext ? basePrompt + serverContext : basePrompt;

      // ğŸš€ ì•„í‚¤í…ì²˜ ê°œì„ : ì§ì ‘ Google AI SDK í˜¸ì¶œ (ì¤‘ê°„ API Route ì œê±°)
      const timeouts = getEnvironmentTimeouts();

      // DirectGoogleAIService ì‚¬ìš© (API Wrapper Anti-Pattern ì œê±°)

      // ğŸ›¡ï¸ í• ë‹¹ëŸ‰ ë³´í˜¸ ë¡œì§ (usageTrackerëŠ” try ë¸”ë¡ ì™¸ë¶€ì—ì„œ ì„ ì–¸ë¨)
      if (!usageTracker.canUseModel(selectedModel)) {
        console.warn(
          `âš ï¸ [Google AI] ${selectedModel} í• ë‹¹ëŸ‰ ì´ˆê³¼, ëŒ€ì²´ ëª¨ë¸ í™•ì¸ ì¤‘...`
        );

        // ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ì²´ ëª¨ë¸ ì°¾ê¸°
        const availableModels = usageTracker.getAvailableModels();

        if (availableModels.length === 0) {
          // ëª¨ë“  ëª¨ë¸ í• ë‹¹ëŸ‰ ì´ˆê³¼ - ì—ëŸ¬ ë°˜í™˜
          const errorMsg =
            'Google AI ëª¨ë“  ëª¨ë¸ì˜ í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
          console.error(`âŒ [Google AI] ${errorMsg}`);

          throw new Error(errorMsg);
        }

        // ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ì „í™˜
        const fallbackModel = availableModels.at(0);
        if (!fallbackModel) {
          throw new Error(
            'Internal error: availableModels array is empty after length check'
          );
        }
        console.log(
          `âœ… [Google AI] ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: ${selectedModel} â†’ ${fallbackModel}`
        );
        selectedModel = fallbackModel;
      }

      console.log('ğŸš€ [Google AI] ìš”ì²­ ì‹œì‘:', {
        model: selectedModel,
        query: query.substring(0, 50) + (query.length > 50 ? '...' : ''),
        temperature: standardTemperature,
        maxTokens: standardMaxTokens,
        timeout: timeouts.GOOGLE_AI,
        promptLength: prompt.length,
      });

      const directGoogleAI = getDirectGoogleAIService();
      const apiResponse = await directGoogleAI.generateContent(prompt, {
        model: selectedModel,
        temperature: standardTemperature,
        maxTokens: standardMaxTokens,
        timeout: timeouts.GOOGLE_AI, // ğŸ¯ ë„‰ë„‰í•œ íƒ€ì„ì•„ì›ƒ: timeout-config.ts ì„¤ì • ì‚¬ìš© (8ì´ˆ)
      });

      console.log('ğŸ“Š [Google AI] ì‘ë‹µ ìƒíƒœ:', {
        success: apiResponse.success,
        error: apiResponse.error,
        responseTime: apiResponse.responseTime,
        contentLength: apiResponse.content?.length,
      });

      if (!apiResponse.success) {
        console.error('âŒ [Google AI] ìƒì„¸ ì—ëŸ¬:', {
          error: apiResponse.error,
          model: selectedModel,
          query,
          promptLength: prompt.length,
          responseTime: apiResponse.responseTime,
        });
        throw new Error(`Google AI ì§ì ‘ í˜¸ì¶œ ì˜¤ë¥˜: ${apiResponse.error}`);
      }

      const googleStep = thinkingSteps[thinkingSteps.length - 1];
      if (googleStep) {
        googleStep.status = 'completed';
        googleStep.description = 'Gemini API ì‘ë‹µ ìˆ˜ì‹ ';
        googleStep.duration = Date.now() - googleStepStart;
      }

      // ğŸ”„ ì‚¬ìš©ëŸ‰ ì¶”ì : ì„±ê³µí•œ API í˜¸ì¶œ ê¸°ë¡
      usageTracker.recordUsage({
        model: selectedModel,
        timestamp: Date.now(),
        requestCount: 1,
        tokenCount: apiResponse.usage?.totalTokens || 0,
        latency: apiResponse.responseTime,
        success: true,
        difficultyScore: 0.5,
      });

      // ğŸš€ ì§ì ‘ ì‘ë‹µ ì‚¬ìš© (êµ¬ì¡° ë‹¨ìˆœí™”)
      const finalResponse = apiResponse.content || 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
      const finalConfidence = 0.9; // DirectGoogleAIServiceëŠ” í•­ìƒ ë†’ì€ ì‹ ë¢°ë„
      
      // ë¹„ìš© ê³„ì‚°
      const tokenCount = apiResponse.usage?.totalTokens || Math.ceil((query.length + finalResponse.length) / 4);
      const actualCost = tokenCount * 0.000002; // $0.002 per 1K tokens

      return {
        success: true,
        response: finalResponse,
        engine: 'google-ai-rag',
        confidence: finalConfidence,
        thinkingSteps,
        metadata: {
          model: selectedModel,
          tokensUsed: tokenCount,
          mcpUsed: !!(mcpContext && enableAIAssistantMCP),
          aiAssistantMCPUsed: enableAIAssistantMCP,
          koreanNLPUsed: enableKoreanNLP,
          // GCP VM MCP ì œê±°ë¨ - Cloud Functions ì „ìš©ìœ¼ë¡œ ë‹¨ìˆœí™”
          mockMode: !!this.mockContextLoader.getMockContext(),
          mode: 'google-ai-rag',
          cloudFunctionsUsed: !!unifiedInsights.raw,
          cloudFunctionsCacheHit: unifiedInsights.raw?.metadata?.cacheHit ?? false,
          cloudFunctionsSummary: unifiedInsights.summary || undefined,
          // ë¹„ìš© ì •ë³´
          engineType: 'google-ai',
          savedCost: 0,
          actualCost: actualCost,
          tokenCount: tokenCount,
          // ê¸°ë³¸ ëª¨ë¸ ê³ ì • ì •ë³´
          modelInfo: {
            selectedModel,
            temperature: standardTemperature,
            maxTokens: standardMaxTokens,
            strategy: 'fixed-model', // ê³ ì • ëª¨ë¸ ì „ëµ
          },
        } as unknown as AIMetadata & {
          aiAssistantMCPUsed?: boolean;
          koreanNLPUsed?: boolean;
          mockMode?: boolean;
          engineType?: string;
          savedCost?: number;
          actualCost?: number;
          tokenCount?: number;
          modelInfo?: {
            selectedModel: GoogleAIModel;
            temperature: number;
            maxTokens: number;
            strategy: string;
          };
        },
        processingTime: Date.now() - startTime,
      };
    } catch (error) {
      console.error('âŒ [Google AI] ì²˜ë¦¬ ì˜¤ë¥˜ (catch):', {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined,
        query,
        model: selectedModel,
        processingTime: Date.now() - startTime,
      });

      // ğŸ”„ ì‚¬ìš©ëŸ‰ ì¶”ì : ì‹¤íŒ¨í•œ API í˜¸ì¶œ ê¸°ë¡
      usageTracker.recordUsage({
        model: selectedModel,
        timestamp: Date.now(),
        requestCount: 1,
        tokenCount: 0,
        latency: Date.now() - googleStepStart,
        success: false,
        difficultyScore: 0.5,
      });

      // ğŸš¨ í´ë°± ì œê±°: ì—ëŸ¬ ì§ì ‘ ë°˜í™˜
      const googleFailedStep = thinkingSteps[thinkingSteps.length - 1];
      if (googleFailedStep) {
        googleFailedStep.status = 'failed';
        googleFailedStep.description = 'Google AI ì²˜ë¦¬ ì‹¤íŒ¨';
        googleFailedStep.duration = Date.now() - googleStepStart;
      }

      // Google AI ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜ (í´ë°± ì—†ìŒ)
      return {
        success: false,
        response: 'Google AI ëª¨ë“œì—ì„œ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
        engine: 'google-ai-rag',
        confidence: 0,
        thinkingSteps,
        error: error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜',
        metadata: {
          model: selectedModel,
          aiAssistantMCPUsed: enableAIAssistantMCP,
          koreanNLPUsed: enableKoreanNLP,
          mockMode: !!this.mockContextLoader.getMockContext(),
          mode: 'google-ai-rag',
          cloudFunctionsUsed: !!unifiedInsights.raw,
          cloudFunctionsCacheHit: unifiedInsights.raw?.metadata?.cacheHit ?? false,
          // ê¸°ë³¸ ëª¨ë¸ ê³ ì • ì •ë³´ (ì—ëŸ¬ ì‹œì—ë„ í¬í•¨)
          modelInfo: {
            selectedModel,
            temperature: standardTemperature,
            maxTokens: standardMaxTokens,
            strategy: 'fixed-model', // ê³ ì • ëª¨ë¸ ì „ëµ
          },
        } as AIMetadata & {
          aiAssistantMCPUsed?: boolean;
          koreanNLPUsed?: boolean;
          mockMode?: boolean;
          modelInfo?: {
            selectedModel: GoogleAIModel;
            temperature: number;
            maxTokens: number;
            strategy: string;
          };
        },
        processingTime: Date.now() - startTime,
      };
    }
  }
}
