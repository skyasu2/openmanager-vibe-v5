/**
 * ğŸ¤– í†µí•© AI ì—”ì§„ v4.0 - ë¦¬íŒ©í† ë§ëœ ë²„ì „
 *
 * Design Patterns Applied:
 * - Single Responsibility Principle: ê° ëª¨ë“ˆì€ ë‹¨ì¼ ì±…ì„ì„ ê°€ì§
 * - Dependency Injection: ì˜ì¡´ì„±ì„ ì£¼ì…ë°›ì•„ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ
 * - Strategy Pattern: ì˜ë„ë³„ ì²˜ë¦¬ ì „ëµì„ êµì²´ ê°€ëŠ¥í•˜ê²Œ êµ¬í˜„
 * - Factory Pattern: í•¸ë“¤ëŸ¬ì™€ ì‘ë‹µ ìƒì„±ê¸°ë¥¼ íŒ©í† ë¦¬ë¡œ ê´€ë¦¬
 * - Template Method Pattern: ì¿¼ë¦¬ ì²˜ë¦¬ì˜ ê³µí†µ í”Œë¡œìš° ì •ì˜
 */

import { realMCPClient } from '../../mcp/real-mcp-client';
import { tensorFlowAIEngine } from '../tensorflow-engine';
import { 
  AIQueryRequest, 
  AIQueryResponse, 
  RenderStatus, 
  AIEngineConfiguration,
  NLPAnalysisResult 
} from './ai-types/AITypes';
import { NLPProcessor } from './nlp/NLPProcessor';
import { IntentHandlerFactory } from './intent-handlers/IntentHandlers';
import { ResponseGenerator } from './response/ResponseGenerator';
import { MetricsCollector } from './metrics/MetricsCollector';

export class IntegratedAIEngineRefactored {
  private initialized = false;
  private lastAnalysisCache: Map<string, any> = new Map();
  private activeSessions: Set<string> = new Set();
  private renderPingInterval?: NodeJS.Timeout;
  private renderStatus: RenderStatus = 'active';
  
  // ì˜ì¡´ì„± ì£¼ì…ëœ ì»´í¬ë„ŒíŠ¸ë“¤
  private nlpProcessor: NLPProcessor;
  private responseGenerator: ResponseGenerator;
  private metricsCollector: MetricsCollector;
  private config: AIEngineConfiguration;

  constructor(config: Partial<AIEngineConfiguration> = {}) {
    // ê¸°ë³¸ ì„¤ì •ê³¼ ì‚¬ìš©ì ì„¤ì • ë³‘í•©
    this.config = {
      renderPingInterval: 5 * 60 * 1000, // 5ë¶„
      maxCacheSize: 100,
      defaultConfidenceThreshold: 0.7,
      supportedLanguages: ['ko', 'en'],
      ...config
    };

    // ì˜ì¡´ì„± ì´ˆê¸°í™”
    this.nlpProcessor = new NLPProcessor();
    this.responseGenerator = new ResponseGenerator({
      defaultLanguage: 'ko',
      includeDebugInfo: false,
      maxResponseLength: 2000
    });
    this.metricsCollector = new MetricsCollector();

    this.startRenderManagement();
  }

  /**
   * ğŸ”„ Render ìë™ ê´€ë¦¬ ì‹œì‘ (ì„ íƒì )
   */
  private startRenderManagement(): void {
    const renderUrl = process.env.FASTAPI_URL;
    if (!renderUrl?.includes('onrender.com')) {
      // ê°œë°œ í™˜ê²½ì—ì„œë§Œ ë¡œê·¸ ì¶œë ¥
      if (process.env.NODE_ENV === 'development' && renderUrl) {
        console.log('â„¹ï¸ Render URLì´ ì•„ë‹˜ - Render ìë™ ê´€ë¦¬ ë¹„í™œì„±í™”');
      }
      return;
    }

    console.log('ğŸ”„ Render ìë™ ê´€ë¦¬ ì‹œì‘...');

    this.renderPingInterval = setInterval(
      async () => {
        try {
          const response = await fetch(renderUrl + '/health', {
            method: 'GET',
            headers: { 'Content-Type': 'application/json' },
          });

          this.renderStatus = response.ok ? 'active' : 'sleeping';
          console.log(response.ok ? 'âœ… Render ì„œë¹„ìŠ¤ ì •ìƒ' : 'âš ï¸ Render ì„œë¹„ìŠ¤ ì‘ë‹µ ì—†ìŒ');
        } catch (error) {
          this.renderStatus = 'error';
          console.log('âŒ Render ping ì‹¤íŒ¨:', error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜');
        }
      },
      this.config.renderPingInterval
    );

    process.on('beforeExit', () => {
      if (this.renderPingInterval) {
        clearInterval(this.renderPingInterval);
        console.log('ğŸ”„ Render ìë™ ê´€ë¦¬ ì¤‘ì§€');
      }
    });
  }

  /**
   * ğŸ¥ Render ìƒíƒœ í™•ì¸
   */
  getRenderStatus(): RenderStatus {
    return this.renderStatus;
  }

  /**
   * ì´ˆê¸°í™”
   */
  async initialize(): Promise<void> {
    if (this.initialized) return;

    console.log('ğŸ¤– í†µí•© AI ì—”ì§„ v4.0 ì´ˆê¸°í™” ì¤‘...');

    try {
      await Promise.all([
        realMCPClient.initialize(),
        tensorFlowAIEngine.initialize(),
      ]);

      this.initialized = true;
      console.log('âœ… í†µí•© AI ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ');
      console.log('ğŸ”§ í™œì„±í™”ëœ ì»´í¬ë„ŒíŠ¸:');
      console.log('  - âœ… NLP í”„ë¡œì„¸ì„œ');
      console.log('  - âœ… ì˜ë„ë³„ í•¸ë“¤ëŸ¬');
      console.log('  - âœ… ì‘ë‹µ ìƒì„±ê¸°');
      console.log('  - âœ… ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°');
      console.log('  - âœ… MCP í´ë¼ì´ì–¸íŠ¸');
      console.log('  - âœ… TensorFlow ì—”ì§„');
    } catch (error: any) {
      console.error('âŒ í†µí•© AI ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨:', error);
      this.initialized = true; // í´ë°± ëª¨ë“œë¡œ ê³„ì† ì§„í–‰
    }
  }

  /**
   * ğŸ§  ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì„œë“œ (Template Method Pattern)
   */
  async processQuery(request: AIQueryRequest): Promise<AIQueryResponse> {
    await this.initialize();

    const startTime = Date.now();
    const queryId = this.generateQueryId();
    const sessionId = request.context?.session_id || queryId;

    console.log(`ğŸ¤– AI ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: ${queryId}`);
    console.log(`ğŸ“ ì§ˆë¬¸: "${request.query}"`);

    this.activeSessions.add(sessionId);

    // ì‘ë‹µ ê°ì²´ ì´ˆê¸°í™”
    const response = this.initializeResponse(queryId, sessionId, request);

    try {
      // 1ë‹¨ê³„: ìì—°ì–´ ì²˜ë¦¬
      const nlpResult = await this.processNaturalLanguage(request, response);

      // 2ë‹¨ê³„: ì˜ë„ë³„ ì²˜ë¦¬
      await this.processIntentSpecificLogic(nlpResult, request, response);

      // 3ë‹¨ê³„: ì¢…í•© ì‘ë‹µ ìƒì„±
      await this.generateFinalResponse(nlpResult, request, response);

      // 4ë‹¨ê³„: ê¶Œì¥ì‚¬í•­ ìƒì„±
      this.generateRecommendations(nlpResult, response);

      // 5ë‹¨ê³„: ë³´ê³ ì„œ ìƒì„± (í•„ìš”í•œ ê²½ìš°)
      await this.generateReportIfNeeded(nlpResult, request, response);

      this.markSuccessfulCompletion(response, startTime);

    } catch (error: any) {
      this.handleProcessingError(error, request, response, startTime);
    } finally {
      this.activeSessions.delete(sessionId);
    }

    return response;
  }

  /**
   * 1ë‹¨ê³„: ìì—°ì–´ ì²˜ë¦¬
   */
  private async processNaturalLanguage(
    request: AIQueryRequest, 
    response: AIQueryResponse
  ): Promise<NLPAnalysisResult> {
    console.log('ğŸ“ 1ë‹¨ê³„: NLP ë¶„ì„ ì¤‘...');
    
    const nlpResult = await this.nlpProcessor.processNLP(request.query);
    response.analysis_results.nlp_analysis = nlpResult;
    response.intent = nlpResult.intent;
    response.confidence = nlpResult.confidence;
    response.processing_stats.components_used.push('nlp_processor');

    console.log(`ğŸ¯ ì˜ë„ ë¶„ì„ ê²°ê³¼: ${nlpResult.intent} (ì‹ ë¢°ë„: ${(nlpResult.confidence * 100).toFixed(1)}%)`);
    
    return nlpResult;
  }

  /**
   * 2ë‹¨ê³„: ì˜ë„ë³„ ì²˜ë¦¬
   */
  private async processIntentSpecificLogic(
    nlpResult: NLPAnalysisResult,
    request: AIQueryRequest,
    response: AIQueryResponse
  ): Promise<void> {
    console.log('ğŸ¯ 2ë‹¨ê³„: ì˜ë„ë³„ ì²˜ë¦¬ ì¤‘...');
    
    const handler = IntentHandlerFactory.getHandler(nlpResult.intent);
    await handler.handle({ nlpResult, request, response });
  }

  /**
   * 3ë‹¨ê³„: ì¢…í•© ì‘ë‹µ ìƒì„±
   */
  private async generateFinalResponse(
    nlpResult: NLPAnalysisResult,
    request: AIQueryRequest,
    response: AIQueryResponse
  ): Promise<void> {
    console.log('ğŸ’¬ 3ë‹¨ê³„: ì‘ë‹µ ìƒì„± ì¤‘...');
    
    await this.responseGenerator.generateComprehensiveAnswer(nlpResult, request, response);
  }

  /**
   * 4ë‹¨ê³„: ê¶Œì¥ì‚¬í•­ ìƒì„±
   */
  private generateRecommendations(
    nlpResult: NLPAnalysisResult,
    response: AIQueryResponse
  ): void {
    console.log('ğŸ’¡ 4ë‹¨ê³„: ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...');
    
    this.responseGenerator.generateRecommendations(nlpResult, response);
  }

  /**
   * 5ë‹¨ê³„: ë³´ê³ ì„œ ìƒì„±
   */
  private async generateReportIfNeeded(
    nlpResult: NLPAnalysisResult,
    request: AIQueryRequest,
    response: AIQueryResponse
  ): Promise<void> {
    if (this.responseGenerator.shouldGenerateReport(nlpResult, request)) {
      console.log('ğŸ“„ 5ë‹¨ê³„: ë³´ê³ ì„œ ìƒì„± ì¤‘...');
      await this.responseGenerator.generateReport(response, request);
    }
  }

  /**
   * ì‘ë‹µ ê°ì²´ ì´ˆê¸°í™”
   */
  private initializeResponse(
    queryId: string, 
    sessionId: string, 
    request: AIQueryRequest
  ): AIQueryResponse {
    return {
      success: false,
      query_id: queryId,
      intent: 'unknown',
      confidence: 0,
      answer: '',
      analysis_results: {
        nlp_analysis: null,
      },
      recommendations: [],
      processing_stats: {
        total_time: 0,
        components_used: [],
        models_executed: [],
        data_sources: [],
      },
      metadata: {
        timestamp: new Date().toISOString(),
        language: request.context?.language || 'ko',
        session_id: sessionId,
      },
    };
  }

  /**
   * ì„±ê³µ ì™„ë£Œ ì²˜ë¦¬
   */
  private markSuccessfulCompletion(response: AIQueryResponse, startTime: number): void {
    response.success = true;
    response.processing_stats.total_time = Date.now() - startTime;

    console.log(`âœ… AI ì¿¼ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: ${response.processing_stats.total_time}ms`);
    console.log(`ğŸ¯ ìµœì¢… ì‘ë‹µ: "${response.answer.substring(0, 100)}..."`);
  }

  /**
   * ì˜¤ë¥˜ ì²˜ë¦¬
   */
  private handleProcessingError(
    error: any, 
    request: AIQueryRequest, 
    response: AIQueryResponse, 
    startTime: number
  ): void {
    console.error('âŒ AI ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨:', error);
    
    response.success = false;
    response.answer = response.metadata.language === 'ko'
      ? 'ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.'
      : 'Sorry, an error occurred during processing. Please try again.';
    response.processing_stats.total_time = Date.now() - startTime;

    if (request.options?.include_debug) {
      response.metadata.debug_info = {
        error: error.message,
        stack: error.stack,
      };
    }
  }

  /**
   * ì¿¼ë¦¬ ID ìƒì„±
   */
  private generateQueryId(): string {
    return `query_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬
   */
  async *processQueryStream(
    request: AIQueryRequest
  ): AsyncGenerator<Partial<AIQueryResponse>, void, unknown> {
    const queryId = this.generateQueryId();
    const sessionId = request.context?.session_id || queryId;

    // ì´ˆê¸° ì‘ë‹µ
    yield {
      query_id: queryId,
      metadata: {
        timestamp: new Date().toISOString(),
        language: request.context?.language || 'ko',
        session_id: sessionId,
      },
    };

    try {
      // NLP ë¶„ì„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
      const nlpResult = await this.nlpProcessor.processNLP(request.query);
      yield {
        intent: nlpResult.intent,
        confidence: nlpResult.confidence,
        analysis_results: { nlp_analysis: nlpResult },
      };

      // ìµœì¢… ì²˜ë¦¬ ê²°ê³¼
      const fullResponse = await this.processQuery(request);
      yield fullResponse;

    } catch (error: any) {
      yield {
        success: false,
        answer: request.context?.language === 'ko' 
          ? 'ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
          : 'Error occurred during streaming processing.',
      };
    }
  }

  /**
   * ì—”ì§„ ìƒíƒœ í™•ì¸
   */
  async getEngineStatus(): Promise<any> {
    return {
      initialized: this.initialized,
      active_sessions: this.activeSessions.size,
      render_status: this.renderStatus,
      cache_size: this.lastAnalysisCache.size,
      supported_languages: this.config.supportedLanguages,
      components: {
        nlp_processor: true,
        intent_handlers: true,
        response_generator: true,
        metrics_collector: true,
        mcp_client: await realMCPClient.getStatus?.() || 'unknown',
        tensorflow_engine: await tensorFlowAIEngine.getStatus?.() || 'unknown',
      },
      config: this.config,
      metrics_cache_stats: this.metricsCollector.getCacheStats(),
    };
  }

  /**
   * ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   */
  dispose(): void {
    if (this.renderPingInterval) {
      clearInterval(this.renderPingInterval);
    }
    this.activeSessions.clear();
    this.lastAnalysisCache.clear();
    this.metricsCollector.clearCache();
    
    console.log('ğŸ”„ í†µí•© AI ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ');
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ë‚´ë³´ë‚´ê¸°
export const integratedAIEngine = new IntegratedAIEngineRefactored(); 