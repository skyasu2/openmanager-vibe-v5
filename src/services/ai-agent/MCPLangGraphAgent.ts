/**
 * ğŸ¤– MCP LangGraph Agent
 * 
 * LangGraph ìŠ¤íƒ€ì¼ì˜ ì‚¬ê³  ê³¼ì •ê³¼ ReAct í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•©í•œ MCP Agent
 * - ë‹¨ê³„ë³„ ë¡œì§ ì¶”ì  (logStep)
 * - Thought â†’ Observation â†’ Action â†’ Answer íë¦„
 * - OpenManager Vibe v5 ì‹œìŠ¤í…œê³¼ ì™„ì „ í†µí•©
 * - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„ ê¸°ëŠ¥
 */

import { 
  langGraphProcessor, 
  logStep, 
  thought, 
  observation, 
  action, 
  answer, 
  reflection 
} from '@/modules/ai-agent/core/LangGraphThinkingProcessor';
import { simulationEngine } from '@/services/simulationEngine';
import type { ServerMetrics } from '@/types/server';

export interface MCPQuery {
  id: string;
  question: string;
  context?: string;
  priority: 'low' | 'medium' | 'high' | 'critical';
  category: 'monitoring' | 'analysis' | 'prediction' | 'incident' | 'general';
}

export interface MCPResponse {
  query_id: string;
  answer: string;
  confidence: number;
  reasoning_steps: string[];
  recommendations: string[];
  related_servers: string[];
  execution_time: number;
  sources: string[];
}

export class MCPLangGraphAgent {
  private static instance: MCPLangGraphAgent;
  private isInitialized = false;
  private sessionId: string;

  private constructor() {
    this.sessionId = `mcp_session_${Date.now()}`;
  }

  static getInstance(): MCPLangGraphAgent {
    if (!MCPLangGraphAgent.instance) {
      MCPLangGraphAgent.instance = new MCPLangGraphAgent();
    }
    return MCPLangGraphAgent.instance;
  }

  async initialize(): Promise<void> {
    if (this.isInitialized) return;

    await langGraphProcessor.initialize();
    this.isInitialized = true;
    
    console.log(`ğŸ¤– MCP LangGraph Agent initialized (session: ${this.sessionId})`);
  }

  /**
   * ğŸ¯ ë©”ì¸ ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ - LangGraph ìŠ¤íƒ€ì¼
   */
  async processQuery(query: MCPQuery): Promise<MCPResponse> {
    const startTime = Date.now();
    
    // 1. ì‚¬ê³  íë¦„ ì‹œì‘
    const queryId = langGraphProcessor.startThinking(this.sessionId, query.question, 'react');
    
    try {
      // 2. ì§ˆë¬¸ ë¶„ì„ ë‹¨ê³„
      const intent = await this.analyzeQuery(query);
      
      // 3. ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë‹¨ê³„
      const context = await this.gatherContext(query, intent);
      
      // 4. ë¶„ì„ ë° ì¶”ë¡  ë‹¨ê³„
      const analysis = await this.performAnalysis(context, intent);
      
      // 5. ë‹µë³€ ìƒì„± ë‹¨ê³„
      const response = await this.generateResponse(query, analysis);
      
      // 6. ê²€ì¦ ë° ìµœì¢…í™” ë‹¨ê³„
      const finalResponse = await this.validateAndFinalize(response);
      
      // 7. ì‚¬ê³  ê³¼ì • ì™„ë£Œ
      langGraphProcessor.completeThinking(finalResponse);
      
      return {
        ...finalResponse,
        query_id: queryId,
        execution_time: Date.now() - startTime
      };
      
    } catch (error) {
      console.error('MCP ì§ˆì˜ ì²˜ë¦¬ ì‹¤íŒ¨:', error);
      langGraphProcessor.errorThinking(error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜');
      
      throw error;
    }
  }

  /**
   * ğŸ” 1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ì„
   */
  private async analyzeQuery(query: MCPQuery): Promise<string> {
    const stepId = logStep("ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...", `ì‚¬ìš©ì ì§ˆë¬¸: "${query.question}"`, 'analysis');
    
    thought(`ì‚¬ìš©ìê°€ "${query.question}"ì— ëŒ€í•´ ${query.priority} ìš°ì„ ìˆœìœ„ë¡œ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬ëŠ” ${query.category}ì…ë‹ˆë‹¤.`);
    
    // ì§ˆë¬¸ ì˜ë„ ë¶„ì„
    const keywords = query.question.toLowerCase();
    let intent = 'general_inquiry';
    
    if (keywords.includes('ì„œë²„') || keywords.includes('server')) {
      if (keywords.includes('ìƒíƒœ') || keywords.includes('status')) {
        intent = 'server_status_check';
      } else if (keywords.includes('ì„±ëŠ¥') || keywords.includes('performance')) {
        intent = 'performance_analysis';
      } else if (keywords.includes('ì¥ì• ') || keywords.includes('error') || keywords.includes('down')) {
        intent = 'incident_investigation';
      }
    } else if (keywords.includes('ì˜ˆì¸¡') || keywords.includes('predict')) {
      intent = 'predictive_analysis';
    } else if (keywords.includes('ì¶”ì²œ') || keywords.includes('recommend')) {
      intent = 'recommendation_request';
    }
    
    observation(`ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ: ì˜ë„=${intent}, ì¹´í…Œê³ ë¦¬=${query.category}, ìš°ì„ ìˆœìœ„=${query.priority}`);
    langGraphProcessor.completeStep(stepId, { intent, category: query.category, priority: query.priority });
    
    return intent;
  }

  /**
   * ğŸ“Š 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
   */
  private async gatherContext(query: MCPQuery, intent: string): Promise<any> {
    const stepId = logStep("ê´€ë ¨ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì¤‘...", `ì˜ë„: ${intent}`, 'query');
    langGraphProcessor.thought("ë¶„ì„ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì•¼ í•©ë‹ˆë‹¤.");
    
    try {
      console.log('ğŸ“¡ ì„œë²„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...');
      
      // ğŸ”§ ì„œë²„ì‚¬ì´ë“œì—ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ ì§ì ‘ ì‚¬ìš©
      const servers = simulationEngine.getServers();
      console.log('âœ… ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ì—ì„œ ì§ì ‘ ë°ì´í„° ìˆ˜ì‹ :', servers.length + 'ê°œ ì„œë²„');
      
      if (servers.length === 0) {
        console.warn('âš ï¸ ì„œë²„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ ì‹œì‘...');
        simulationEngine.start();
        await new Promise(resolve => setTimeout(resolve, 500));
        const retryServers = simulationEngine.getServers();
        console.log('ğŸ”„ ì¬ì‹œë„ í›„ ì„œë²„ ìˆ˜:', retryServers.length);
        
        if (retryServers.length === 0) {
          throw new Error('ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ì—ì„œ ì„œë²„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
        }
        
        return await this.processServerData(retryServers, intent);
      }
      
      const context = await this.processServerData(servers, intent);
      
      observation(`ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ: ${servers.length}ê°œ ì„œë²„, ë¶„ì„ ëŒ€ìƒ ì„ ì •ë¨`);
      langGraphProcessor.completeStep(stepId, context);
      
      return context;
      
    } catch (error) {
      console.error('âŒ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨:', error);
      langGraphProcessor.errorStep(stepId, error instanceof Error ? error.message : 'ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨');
      
      // ğŸ”„ Fallback: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
      const fallbackContext = {
        servers: [],
        intent,
        timestamp: new Date().toISOString(),
        source: 'fallback',
        error: error instanceof Error ? error.message : 'ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨'
      };
      
      observation(`Fallback ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©: ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ë°ì´í„° ìƒì„±`);
      langGraphProcessor.completeStep(stepId, fallbackContext);
      
      return fallbackContext;
    }
  }

  /**
   * ğŸ”§ ì„œë²„ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„
   */
  private async processServerData(servers: any[], intent: string): Promise<any> {
    // ì˜ë„ì— ë”°ë¥¸ ê´€ë ¨ ì„œë²„ í•„í„°ë§ ë° ë¶„ì„
    const relevantServers = servers.filter(server => {
      if (intent === 'server_status_check') return true;
      if (intent === 'performance_analysis') return server.cpu_usage > 70 || server.memory_usage > 80;
      if (intent === 'incident_investigation') return server.status !== 'healthy' || server.alerts?.length > 0;
      return true; // ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ì„œë²„ í¬í•¨
    });

    // ìƒíƒœë³„ ë¶„ë¥˜
    const statusSummary = {
      healthy: relevantServers.filter(s => s.status === 'healthy').length,
      warning: relevantServers.filter(s => s.status === 'warning').length,
      critical: relevantServers.filter(s => s.status === 'critical').length,
      total: relevantServers.length
    };

    // ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìš”ì•½
    const performanceSummary = {
      avg_cpu: relevantServers.reduce((sum, s) => sum + (s.cpu_usage || 0), 0) / relevantServers.length,
      avg_memory: relevantServers.reduce((sum, s) => sum + (s.memory_usage || 0), 0) / relevantServers.length,
      max_response_time: Math.max(...relevantServers.map(s => s.response_time || 0)),
      total_alerts: relevantServers.reduce((sum, s) => sum + (s.alerts?.length || 0), 0)
    };

    const relevantData: any = {
      servers: relevantServers,
      status_summary: statusSummary,
      performance_summary: performanceSummary,
      intent,
      timestamp: new Date().toISOString(),
      analysis_scope: `${relevantServers.length}/${servers.length} ì„œë²„ ë¶„ì„`
    };

    return relevantData;
  }

  /**
   * ğŸ§® 3ë‹¨ê³„: ë¶„ì„ ë° ì¶”ë¡ 
   */
  private async performAnalysis(context: any, intent: string): Promise<any> {
    const stepId = logStep("ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  íŒ¨í„´ì„ ì°¾ëŠ” ì¤‘...", `${intent} ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„ ìˆ˜í–‰`, 'processing');
    
    thought("ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.");
    action(`${intent} ë¶„ì„ ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰`);
    
    let analysis: any = {};
    
    switch (intent) {
      case 'server_status_check':
        analysis = this.analyzeServerStatus(context);
        break;
      case 'performance_analysis':
        analysis = this.analyzePerformance(context);
        break;
      case 'incident_investigation':
        analysis = this.analyzeIncidents(context);
        break;
      case 'predictive_analysis':
        analysis = this.predictTrends(context);
        break;
      default:
        analysis = this.generalAnalysis(context);
    }
    
    observation(`ë¶„ì„ ì™„ë£Œ: ${Object.keys(analysis).length}ê°œ ë¶„ì„ í•­ëª© ë„ì¶œ`);
    langGraphProcessor.completeStep(stepId, analysis);
    
    return analysis;
  }

  /**
   * ğŸ“ 4ë‹¨ê³„: ë‹µë³€ ìƒì„±
   */
  private async generateResponse(query: MCPQuery, analysis: any): Promise<Partial<MCPResponse>> {
    const stepId = logStep("ë‹µë³€ì„ êµ¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...", "ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì¹œí™”ì ì¸ ë‹µë³€ ìƒì„±", 'summary');
    
    thought("ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹µë³€ì„ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.");
    action("ë‹µë³€ í…œí”Œë¦¿ ìƒì„± ë° ê°œì¸í™”");
    
    let responseText = '';
    let recommendations: string[] = [];
    let relatedServers: string[] = [];
    let confidence = 0.8; // ê¸°ë³¸ ì‹ ë¢°ë„
    
    // ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ë‹µë³€ ìƒì„±
    if (analysis.serverStatus) {
      responseText = this.formatServerStatusResponse(analysis.serverStatus);
      relatedServers = analysis.serverStatus.allServers?.map((s: any) => s.hostname) || [];
      recommendations = this.generateServerStatusRecommendations(analysis.serverStatus);
    } else if (analysis.performance) {
      responseText = this.formatPerformanceResponse(analysis.performance);
      relatedServers = analysis.performance.issueServers?.map((s: any) => s.hostname) || [];
      recommendations = this.generatePerformanceRecommendations(analysis.performance);
    } else if (analysis.incidents) {
      responseText = this.formatIncidentResponse(analysis.incidents);
      relatedServers = analysis.incidents.affectedServers?.map((s: any) => s.hostname) || [];
      recommendations = this.generateIncidentRecommendations(analysis.incidents);
      confidence = 0.9; // ì¸ì‹œë˜íŠ¸ ë¶„ì„ì€ ë†’ì€ ì‹ ë¢°ë„
    } else {
      responseText = this.formatGeneralResponse(analysis);
      recommendations = ['ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.', 'ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.'];
      confidence = 0.6; // ì¼ë°˜ ë¶„ì„ì€ ë‚®ì€ ì‹ ë¢°ë„
    }
    
    const response = {
      answer: responseText,
      confidence,
      recommendations,
      related_servers: relatedServers,
      reasoning_steps: this.extractReasoningSteps(),
      sources: ['OpenManager Simulation Engine', 'Real-time Server Metrics']
    };
    
    answer(responseText);
    langGraphProcessor.completeStep(stepId, response);
    
    return response;
  }

  /**
   * âœ… 5ë‹¨ê³„: ê²€ì¦ ë° ìµœì¢…í™”
   */
  private async validateAndFinalize(response: Partial<MCPResponse>): Promise<MCPResponse> {
    const stepId = logStep("ë‹µë³€ì„ ê²€ì¦í•˜ê³  ìµœì¢…í™”í•˜ëŠ” ì¤‘...", "í’ˆì§ˆ ê²€ì‚¬ ë° ìµœì¢… ë‹µë³€ ì¤€ë¹„", 'validation');
    
    thought("ìƒì„±ëœ ë‹µë³€ì˜ ì •í™•ì„±ê³¼ ì™„ì„±ë„ë¥¼ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤.");
    action("ë‹µë³€ í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰");
    
    // ê¸°ë³¸ê°’ ì„¤ì •
    const finalResponse: MCPResponse = {
      query_id: '',
      answer: response.answer || 'ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
      confidence: response.confidence || 0.5,
      reasoning_steps: response.reasoning_steps || [],
      recommendations: response.recommendations || [],
      related_servers: response.related_servers || [],
      execution_time: 0,
      sources: response.sources || []
    };
    
    // ì‹ ë¢°ë„ ì¡°ì •
    if (finalResponse.answer.length < 50) {
      finalResponse.confidence *= 0.8; // ì§§ì€ ë‹µë³€ì€ ì‹ ë¢°ë„ ê°ì†Œ
    }
    if (finalResponse.recommendations.length === 0) {
      finalResponse.confidence *= 0.9; // ì¶”ì²œì‚¬í•­ ì—†ìœ¼ë©´ ì‹ ë¢°ë„ ê°ì†Œ
    }
    
    // ìµœì¢… ê²€ì¦
    if (finalResponse.confidence < 0.3) {
      finalResponse.answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì¶©ë¶„í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?";
      finalResponse.recommendations = ["ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”", "ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”"];
    }
    
    observation(`ë‹µë³€ ê²€ì¦ ì™„ë£Œ: ì‹ ë¢°ë„ ${Math.round(finalResponse.confidence * 100)}%, ${finalResponse.recommendations.length}ê°œ ì¶”ì²œì‚¬í•­`);
    langGraphProcessor.completeStep(stepId, finalResponse);
    
    reflection(`ì´ ë¶„ì„ ì‹œê°„ê³¼ í’ˆì§ˆì„ ê³ ë ¤í•  ë•Œ, ì´ ë‹µë³€ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì ì ˆíˆ ëŒ€ì‘í–ˆë‹¤ê³  íŒë‹¨ë©ë‹ˆë‹¤.`);
    
    return finalResponse;
  }

  /**
   * ğŸ¥ ì„œë²„ ìƒíƒœ ë¶„ì„
   */
  private analyzeServerStatus(context: any): any {
    const { servers, healthyServers, warningServers, errorServers } = context;
    
    return {
      serverStatus: {
        total: servers.length,
        healthy: healthyServers.length,
        warning: warningServers.length,
        error: errorServers.length,
        healthPercentage: Math.round((healthyServers.length / servers.length) * 100),
        allServers: servers,
        issueServers: [...warningServers, ...errorServers]
      }
    };
  }

  /**
   * âš¡ ì„±ëŠ¥ ë¶„ì„
   */
  private analyzePerformance(context: any): any {
    const { servers, highCpuServers, highMemoryServers, slowResponseServers } = context;
    
    return {
      performance: {
        totalServers: servers.length,
        highCpuCount: highCpuServers.length,
        highMemoryCount: highMemoryServers.length,
        slowResponseCount: slowResponseServers.length,
        issueServers: [...new Set([...highCpuServers, ...highMemoryServers, ...slowResponseServers])],
        avgCpu: servers.reduce((sum: number, s: any) => sum + s.cpu_usage, 0) / servers.length,
        avgMemory: servers.reduce((sum: number, s: any) => sum + s.memory_usage, 0) / servers.length,
        avgResponseTime: servers.reduce((sum: number, s: any) => sum + s.response_time, 0) / servers.length
      }
    };
  }

  /**
   * ğŸš¨ ì¸ì‹œë˜íŠ¸ ë¶„ì„
   */
  private analyzeIncidents(context: any): any {
    const { servers, alertedServers, criticalAlerts } = context;
    
    return {
      incidents: {
        totalServers: servers.length,
        affectedServers: alertedServers,
        criticalAlerts: criticalAlerts,
        severityDistribution: this.calculateSeverityDistribution(criticalAlerts),
        topIssues: this.identifyTopIssues(criticalAlerts)
      }
    };
  }

  /**
   * ğŸ”® íŠ¸ë Œë“œ ì˜ˆì¸¡
   */
  private predictTrends(context: any): any {
    const { servers } = context;
    
    return {
      predictions: {
        systemLoad: this.predictSystemLoad(servers),
        potentialIssues: this.identifyPotentialIssues(servers),
        recommendations: this.generatePredictiveRecommendations(servers)
      }
    };
  }

  /**
   * ğŸ“Š ì¼ë°˜ ë¶„ì„
   */
  private generalAnalysis(context: any): any {
    const { servers, summary } = context;
    
    return {
      general: {
        serverCount: servers.length,
        simulationInfo: summary,
        systemOverview: {
          avgCpu: servers.reduce((sum: number, s: any) => sum + s.cpu_usage, 0) / servers.length,
          avgMemory: servers.reduce((sum: number, s: any) => sum + s.memory_usage, 0) / servers.length,
          totalAlerts: servers.reduce((sum: number, s: any) => sum + (s.alerts?.length || 0), 0)
        }
      }
    };
  }

  // ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤...
  private formatServerStatusResponse(status: any): string {
    return `ğŸ¥ **ì„œë²„ ìƒíƒœ ìš”ì•½**

ì „ì²´ ${status.total}ê°œ ì„œë²„ ì¤‘:
- âœ… ì •ìƒ: ${status.healthy}ê°œ (${status.healthPercentage}%)
- âš ï¸ ê²½ê³ : ${status.warning}ê°œ
- âŒ ì˜¤ë¥˜: ${status.error}ê°œ

${status.error > 0 ? 'âš ï¸ ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•œ ì„œë²„ê°€ ìˆìŠµë‹ˆë‹¤.' : 'âœ¨ ëª¨ë“  ì„œë²„ê°€ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤.'}`;
  }

  private formatPerformanceResponse(perf: any): string {
    return `âš¡ **ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼**

ì‹œìŠ¤í…œ í‰ê·  ì§€í‘œ:
- CPU ì‚¬ìš©ë¥ : ${Math.round(perf.avgCpu)}%
- ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : ${Math.round(perf.avgMemory)}%
- í‰ê·  ì‘ë‹µì‹œê°„: ${Math.round(perf.avgResponseTime)}ms

ì„±ëŠ¥ ì´ìŠˆ:
- ë†’ì€ CPU ì‚¬ìš©ë¥ : ${perf.highCpuCount}ëŒ€
- ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : ${perf.highMemoryCount}ëŒ€
- ëŠë¦° ì‘ë‹µì‹œê°„: ${perf.slowResponseCount}ëŒ€`;
  }

  private formatIncidentResponse(incidents: any): string {
    return `ğŸš¨ **ì¸ì‹œë˜íŠ¸ ë¶„ì„**

- ì˜í–¥ë°›ì€ ì„œë²„: ${incidents.affectedServers.length}ëŒ€
- ì‹¬ê°í•œ ì•Œë¦¼: ${incidents.criticalAlerts.length}ê±´
- ì£¼ìš” ì´ìŠˆ ìœ í˜•: ${incidents.topIssues.slice(0, 3).join(', ')}

ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤.`;
  }

  private formatGeneralResponse(analysis: any): string {
    return `ğŸ“Š **ì‹œìŠ¤í…œ ê°œìš”**

í˜„ì¬ ${analysis.general?.serverCount || 0}ê°œì˜ ì„œë²„ê°€ ëª¨ë‹ˆí„°ë§ë˜ê³  ìˆìœ¼ë©°, ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ì„ í†µí•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒíƒœë¥¼ ì¶”ì í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ì„œë²„ ìƒíƒœ, ì„±ëŠ¥ ë¶„ì„, ë˜ëŠ” íŠ¹ì • ì„œë²„ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.`;
  }

  private generateServerStatusRecommendations(status: any): string[] {
    const recommendations = [];
    
    if (status.error > 0) {
      recommendations.push("ì˜¤ë¥˜ ìƒíƒœ ì„œë²„ì˜ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê³  ì¦‰ì‹œ ì¡°ì¹˜í•˜ì„¸ìš”");
      recommendations.push("ì¥ì•  ì„œë²„ì˜ ë°±ì—… ì‹œìŠ¤í…œ ê°€ë™ì„ ê²€í† í•˜ì„¸ìš”");
    }
    
    if (status.warning > 0) {
      recommendations.push("ê²½ê³  ìƒíƒœ ì„œë²„ì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”");
    }
    
    if (status.healthPercentage < 80) {
      recommendations.push("ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì¸í”„ë¼ ì ê²€ì„ ê¶Œì¥í•©ë‹ˆë‹¤");
    }
    
    return recommendations;
  }

  private generatePerformanceRecommendations(perf: any): string[] {
    const recommendations = [];
    
    if (perf.avgCpu > 80) {
      recommendations.push("CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ë¡œë“œ ë°¸ëŸ°ì‹±ì„ ê²€í† í•˜ì„¸ìš”");
    }
    
    if (perf.avgMemory > 80) {
      recommendations.push("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ì¦ì„¤ì„ ê³ ë ¤í•˜ì„¸ìš”");
    }
    
    if (perf.avgResponseTime > 300) {
      recommendations.push("ì‘ë‹µì‹œê°„ì´ ëŠë¦½ë‹ˆë‹¤. ìºì‹œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”");
    }
    
    return recommendations;
  }

  private generateIncidentRecommendations(incidents: any): string[] {
    return [
      "ì˜í–¥ë°›ì€ ì„œë²„ë“¤ì˜ ë¡œê·¸ë¥¼ ì¦‰ì‹œ í™•ì¸í•˜ì„¸ìš”",
      "ì‹¬ê°í•œ ì•Œë¦¼ì— ëŒ€í•œ ì—ìŠ¤ì»¬ë ˆì´ì…˜ì„ ì‹œì‘í•˜ì„¸ìš”",
      "ë°±ì—… ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”",
      "ê´€ë ¨ íŒ€ì— ìƒí™©ì„ ê³µìœ í•˜ì„¸ìš”"
    ];
  }

  private extractReasoningSteps(): string[] {
    const currentFlow = langGraphProcessor.getCurrentFlow();
    if (!currentFlow) return [];
    
    return currentFlow.logic_steps.map(step => `${step.step}. ${step.title}`);
  }

  // ì¶”ê°€ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
  private calculateSeverityDistribution(alerts: any[]): Record<number, number> {
    return alerts.reduce((dist, alert) => {
      const severity = Number(alert.severity);
      dist[severity] = (dist[severity] || 0) + 1;
      return dist;
    }, {} as Record<number, number>);
  }

  private identifyTopIssues(alerts: any[]): string[] {
    const issueTypes = alerts.reduce((types, alert) => {
      types[alert.type] = (types[alert.type] || 0) + 1;
      return types;
    }, {} as Record<string, number>);
    
    return Object.entries(issueTypes)
      .sort(([,a], [,b]) => (b as number) - (a as number))
      .slice(0, 5)
      .map(([type]) => type);
  }

  private predictSystemLoad(servers: any[]): string {
    const avgLoad = servers.reduce((sum, s) => sum + s.cpu_usage, 0) / servers.length;
    
    if (avgLoad > 80) return 'HIGH';
    if (avgLoad > 60) return 'MEDIUM';
    return 'LOW';
  }

  private identifyPotentialIssues(servers: any[]): string[] {
    const issues = [];
    
    const highCpuServers = servers.filter(s => s.cpu_usage > 85);
    if (highCpuServers.length > 0) {
      issues.push(`${highCpuServers.length}ê°œ ì„œë²„ì—ì„œ CPU ê³¼ë¶€í•˜ ì˜ˆìƒ`);
    }
    
    const highMemoryServers = servers.filter(s => s.memory_usage > 90);
    if (highMemoryServers.length > 0) {
      issues.push(`${highMemoryServers.length}ê°œ ì„œë²„ì—ì„œ ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜ˆìƒ`);
    }
    
    return issues;
  }

  private generatePredictiveRecommendations(servers: any[]): string[] {
    return [
      "ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ íŠ¸ë Œë“œë¥¼ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ì„¸ìš”",
      "í”¼í¬ ì‹œê°„ëŒ€ ëŒ€ë¹„ ì¶”ê°€ ìš©ëŸ‰ì„ ì¤€ë¹„í•˜ì„¸ìš”",
      "ìë™ ìŠ¤ì¼€ì¼ë§ ì •ì±…ì„ ê²€í† í•˜ì„¸ìš”"
    ];
  }
}

// ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
export const mcpLangGraphAgent = MCPLangGraphAgent.getInstance(); 