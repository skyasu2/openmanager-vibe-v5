/**
 * ğŸ—„ï¸ ML ë°ì´í„° ë§¤ë‹ˆì €
 * 
 * ë¬´ë£Œ í‹°ì–´ ìµœì í™”ë¥¼ ìœ„í•œ ê³µí†µ ë°ì´í„° ë ˆì´ì–´
 * - Redis ìºì‹± ì „ëµ
 * - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
 * - ì§€ì—° ë¡œë”©
 * - ë°ì´í„° ì •ê·œí™”
 */

import redis from '@/lib/redis';
import { systemLogger as logger } from '@/lib/logger';
import type { ServerInstance } from '@/types/data-generator';
import type { ServerMetrics } from '@/services/ai/AnomalyDetection';

// ìºì‹± ì„¤ì •
const CACHE_DURATION = {
  PATTERN_ANALYSIS: 5 * 60, // 5ë¶„
  ANOMALY_DETECTION: 2 * 60, // 2ë¶„
  PREDICTION: 30 * 60, // 30ë¶„
  INCIDENT_REPORT: 10 * 60, // 10ë¶„
  SERVER_METRICS: 60, // 1ë¶„
} as const;

// ë°°ì¹˜ í¬ê¸°
const BATCH_SIZE = {
  SERVER_METRICS: 10,
  HISTORY_DATA: 100,
  PATTERN_ANALYSIS: 20,
} as const;

export interface MLDataCache<T> {
  data: T;
  timestamp: number;
  ttl: number;
}

export interface BatchProcessingOptions {
  batchSize?: number;
  parallel?: boolean;
  timeout?: number;
}

export interface MLAnalysisResult {
  patterns: any[];
  anomalies: any[];
  predictions: any[];
  confidence: number;
  timestamp: Date;
}

export class MLDataManager {
  private static instance: MLDataManager;
  private memoryCache: Map<string, MLDataCache<any>> = new Map();
  private pendingBatches: Map<string, any[]> = new Map();
  private batchTimers: Map<string, NodeJS.Timeout> = new Map();

  static getInstance(): MLDataManager {
    if (!this.instance) {
      this.instance = new MLDataManager();
    }
    return this.instance;
  }

  /**
   * ğŸ¯ ìºì‹±ëœ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
   */
  async getCachedData<T>(
    key: string,
    ttl: number = CACHE_DURATION.SERVER_METRICS
  ): Promise<T | null> {
    try {
      // ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
      const memCached = this.memoryCache.get(key);
      if (memCached && Date.now() - memCached.timestamp < memCached.ttl * 1000) {
        logger.info(`âœ… Memory cache hit: ${key}`);
        return memCached.data;
      }

      // Redis ìºì‹œ í™•ì¸
      if (redis) {
        const cached = await redis.get(key);
        if (cached) {
          const data = JSON.parse(cached);
          // ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
          this.memoryCache.set(key, {
            data,
            timestamp: Date.now(),
            ttl,
          });
          logger.info(`âœ… Redis cache hit: ${key}`);
          return data;
        }
      }

      return null;
    } catch (error) {
      logger.error(`ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜ (${key}):`, error);
      return null;
    }
  }

  /**
   * ğŸ¯ ë°ì´í„° ìºì‹±
   */
  async setCachedData<T>(
    key: string,
    data: T,
    ttl: number = CACHE_DURATION.SERVER_METRICS
  ): Promise<void> {
    try {
      // ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
      this.memoryCache.set(key, {
        data,
        timestamp: Date.now(),
        ttl,
      });

      // Redis ìºì‹œ ì €ì¥
      if (redis) {
        await redis.set(key, JSON.stringify(data), { ex: ttl });
      }

      logger.info(`âœ… Data cached: ${key} (TTL: ${ttl}s)`);
    } catch (error) {
      logger.error(`ìºì‹œ ì €ì¥ ì˜¤ë¥˜ (${key}):`, error);
    }
  }

  /**
   * ğŸ¯ ì„œë²„ ë©”íŠ¸ë¦­ ë°°ì¹˜ ì²˜ë¦¬
   */
  async processServerMetricsBatch(
    servers: ServerInstance[],
    options: BatchProcessingOptions = {}
  ): Promise<ServerMetrics[]> {
    const {
      batchSize = BATCH_SIZE.SERVER_METRICS,
      parallel = true,
    } = options;

    const results: ServerMetrics[] = [];
    const batches = this.createBatches(servers, batchSize);

    if (parallel) {
      // ë³‘ë ¬ ì²˜ë¦¬
      const batchPromises = batches.map(batch => this.transformServerBatch(batch));
      const batchResults = await Promise.all(batchPromises);
      batchResults.forEach(result => results.push(...result));
    } else {
      // ìˆœì°¨ ì²˜ë¦¬
      for (const batch of batches) {
        const batchResult = await this.transformServerBatch(batch);
        results.push(...batchResult);
      }
    }

    // ê²°ê³¼ ìºì‹±
    await this.setCachedData(
      'ml:server-metrics:latest',
      results,
      CACHE_DURATION.SERVER_METRICS
    );

    return results;
  }

  /**
   * ğŸ¯ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ìºì‹±
   */
  async cachePatternAnalysis(
    analysisId: string,
    result: MLAnalysisResult
  ): Promise<void> {
    const key = `ml:pattern:${analysisId}`;
    await this.setCachedData(key, result, CACHE_DURATION.PATTERN_ANALYSIS);
  }

  /**
   * ğŸ¯ ì´ìƒê°ì§€ ê²°ê³¼ ìºì‹±
   */
  async cacheAnomalyDetection(
    serverId: string,
    anomalies: any[]
  ): Promise<void> {
    const key = `ml:anomaly:${serverId}`;
    await this.setCachedData(
      key,
      {
        anomalies,
        timestamp: new Date(),
        count: anomalies.length,
      },
      CACHE_DURATION.ANOMALY_DETECTION
    );
  }

  /**
   * ğŸ¯ ì˜ˆì¸¡ ê²°ê³¼ ìºì‹±
   */
  async cachePrediction(
    predictionId: string,
    result: any
  ): Promise<void> {
    const key = `ml:prediction:${predictionId}`;
    await this.setCachedData(key, result, CACHE_DURATION.PREDICTION);
  }

  /**
   * ğŸ¯ ì¥ì•  ë³´ê³ ì„œ ìºì‹±
   */
  async cacheIncidentReport(
    reportId: string,
    report: any
  ): Promise<void> {
    const key = `ml:incident-report:${reportId}`;
    await this.setCachedData(key, report, CACHE_DURATION.INCIDENT_REPORT);
  }

  /**
   * ğŸ¯ íˆìŠ¤í† ë¦¬ ë°ì´í„° ì¼ê´„ ì¡°íšŒ
   */
  async getHistoricalData(
    serverId: string,
    hours: number = 24
  ): Promise<ServerMetrics[]> {
    const cacheKey = `ml:history:${serverId}:${hours}h`;
    
    // ìºì‹œ í™•ì¸
    const cached = await this.getCachedData<ServerMetrics[]>(cacheKey);
    if (cached) {
      return cached;
    }

    // ì‹¤ì œ ë°ì´í„° ì¡°íšŒ (ì‹œë®¬ë ˆì´ì…˜)
    const historicalData = this.generateMockHistoricalData(serverId, hours);
    
    // ìºì‹±
    await this.setCachedData(
      cacheKey,
      historicalData,
      Math.min(hours * 60, CACHE_DURATION.PREDICTION)
    );

    return historicalData;
  }

  /**
   * ğŸ¯ ì§€ì—° ë°°ì¹˜ ì¶”ê°€
   */
  async addToBatch(
    batchId: string,
    item: any,
    processFn: (batch: any[]) => Promise<void>,
    delayMs: number = 1000
  ): Promise<void> {
    // ë°°ì¹˜ì— ì¶”ê°€
    if (!this.pendingBatches.has(batchId)) {
      this.pendingBatches.set(batchId, []);
    }
    this.pendingBatches.get(batchId)!.push(item);

    // ê¸°ì¡´ íƒ€ì´ë¨¸ ì·¨ì†Œ
    const existingTimer = this.batchTimers.get(batchId);
    if (existingTimer) {
      clearTimeout(existingTimer);
    }

    // ìƒˆ íƒ€ì´ë¨¸ ì„¤ì •
    const timer = setTimeout(async () => {
      const batch = this.pendingBatches.get(batchId) || [];
      if (batch.length > 0) {
        this.pendingBatches.delete(batchId);
        this.batchTimers.delete(batchId);
        await processFn(batch);
      }
    }, delayMs);

    this.batchTimers.set(batchId, timer);
  }

  /**
   * ğŸ¯ ë°ì´í„° ì •ê·œí™”
   */
  normalizeServerData(server: ServerInstance): ServerMetrics {
    return {
      id: server.id,
      hostname: server.name,
      cpu_usage: server.cpu || 0,
      memory_usage: server.memory || 0,
      disk_usage: server.disk || 0,
      response_time: server.requests?.averageTime || 0,
      status: server.status || 'unknown',
      uptime: server.uptime || 0,
      timestamp: server.lastUpdated || new Date().toISOString(),
    };
  }

  /**
   * ğŸ¯ ìºì‹œ í†µê³„
   */
  async getCacheStats(): Promise<{
    memorySize: number;
    hitRate: number;
    keys: string[];
  }> {
    const keys = Array.from(this.memoryCache.keys());
    return {
      memorySize: this.memoryCache.size,
      hitRate: 0.7, // ì¶”í›„ ì‹¤ì œ ê³„ì‚°
      keys,
    };
  }

  /**
   * ğŸ¯ ìºì‹œ ì •ë¦¬
   */
  async clearExpiredCache(): Promise<void> {
    const now = Date.now();
    for (const [key, cache] of this.memoryCache.entries()) {
      if (now - cache.timestamp > cache.ttl * 1000) {
        this.memoryCache.delete(key);
      }
    }
    logger.info(`âœ… Expired cache cleared. Remaining: ${this.memoryCache.size}`);
  }

  // === Private í—¬í¼ ë©”ì†Œë“œ ===

  private createBatches<T>(items: T[], batchSize: number): T[][] {
    const batches: T[][] = [];
    for (let i = 0; i < items.length; i += batchSize) {
      batches.push(items.slice(i, i + batchSize));
    }
    return batches;
  }

  private async transformServerBatch(servers: ServerInstance[]): Promise<ServerMetrics[]> {
    return servers.map(server => this.normalizeServerData(server));
  }

  private generateMockHistoricalData(serverId: string, hours: number): ServerMetrics[] {
    const data: ServerMetrics[] = [];
    const now = Date.now();
    const interval = 5 * 60 * 1000; // 5ë¶„ ê°„ê²©

    for (let i = 0; i < (hours * 60) / 5; i++) {
      data.push({
        id: serverId,
        hostname: `server-${serverId}`,
        cpu_usage: 50 + Math.random() * 40,
        memory_usage: 60 + Math.random() * 30,
        disk_usage: 70 + Math.random() * 20,
        response_time: 50 + Math.random() * 100,
        status: Math.random() > 0.9 ? 'warning' : 'healthy',
        uptime: 99 + Math.random(),
        timestamp: new Date(now - i * interval).toISOString(),
      });
    }

    return data.reverse();
  }

  /**
   * ğŸ¯ ML ì—”ì§„ ì§€ì—° ë¡œë”©
   */
  private mlEngineCache: Map<string, any> = new Map();

  async getMLEngine(engineType: 'anomaly' | 'prediction' | 'incident'): Promise<any> {
    if (this.mlEngineCache.has(engineType)) {
      return this.mlEngineCache.get(engineType);
    }

    let engine: any;
    switch (engineType) {
      case 'anomaly':
        const { AnomalyDetection } = await import('@/services/ai/AnomalyDetection');
        engine = AnomalyDetection.getInstance();
        break;
      case 'prediction':
        const { LightweightMLEngine } = await import('@/lib/ml/LightweightMLEngine');
        engine = new LightweightMLEngine();
        break;
      case 'incident':
        const { incidentReportService } = await import('@/services/ai/IncidentReportService');
        engine = incidentReportService;
        break;
    }

    this.mlEngineCache.set(engineType, engine);
    logger.info(`âœ… ML Engine loaded: ${engineType}`);
    return engine;
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ export
export const mlDataManager = MLDataManager.getInstance();