/**
 * LangGraph Model Configuration
 * ì¤‘ì•™ ì§‘ì¤‘ì‹ ëª¨ë¸ ì„¤ì • ê´€ë¦¬
 *
 * LangChain íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš© (LangGraph StateGraph í˜¸í™˜)
 * - @langchain/google-genai: Gemini ëª¨ë¸
 * - @langchain/groq: Groq (Llama) ëª¨ë¸
 */

import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { ChatGroq } from '@langchain/groq';
import { AI_MODELS } from '@/config/ai-engine';
import { requireAPIKey } from './errors';
import {
  type CircuitBreakerState,
  isModelHealthy,
  recordFailure,
  recordSuccess,
} from './state-definition';

// ============================================================================
// 1. Model Types
// ============================================================================

export type GeminiModel =
  | 'gemini-2.5-flash'
  | 'gemini-2.5-flash-lite'
  | 'gemini-2.5-pro';
export type GroqModel = 'llama-3.1-8b-instant' | 'llama-3.3-70b-versatile';

export interface ModelOptions {
  temperature?: number;
  maxOutputTokens?: number;
}

// ============================================================================
// 2. Model Registry
// ============================================================================

/**
 * Agentë³„ ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
 */
export const AGENT_MODEL_CONFIG = {
  supervisor: {
    provider: 'groq' as const,
    model: 'llama-3.1-8b-instant' as GroqModel,
    temperature: 0.2,
    maxOutputTokens: 512,
  },
  nlq: {
    provider: 'google' as const,
    model: AI_MODELS.FLASH as GeminiModel,
    temperature: 0.3,
    maxOutputTokens: 1024,
  },
  analyst: {
    provider: 'google' as const,
    model: AI_MODELS.PRO as GeminiModel,
    temperature: 0.2,
    maxOutputTokens: 2048,
  },
  reporter: {
    provider: 'groq' as const,
    model: 'llama-3.3-70b-versatile' as GroqModel,
    temperature: 0.3,
    maxOutputTokens: 4096,
  },
} as const;

export type AgentType = keyof typeof AGENT_MODEL_CONFIG;

// ============================================================================
// 3. Model Factory Functions
// ============================================================================

/**
 * Google Gemini ëª¨ë¸ ìƒì„±
 * @throws {APIKeyMissingError} API í‚¤ê°€ ì—†ì„ ê²½ìš°
 */
export function createGeminiModel(
  model: GeminiModel = AI_MODELS.FLASH,
  options?: ModelOptions
): ChatGoogleGenerativeAI {
  const apiKey = requireAPIKey(
    'Google AI',
    'GEMINI_API_KEY_PRIMARY',
    'GOOGLE_API_KEY'
  );

  return new ChatGoogleGenerativeAI({
    apiKey,
    model,
    temperature: options?.temperature ?? 0.3,
    maxOutputTokens: options?.maxOutputTokens ?? 1024,
  });
}

/**
 * Groq (Llama) ëª¨ë¸ ìƒì„±
 * @throws {APIKeyMissingError} API í‚¤ê°€ ì—†ì„ ê²½ìš°
 */
export function createGroqModel(
  model: GroqModel = 'llama-3.1-8b-instant',
  options?: ModelOptions
): ChatGroq {
  const apiKey = requireAPIKey('Groq', 'GROQ_API_KEY');

  return new ChatGroq({
    apiKey,
    model,
    temperature: options?.temperature ?? 0.2,
    maxTokens: options?.maxOutputTokens ?? 1024,
  });
}

// ============================================================================
// 4. Agent-Specific Model Getters
// ============================================================================

/**
 * ì—ì´ì „íŠ¸ íƒ€ì…ì— ë§ëŠ” ëª¨ë¸ ìƒì„±
 */
export function getModelForAgent(
  agentType: AgentType
): ChatGoogleGenerativeAI | ChatGroq {
  const config = AGENT_MODEL_CONFIG[agentType];

  if (config.provider === 'google') {
    return createGeminiModel(config.model as GeminiModel, {
      temperature: config.temperature,
      maxOutputTokens: config.maxOutputTokens,
    });
  }

  return createGroqModel(config.model as GroqModel, {
    temperature: config.temperature,
    maxOutputTokens: config.maxOutputTokens,
  });
}

/**
 * Supervisor Agentìš© ëª¨ë¸
 */
export function getSupervisorModel(): ChatGroq {
  const config = AGENT_MODEL_CONFIG.supervisor;
  return createGroqModel(config.model, {
    temperature: config.temperature,
    maxOutputTokens: config.maxOutputTokens,
  });
}

/**
 * NLQ Agentìš© ëª¨ë¸
 */
export function getNLQModel(): ChatGoogleGenerativeAI {
  const config = AGENT_MODEL_CONFIG.nlq;
  return createGeminiModel(config.model as GeminiModel, {
    temperature: config.temperature,
    maxOutputTokens: config.maxOutputTokens,
  });
}

/**
 * Analyst Agentìš© ëª¨ë¸
 */
export function getAnalystModel(): ChatGoogleGenerativeAI {
  const config = AGENT_MODEL_CONFIG.analyst;
  return createGeminiModel(config.model as GeminiModel, {
    temperature: config.temperature,
    maxOutputTokens: config.maxOutputTokens,
  });
}

/**
 * Reporter Agentìš© ëª¨ë¸
 */
export function getReporterModel(): ChatGroq {
  const config = AGENT_MODEL_CONFIG.reporter;
  return createGroqModel(config.model, {
    temperature: config.temperature,
    maxOutputTokens: config.maxOutputTokens,
  });
}

// ============================================================================
// 5. API Key Validation
// ============================================================================

/**
 * ëª¨ë“  í•„ìˆ˜ API í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
 * ì‹œì‘ ì‹œì ì— í˜¸ì¶œí•˜ì—¬ ì„¤ì • ê²€ì¦
 */
export function validateAPIKeys(): {
  google: boolean;
  groq: boolean;
  all: boolean;
} {
  const googleKey =
    process.env.GEMINI_API_KEY_PRIMARY || process.env.GOOGLE_API_KEY;
  const groqKey = process.env.GROQ_API_KEY;

  return {
    google: !!googleKey,
    groq: !!groqKey,
    all: !!googleKey && !!groqKey,
  };
}

/**
 * API í‚¤ ìƒíƒœ ë¡œê¹… (ê°œë°œìš©)
 */
export function logAPIKeyStatus(): void {
  const status = validateAPIKeys();
  console.log('ğŸ”‘ API Key Status:', {
    'Google AI': status.google ? 'âœ…' : 'âŒ',
    Groq: status.groq ? 'âœ…' : 'âŒ',
  });
}

// ============================================================================
// 6. Circuit Breaker Integration
// ============================================================================

/**
 * í´ë°± ì²´ì¸ ì •ì˜
 * ê° ëª¨ë¸ì´ ì‹¤íŒ¨í•  ê²½ìš° ë‹¤ìŒ ëª¨ë¸ë¡œ ì „í™˜
 */
export const MODEL_FALLBACK_CHAIN: Record<string, string[]> = {
  // Gemini Flash â†’ Pro â†’ Groq Llama
  'gemini-2.5-flash': ['gemini-2.5-pro', 'llama-3.3-70b-versatile'],
  'gemini-2.5-flash-lite': ['gemini-2.5-flash', 'llama-3.3-70b-versatile'],
  'gemini-2.5-pro': ['gemini-2.5-flash', 'llama-3.3-70b-versatile'],
  // Groq Llama â†’ Gemini
  'llama-3.1-8b-instant': ['llama-3.3-70b-versatile', 'gemini-2.5-flash'],
  'llama-3.3-70b-versatile': ['gemini-2.5-pro', 'gemini-2.5-flash'],
};

/**
 * ê±´ê°•í•œ ëª¨ë¸ ì„ íƒ (Circuit Breaker ì ìš©)
 * @param preferredModel ì„ í˜¸ ëª¨ë¸
 * @param circuitState Circuit Breaker ìƒíƒœ
 * @returns ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ID
 */
export function selectHealthyModel(
  preferredModel: string,
  circuitState: CircuitBreakerState
): string {
  // ì„ í˜¸ ëª¨ë¸ì´ ê±´ê°•í•˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
  if (isModelHealthy(circuitState, preferredModel)) {
    return preferredModel;
  }

  console.log(
    `âš ï¸ [Circuit Breaker] ${preferredModel} is unhealthy, trying fallback`
  );

  // í´ë°± ì²´ì¸ì—ì„œ ê±´ê°•í•œ ëª¨ë¸ ì°¾ê¸°
  const fallbackChain = MODEL_FALLBACK_CHAIN[preferredModel] || [];
  for (const fallbackModel of fallbackChain) {
    if (isModelHealthy(circuitState, fallbackModel)) {
      console.log(`âœ… [Circuit Breaker] Using fallback: ${fallbackModel}`);
      return fallbackModel;
    }
  }

  // ëª¨ë“  ëª¨ë¸ì´ unhealthyë©´ ì„ í˜¸ ëª¨ë¸ë¡œ ë‹¤ì‹œ ì‹œë„ (half-open)
  console.log(
    `âš ï¸ [Circuit Breaker] All fallbacks unhealthy, retrying ${preferredModel}`
  );
  return preferredModel;
}

/**
 * Circuit Breaker ë˜í¼ë¡œ ëª¨ë¸ í˜¸ì¶œ
 * ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ í´ë°± ëª¨ë¸ ì‹œë„
 */
export async function invokeWithCircuitBreaker<T>(
  modelId: string,
  circuitState: CircuitBreakerState,
  invoker: (model: ChatGoogleGenerativeAI | ChatGroq) => Promise<T>
): Promise<{
  result: T;
  usedModel: string;
  newCircuitState: CircuitBreakerState;
}> {
  const healthyModel = selectHealthyModel(modelId, circuitState);
  let currentCircuitState = circuitState;
  let lastError: Error | null = null;

  // ì„ íƒëœ ëª¨ë¸ê³¼ í´ë°± ì²´ì¸ ì‹œë„
  const modelsToTry = [
    healthyModel,
    ...(MODEL_FALLBACK_CHAIN[healthyModel] || []),
  ];

  for (const tryModel of modelsToTry) {
    if (
      !isModelHealthy(currentCircuitState, tryModel) &&
      tryModel !== modelsToTry[0]
    ) {
      continue; // ì´ë¯¸ unhealthyí•œ í´ë°±ì€ ê±´ë„ˆë›°ê¸°
    }

    try {
      const model = createModelByName(tryModel);
      const result = await invoker(model);

      // ì„±ê³µ ì‹œ Circuit Breaker ìƒíƒœ ì—…ë°ì´íŠ¸
      currentCircuitState = recordSuccess(currentCircuitState, tryModel);
      console.log(`âœ… [Circuit Breaker] ${tryModel} succeeded`);

      return {
        result,
        usedModel: tryModel,
        newCircuitState: currentCircuitState,
      };
    } catch (error) {
      lastError = error instanceof Error ? error : new Error(String(error));
      currentCircuitState = recordFailure(currentCircuitState, tryModel);
      console.error(
        `âŒ [Circuit Breaker] ${tryModel} failed:`,
        lastError.message
      );
    }
  }

  // ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨
  throw lastError || new Error('All models in fallback chain failed');
}

/**
 * ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
 */
function createModelByName(
  modelName: string
): ChatGoogleGenerativeAI | ChatGroq {
  if (modelName.startsWith('gemini')) {
    return createGeminiModel(modelName as GeminiModel);
  }
  if (modelName.startsWith('llama')) {
    return createGroqModel(modelName as GroqModel);
  }
  throw new Error(`Unknown model: ${modelName}`);
}
