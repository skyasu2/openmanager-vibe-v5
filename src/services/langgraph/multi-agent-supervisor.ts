/**
 * Multi-Agent Supervisor using @langchain/langgraph-supervisor
 *
 * Features:
 * - createSupervisor: Automatic routing between worker agents
 * - createReactAgent: Tool-equipped worker agents
 * - A2A Communication: Automatic handoffs between agents
 * - HITL: Human-in-the-Loop for critical actions
 *
 * @see https://langchain-ai.github.io/langgraphjs/reference/functions/langgraph_supervisor.createSupervisor.html
 */

import { HumanMessage } from '@langchain/core/messages';
import { tool } from '@langchain/core/tools';
import { createReactAgent } from '@langchain/langgraph/prebuilt';
import { createSupervisor } from '@langchain/langgraph-supervisor';
import { z } from 'zod';
import {
  getAnomalyDetector,
  type MetricDataPoint,
} from '@/lib/ai/monitoring/SimpleAnomalyDetector';
import {
  getTrendPredictor,
  type TrendDataPoint,
} from '@/lib/ai/monitoring/TrendPredictor';
import { createClient } from '@/lib/supabase/server';
import { SupabaseRAGEngine } from '@/services/ai/supabase-rag-engine';
import { loadHourlyScenarioData } from '@/services/scenario/scenario-loader';
import { createSessionConfig, getAutoCheckpointer } from './checkpointer';
import {
  getAnalystModel,
  getNLQModel,
  getReporterModel,
  getSupervisorModel,
} from './model-config';

// ============================================================================
// 1. Tool Definitions (Re-exported from agents for reuse)
// ============================================================================

// --- NLQ Agent Tools ---
const getServerMetricsTool = tool(
  async ({ serverId, metric: _metric }) => {
    const allServers = await loadHourlyScenarioData();
    const target = serverId
      ? allServers.find((s) => s.id === serverId)
      : allServers;

    const servers = Array.isArray(target)
      ? target
      : target
        ? [target]
        : allServers;

    return {
      success: true,
      servers: servers.map((s) => ({
        id: s.id,
        name: s.name,
        status: s.status,
        cpu: s.cpu,
        memory: s.memory,
        disk: s.disk,
      })),
      summary: {
        total: servers.length,
        alertCount: servers.filter(
          (s) => s.status === 'warning' || s.status === 'critical'
        ).length,
      },
      timestamp: new Date().toISOString(),
      _dataSource: 'scenario-loader',
    };
  },
  {
    name: 'getServerMetrics',
    description:
      'ì„œë²„ CPU/ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ (ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜)',
    schema: z.object({
      serverId: z.string().optional().describe('ì¡°íšŒí•  ì„œë²„ ID (ì„ íƒ)'),
      metric: z
        .enum(['cpu', 'memory', 'disk', 'all'])
        .describe('ì¡°íšŒí•  ë©”íŠ¸ë¦­ íƒ€ì…'),
    }),
  }
);

// --- Analyst Agent Tools ---
function generateSimulatedHistory(
  currentValue: number,
  pointCount: number = 24
): MetricDataPoint[] {
  const now = Date.now();
  const interval = 5 * 60 * 1000;
  const history: MetricDataPoint[] = [];

  for (let i = pointCount - 1; i >= 0; i--) {
    const variance = currentValue * 0.15;
    const randomOffset = (Math.random() - 0.5) * 2 * variance;
    const value = Math.max(0, Math.min(100, currentValue + randomOffset));
    history.push({ timestamp: now - i * interval, value });
  }

  return history;
}

function toTrendDataPoints(metricPoints: MetricDataPoint[]): TrendDataPoint[] {
  return metricPoints.map((p) => ({ timestamp: p.timestamp, value: p.value }));
}

const detectAnomaliesTool = tool(
  async ({ serverId, metricType }) => {
    const allServers = await loadHourlyScenarioData();
    const server = serverId
      ? allServers.find((s) => s.id === serverId)
      : allServers[0];

    if (!server) {
      return { success: false, error: 'ì„œë²„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' };
    }

    const detector = getAnomalyDetector();
    const metrics = ['cpu', 'memory', 'disk'] as const;
    const targetMetrics =
      metricType === 'all' ? metrics : [metricType as (typeof metrics)[number]];

    const results: Record<
      string,
      {
        isAnomaly: boolean;
        severity: string;
        confidence: number;
        currentValue: number;
        threshold: { upper: number; lower: number };
      }
    > = {};

    for (const metric of targetMetrics) {
      const currentValue = server[metric];
      const history = generateSimulatedHistory(currentValue, 24);
      const detection = detector.detectAnomaly(currentValue, history);

      results[metric] = {
        isAnomaly: detection.isAnomaly,
        severity: detection.severity,
        confidence: Math.round(detection.confidence * 100) / 100,
        currentValue,
        threshold: {
          upper: Math.round(detection.details.upperThreshold * 100) / 100,
          lower: Math.round(detection.details.lowerThreshold * 100) / 100,
        },
      };
    }

    const anomalyCount = Object.values(results).filter(
      (r) => r.isAnomaly
    ).length;

    return {
      success: true,
      serverId: server.id,
      serverName: server.name,
      anomalyCount,
      hasAnomalies: anomalyCount > 0,
      results,
      timestamp: new Date().toISOString(),
      _algorithm: '26-hour moving average + 2Ïƒ threshold',
    };
  },
  {
    name: 'detectAnomalies',
    description:
      'ì„œë²„ ë©”íŠ¸ë¦­ì˜ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤ (í†µê³„ì  ì´ìƒê°ì§€: 26ì‹œê°„ ì´ë™í‰ê·  + 2Ïƒ)',
    schema: z.object({
      serverId: z
        .string()
        .optional()
        .describe('ë¶„ì„í•  ì„œë²„ ID (ì„ íƒ, ë¯¸ì…ë ¥ì‹œ ì²« ë²ˆì§¸ ì„œë²„)'),
      metricType: z
        .enum(['cpu', 'memory', 'disk', 'all'])
        .default('all')
        .describe('ë¶„ì„í•  ë©”íŠ¸ë¦­ íƒ€ì…'),
    }),
  }
);

const predictTrendsTool = tool(
  async ({ serverId, metricType, predictionHours }) => {
    const allServers = await loadHourlyScenarioData();
    const server = serverId
      ? allServers.find((s) => s.id === serverId)
      : allServers[0];

    if (!server) {
      return { success: false, error: 'ì„œë²„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' };
    }

    const predictor = getTrendPredictor();
    const metrics = ['cpu', 'memory', 'disk'] as const;
    const targetMetrics =
      metricType === 'all' ? metrics : [metricType as (typeof metrics)[number]];
    const horizon = (predictionHours ?? 1) * 3600 * 1000;

    const results: Record<
      string,
      {
        trend: string;
        currentValue: number;
        predictedValue: number;
        changePercent: number;
        confidence: number;
      }
    > = {};

    for (const metric of targetMetrics) {
      const currentValue = server[metric];
      const history = toTrendDataPoints(
        generateSimulatedHistory(currentValue, 12)
      );
      const prediction = predictor.predictTrend(history, horizon);

      results[metric] = {
        trend: prediction.trend,
        currentValue,
        predictedValue: Math.round(prediction.prediction * 100) / 100,
        changePercent:
          Math.round(prediction.details.predictedChangePercent * 100) / 100,
        confidence: Math.round(prediction.confidence * 100) / 100,
      };
    }

    const increasingMetrics = Object.entries(results)
      .filter(([, r]) => r.trend === 'increasing')
      .map(([m]) => m);

    return {
      success: true,
      serverId: server.id,
      serverName: server.name,
      predictionHorizon: `${predictionHours ?? 1}ì‹œê°„`,
      results,
      summary: {
        increasingMetrics,
        hasRisingTrends: increasingMetrics.length > 0,
      },
      timestamp: new Date().toISOString(),
      _algorithm: 'Linear Regression with RÂ² confidence',
    };
  },
  {
    name: 'predictTrends',
    description:
      'ì„œë²„ ë©”íŠ¸ë¦­ì˜ íŠ¸ë Œë“œë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤ (ì„ í˜• íšŒê·€ ê¸°ë°˜ 1ì‹œê°„ ì˜ˆì¸¡)',
    schema: z.object({
      serverId: z
        .string()
        .optional()
        .describe('ë¶„ì„í•  ì„œë²„ ID (ì„ íƒ, ë¯¸ì…ë ¥ì‹œ ì²« ë²ˆì§¸ ì„œë²„)'),
      metricType: z
        .enum(['cpu', 'memory', 'disk', 'all'])
        .default('all')
        .describe('ë¶„ì„í•  ë©”íŠ¸ë¦­ íƒ€ì…'),
      predictionHours: z
        .number()
        .optional()
        .default(1)
        .describe('ì˜ˆì¸¡ ì‹œê°„ (ê¸°ë³¸ 1ì‹œê°„)'),
    }),
  }
);

const analyzePatternTool = tool(
  async ({ query }) => {
    const patterns: string[] = [];
    const q = query.toLowerCase();

    if (/cpu|í”„ë¡œì„¸ì„œ|ì„±ëŠ¥/i.test(q)) patterns.push('system_performance');
    if (/ë©”ëª¨ë¦¬|ram|memory/i.test(q)) patterns.push('memory_status');
    if (/ë””ìŠ¤í¬|ì €ì¥ì†Œ|ìš©ëŸ‰/i.test(q)) patterns.push('storage_info');
    if (/ì„œë²„|ì‹œìŠ¤í…œ|ìƒíƒœ/i.test(q)) patterns.push('server_status');
    if (/íŠ¸ë Œë“œ|ì¶”ì„¸|ì˜ˆì¸¡/i.test(q)) patterns.push('trend_analysis');
    if (/ì´ìƒ|anomaly|alert/i.test(q)) patterns.push('anomaly_detection');

    if (patterns.length === 0) {
      return { success: false, message: 'ë§¤ì¹­ë˜ëŠ” íŒ¨í„´ ì—†ìŒ' };
    }

    const insights: Record<string, string> = {
      system_performance:
        'ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„: CPU ì‚¬ìš©ë¥ , í”„ë¡œì„¸ìŠ¤ ìˆ˜, ë¡œë“œ í‰ê·  í™•ì¸ í•„ìš”',
      memory_status: 'ë©”ëª¨ë¦¬ ìƒíƒœ ë¶„ì„: ì‚¬ìš©ëŸ‰, ìºì‹œ, ìŠ¤ì™‘ ì‚¬ìš©ë¥  í™•ì¸ í•„ìš”',
      storage_info:
        'ìŠ¤í† ë¦¬ì§€ ë¶„ì„: ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰, I/O ëŒ€ê¸°, íŒŒí‹°ì…˜ ìƒíƒœ í™•ì¸ í•„ìš”',
      server_status:
        'ì„œë²„ ìƒíƒœ ë¶„ì„: ê°€ë™ ì‹œê°„, ì„œë¹„ìŠ¤ ìƒíƒœ, ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸',
      trend_analysis:
        'íŠ¸ë Œë“œ ë¶„ì„: ì‹œê³„ì—´ ë°ì´í„° ê¸°ë°˜ íŒ¨í„´ ì¸ì‹ ë° ì˜ˆì¸¡ ëª¨ë¸ ì ìš©',
      anomaly_detection: 'ì´ìƒ íƒì§€: í†µê³„ì  ì´ìƒì¹˜ ê°ì§€, ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼ í™•ì¸',
    };

    const analysisResults = patterns.map((pattern) => ({
      pattern,
      confidence: 0.8 + Math.random() * 0.2,
      insights: insights[pattern] || 'ì¼ë°˜ ë¶„ì„ ìˆ˜í–‰',
    }));

    return {
      success: true,
      patterns,
      detectedIntent: patterns[0],
      analysisResults,
      _mode: 'pattern-analysis',
    };
  },
  {
    name: 'analyzePattern',
    description: 'ì‚¬ìš©ì ì§ˆë¬¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ì‹œìŠ¤í…œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤',
    schema: z.object({
      query: z.string().describe('ë¶„ì„í•  ì‚¬ìš©ì ì§ˆë¬¸'),
    }),
  }
);

// --- Reporter Agent Tools ---
const searchKnowledgeBaseTool = tool(
  async ({ query }) => {
    try {
      const supabase = await createClient();
      const ragEngine = new SupabaseRAGEngine(supabase);

      const searchResult = await ragEngine.searchHybrid(query, {
        maxResults: 5,
        enableKeywordFallback: true,
      });

      if (!searchResult.success || searchResult.results.length === 0) {
        return {
          success: false,
          message: 'ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
          _source: 'Supabase pgvector',
        };
      }

      return {
        success: true,
        results: searchResult.results.map((r) => ({
          content: r.content,
          similarity: r.similarity,
        })),
        totalFound: searchResult.results.length,
        _source: 'Supabase pgvector',
      };
    } catch (error) {
      console.error('âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨:', error);
      return {
        success: false,
        error: 'ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜',
        _source: 'Supabase pgvector',
      };
    }
  },
  {
    name: 'searchKnowledgeBase',
    description: 'ê³¼ê±° ì¥ì•  ì´ë ¥ ë° í•´ê²° ë°©ë²•ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤ (RAG)',
    schema: z.object({
      query: z.string().describe('ê²€ìƒ‰ ì¿¼ë¦¬'),
    }),
  }
);

const recommendCommandsTool = tool(
  async ({ keywords }) => {
    const recommendations = [
      {
        keywords: ['ì„œë²„', 'ëª©ë¡', 'ì¡°íšŒ'],
        command: 'list servers',
        description: 'ì„œë²„ ëª©ë¡ ì¡°íšŒ',
      },
      {
        keywords: ['ìƒíƒœ', 'ì²´í¬', 'í™•ì¸'],
        command: 'status check',
        description: 'ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€',
      },
      {
        keywords: ['ë¡œê·¸', 'ë¶„ì„', 'ì—ëŸ¬'],
        command: 'analyze logs',
        description: 'ë¡œê·¸ ë¶„ì„',
      },
      {
        keywords: ['ì¬ì‹œì‘', 'restart', 'ë³µêµ¬'],
        command: 'service restart <service_name>',
        description: 'ì„œë¹„ìŠ¤ ì¬ì‹œì‘',
      },
      {
        keywords: ['ë©”ëª¨ë¦¬', 'ì •ë¦¬', 'cache'],
        command: 'clear cache',
        description: 'ìºì‹œ ì •ë¦¬',
      },
    ];

    const matched = recommendations.filter((rec) =>
      keywords.some((k) =>
        rec.keywords.some((rk) => rk.includes(k) || k.includes(rk))
      )
    );

    return {
      success: true,
      recommendations:
        matched.length > 0 ? matched : recommendations.slice(0, 3),
      _mode: 'command-recommendation',
    };
  },
  {
    name: 'recommendCommands',
    description: 'ì‚¬ìš©ì ì§ˆë¬¸ì— ì í•©í•œ CLI ëª…ë ¹ì–´ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤',
    schema: z.object({
      keywords: z.array(z.string()).describe('ì§ˆë¬¸ì—ì„œ ì¶”ì¶œí•œ í•µì‹¬ í‚¤ì›Œë“œ'),
    }),
  }
);

// ============================================================================
// 2. Worker Agent Creation
// ============================================================================

/**
 * Create NLQ Agent - Server metrics queries
 */
function createNLQAgent() {
  return createReactAgent({
    llm: getNLQModel(),
    tools: [getServerMetricsTool],
    name: 'nlq_agent',
    prompt: `ë‹¹ì‹ ì€ OpenManager VIBEì˜ NLQ Agentì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìì—°ì–´ ì§ˆë¬¸ì„ ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ê°€ëŠ¥í•œ ì‘ì—…:
- ì„œë²„ ìƒíƒœ ì¡°íšŒ (CPU, Memory, Disk)
- íŠ¹ì • ì„œë²„ ë©”íŠ¸ë¦­ ì¡°íšŒ
- ì „ì²´ ì„œë²„ ìš”ì•½

ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•œ í›„, ê²°ê³¼ë¥¼ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`,
  });
}

/**
 * Create Analyst Agent - Pattern analysis & anomaly detection
 */
function createAnalystAgent() {
  return createReactAgent({
    llm: getAnalystModel(),
    tools: [detectAnomaliesTool, predictTrendsTool, analyzePatternTool],
    name: 'analyst_agent',
    prompt: `ë‹¹ì‹ ì€ OpenManager VIBEì˜ Analyst Agentì…ë‹ˆë‹¤.
ì„œë²„ ì‹œìŠ¤í…œ íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

ê°€ëŠ¥í•œ ì‘ì—…:
- ì´ìƒ íƒì§€ (detectAnomalies): í†µê³„ì  ì´ìƒì¹˜ ê°ì§€
- íŠ¸ë Œë“œ ì˜ˆì¸¡ (predictTrends): ì„ í˜• íšŒê·€ ê¸°ë°˜ ì˜ˆì¸¡
- íŒ¨í„´ ë¶„ì„ (analyzePattern): ì§ˆë¬¸ ì˜ë„ íŒŒì•…

ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
1. í˜„ì¬ ìƒíƒœ ìš”ì•½
2. ë°œê²¬ëœ íŒ¨í„´/ì´ìƒì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…
3. ì ì¬ì  ë¬¸ì œì  ë˜ëŠ” ì£¼ì˜ì‚¬í•­
4. ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`,
  });
}

/**
 * Create Reporter Agent - Incident reports & RAG
 */
function createReporterAgent() {
  return createReactAgent({
    llm: getReporterModel(),
    tools: [searchKnowledgeBaseTool, recommendCommandsTool],
    name: 'reporter_agent',
    prompt: `ë‹¹ì‹ ì€ OpenManager VIBEì˜ Reporter Agentì…ë‹ˆë‹¤.
ì¥ì•  ë¶„ì„ ë° ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ê°€ëŠ¥í•œ ì‘ì—…:
- ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰ (searchKnowledgeBase): RAG ê¸°ë°˜ ê³¼ê±° ì¥ì•  ì´ë ¥ ê²€ìƒ‰
- ëª…ë ¹ì–´ ì¶”ì²œ (recommendCommands): CLI ëª…ë ¹ì–´ ì¶”ì²œ

ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸ í˜•ì‹:
### ğŸ“‹ ì¸ì‹œë˜íŠ¸ ìš”ì•½
[ë¬¸ì œ ìƒí™© ìš”ì•½]

### ğŸ” ì›ì¸ ë¶„ì„
[ê°€ëŠ¥í•œ ì›ì¸ë“¤]

### ğŸ’¡ ê¶Œì¥ ì¡°ì¹˜
[ë‹¨ê³„ë³„ í•´ê²° ë°©ì•ˆ]

### âŒ¨ï¸ ì¶”ì²œ ëª…ë ¹ì–´
[ì‹¤í–‰ ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë“¤]

í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.`,
  });
}

// ============================================================================
// 3. Supervisor Creation
// ============================================================================

const SUPERVISOR_PROMPT = `ë‹¹ì‹ ì€ OpenManager VIBEì˜ Multi-Agent Supervisorì…ë‹ˆë‹¤.
ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤.

## ì—ì´ì „íŠ¸ ëª©ë¡
1. **nlq_agent**: ì„œë²„ ìƒíƒœ/ë©”íŠ¸ë¦­ ì¡°íšŒ (CPU, Memory, Disk)
   - ì˜ˆ: "ì„œë²„ ìƒíƒœ", "CPU ì‚¬ìš©ë¥ ", "ë©”ëª¨ë¦¬ í™•ì¸"

2. **analyst_agent**: íŒ¨í„´ ë¶„ì„, ì´ìƒ íƒì§€, íŠ¸ë Œë“œ ì˜ˆì¸¡
   - ì˜ˆ: "ì´ìƒ ê°ì§€", "íŠ¸ë Œë“œ ë¶„ì„", "íŒ¨í„´ í™•ì¸", "ì¢…í•© ë¶„ì„"

3. **reporter_agent**: ì¸ì‹œë˜íŠ¸ ë¦¬í¬íŠ¸, ì¥ì•  ë¶„ì„, RAG ê²€ìƒ‰
   - ì˜ˆ: "ì¥ì•  ë¶„ì„", "ì›ì¸ íŒŒì•…", "í•´ê²° ë°©ë²•", "ê³¼ê±° ì´ë ¥"

## ë¼ìš°íŒ… ê·œì¹™
- ë‹¨ìˆœ ì¡°íšŒ â†’ nlq_agent
- ë¶„ì„/ì˜ˆì¸¡ â†’ analyst_agent
- ì¥ì• /ë¦¬í¬íŠ¸ â†’ reporter_agent
- ì¸ì‚¬ë§/ì¼ë°˜ ëŒ€í™” â†’ ì§ì ‘ ì‘ë‹µ

ì ì ˆí•œ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ì—¬ ì‘ì—…ì„ ìœ„ì„í•˜ì„¸ìš”.`;

/**
 * Create Multi-Agent Supervisor Workflow
 */
export async function createMultiAgentSupervisor() {
  const checkpointer = await getAutoCheckpointer();

  // Create worker agents
  const nlqAgent = createNLQAgent();
  const analystAgent = createAnalystAgent();
  const reporterAgent = createReporterAgent();

  // Create supervisor with automatic handoffs
  const workflow = createSupervisor({
    agents: [nlqAgent, analystAgent, reporterAgent],
    llm: getSupervisorModel(),
    prompt: SUPERVISOR_PROMPT,
    outputMode: 'full_history',
  });

  // Compile with checkpointer for session persistence
  return workflow.compile({
    checkpointer,
  });
}

// ============================================================================
// 4. Execution Functions
// ============================================================================

export interface SupervisorExecutionOptions {
  sessionId?: string;
}

/**
 * Execute supervisor workflow (single response)
 */
export async function executeSupervisor(
  query: string,
  options: SupervisorExecutionOptions = {}
): Promise<{
  response: string;
  sessionId: string;
}> {
  const app = await createMultiAgentSupervisor();
  const sessionId = options.sessionId || `session_${Date.now()}`;
  const config = createSessionConfig(sessionId);

  const result = await app.invoke(
    {
      messages: [new HumanMessage(query)],
    },
    config
  );

  // Extract final response from messages
  const messages = result.messages || [];
  const lastMessage = messages[messages.length - 1];
  const response =
    typeof lastMessage?.content === 'string'
      ? lastMessage.content
      : 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';

  console.log(`âœ… [Supervisor] Completed. Session: ${sessionId}`);

  return { response, sessionId };
}

/**
 * Stream supervisor workflow
 */
export async function* streamSupervisor(
  query: string,
  options: SupervisorExecutionOptions = {}
): AsyncGenerator<{
  type: 'token' | 'agent_start' | 'agent_end' | 'final';
  content: string;
  metadata?: Record<string, unknown>;
}> {
  const app = await createMultiAgentSupervisor();
  const sessionId = options.sessionId || `session_${Date.now()}`;
  const config = createSessionConfig(sessionId);

  const stream = await app.streamEvents(
    {
      messages: [new HumanMessage(query)],
    },
    {
      version: 'v2',
      ...config,
    }
  );

  let finalContent = '';

  for await (const event of stream) {
    // LLM token streaming
    if (event.event === 'on_chat_model_stream') {
      const chunk = event.data?.chunk;
      if (chunk?.content && typeof chunk.content === 'string') {
        finalContent += chunk.content;
        yield {
          type: 'token',
          content: chunk.content,
          metadata: { node: event.name },
        };
      }
    }

    // Agent start
    if (event.event === 'on_chain_start' && event.tags?.includes('agent')) {
      yield {
        type: 'agent_start',
        content: event.name || 'unknown_agent',
        metadata: { tags: event.tags },
      };
    }

    // Agent end
    if (event.event === 'on_chain_end' && event.tags?.includes('agent')) {
      yield {
        type: 'agent_end',
        content: event.name || 'unknown_agent',
        metadata: { output: event.data?.output },
      };
    }
  }

  // Final response
  yield {
    type: 'final',
    content: finalContent,
    metadata: { sessionId },
  };
}

/**
 * Create AI SDK compatible streaming response using toUIMessageStream
 * This integrates LangGraph with Vercel AI SDK v5
 */
export async function createSupervisorStreamResponse(
  query: string,
  sessionId?: string
): Promise<ReadableStream<Uint8Array>> {
  const encoder = new TextEncoder();

  return new ReadableStream({
    async start(controller) {
      try {
        const generator = streamSupervisor(query, { sessionId });

        for await (const chunk of generator) {
          if (chunk.type === 'token') {
            // AI SDK v5 Data Stream Protocol: text part
            const dataStreamText = `0:${JSON.stringify(chunk.content)}\n`;
            controller.enqueue(encoder.encode(dataStreamText));
          } else if (chunk.type === 'final') {
            // AI SDK v5 Data Stream Protocol: finish message
            const finishMessage = `d:${JSON.stringify({ finishReason: 'stop' })}\n`;
            controller.enqueue(encoder.encode(finishMessage));
            console.log('ğŸ“¤ Supervisor stream completed (AI SDK v5 Protocol)');
          }
        }

        controller.close();
      } catch (error) {
        console.error('âŒ Supervisor streaming error:', error);
        const errorMessage =
          error instanceof Error ? error.message : 'Unknown error';
        const errorStream = `3:${JSON.stringify(errorMessage)}\n`;
        controller.enqueue(encoder.encode(errorStream));
        controller.close();
      }
    },
  });
}

// ============================================================================
// 5. Export Tools for External Use
// ============================================================================

export {
  getServerMetricsTool,
  detectAnomaliesTool,
  predictTrendsTool,
  analyzePatternTool,
  searchKnowledgeBaseTool,
  recommendCommandsTool,
};
