/**
 * üéØ ÌïòÏù¥Î∏åÎ¶¨Îìú AI ÏãúÏä§ÌÖú React Hook
 *
 * Î¨¥Î£å Ìã∞Ïñ¥ ÏµúÏ†ÅÌôîÎêú AI ÏöîÏ≤≠ Í¥ÄÎ¶¨
 * - ÏûêÎèô ÏöîÏ≤≠ Î∞∞Ïπò Ï≤òÎ¶¨
 * - Ïã§ÏãúÍ∞Ñ ÏÉùÍ∞ÅÏ§ë ÏÉÅÌÉú Ïä§Ìä∏Î¶¨Î∞ç
 * - Î°úÏª¨ Ï∫êÏã±ÏúºÎ°ú ÏöîÏ≤≠ ÏµúÏÜåÌôî
 */

import { useState, useCallback, useRef, useEffect } from 'react';
import { useSession } from 'next-auth/react';
import type { UnifiedAIResponse } from '@/services/ai/formatters/unified-response-formatter';
import type {
  ThinkingStep,
  AIServiceType,
} from '@/services/ai/interfaces/distributed-ai.interface';

// Hook ÏÑ§Ï†ï
const CONFIG = {
  apiEndpoint: '/api/ai/edge',
  streamEndpoint: '/api/ai/thinking/stream',
  localCacheTTL: 5 * 60 * 1000, // 5Î∂Ñ
  batchDelay: 100, // 100ms
  maxBatchSize: 3,
};

// Î°úÏª¨ Ï∫êÏãú ÌÉÄÏûÖ
interface CacheEntry {
  query: string;
  response: UnifiedAIResponse;
  timestamp: number;
}

// Hook ÏÉÅÌÉú ÌÉÄÏûÖ
interface HybridAIState {
  isLoading: boolean;
  response: UnifiedAIResponse | null;
  thinkingSteps: ThinkingStep[];
  error: string | null;
  stats: {
    cacheHits: number;
    totalRequests: number;
    avgResponseTime: number;
  };
}

// Î∞∞Ïπò ÏöîÏ≤≠ ÌÉÄÏûÖ
interface BatchRequest {
  query: string;
  resolve: (response: UnifiedAIResponse) => void;
  reject: (error: Error) => void;
}

export function useHybridAI() {
  const { data: session } = useSession();
  const [state, setState] = useState<HybridAIState>({
    isLoading: false,
    response: null,
    thinkingSteps: [],
    error: null,
    stats: {
      cacheHits: 0,
      totalRequests: 0,
      avgResponseTime: 0,
    },
  });

  // Refs
  const localCache = useRef<Map<string, CacheEntry>>(new Map());
  const batchQueue = useRef<BatchRequest[]>([]);
  const batchTimer = useRef<NodeJS.Timeout>();
  const eventSource = useRef<EventSource>();
  const responseTimes = useRef<number[]>([]);

  // Ï∫êÏãú Ï†ïÎ¶¨
  useEffect(() => {
    const interval = setInterval(() => {
      const now = Date.now();
      for (const [key, entry] of localCache.current.entries()) {
        if (now - entry.timestamp > CONFIG.localCacheTTL) {
          localCache.current.delete(key);
        }
      }
    }, 60000); // 1Î∂ÑÎßàÎã§

    return () => clearInterval(interval);
  }, []);

  // SSE Ïó∞Í≤∞ ÏÑ§Ï†ï
  const connectThinkingStream = useCallback((sessionId: string) => {
    // Í∏∞Ï°¥ Ïó∞Í≤∞ Ï¢ÖÎ£å
    if (eventSource.current) {
      eventSource.current.close();
    }

    // ÏÉà Ïó∞Í≤∞ ÏÉùÏÑ±
    eventSource.current = new EventSource(
      `${CONFIG.streamEndpoint}?sessionId=${sessionId}`
    );

    eventSource.current.addEventListener('thinking', (event) => {
      const step: ThinkingStep = JSON.parse(event.data);
      setState((prev) => ({
        ...prev,
        thinkingSteps: [...prev.thinkingSteps, step],
      }));
    });

    eventSource.current.addEventListener('complete', () => {
      eventSource.current?.close();
      eventSource.current = undefined;
    });

    eventSource.current.addEventListener('error', (event) => {
      console.error('SSE Error:', event);
      eventSource.current?.close();
      eventSource.current = undefined;
    });
  }, []);

  // Î∞∞Ïπò Ï≤òÎ¶¨ Ïã§Ìñâ
  const processBatch = useCallback(async () => {
    const batch = batchQueue.current.splice(0, CONFIG.maxBatchSize);
    if (batch.length === 0) return;

    try {
      // Î≥ëÎ†¨ ÏöîÏ≤≠ (Î¨¥Î£å Ìã∞Ïñ¥ ÎÇ¥ÏóêÏÑú)
      const promises = batch.map(async ({ query, resolve, reject }) => {
        try {
          const response = await fetch(CONFIG.apiEndpoint, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              query,
              userId: session?.user?.id,
              services: ['redis-cache', 'supabase-rag'], // Î¨¥Î£å ÏÑúÎπÑÏä§ Ïö∞ÏÑ†
              parallel: true,
            }),
          });

          if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
          }

          const data: UnifiedAIResponse = await response.json();

          // Î°úÏª¨ Ï∫êÏãú Ï†ÄÏû•
          const cacheKey = query.toLowerCase().trim();
          localCache.current.set(cacheKey, {
            query,
            response: data,
            timestamp: Date.now(),
          });

          resolve(data);
        } catch (error) {
          reject(error instanceof Error ? error : new Error('Unknown error'));
        }
      });

      await Promise.all(promises);
    } catch (error) {
      console.error('Batch processing error:', error);
    }
  }, [session]);

  // AI ÏøºÎ¶¨ Ïã§Ìñâ
  const query = useCallback(
    async (
      prompt: string,
      options?: {
        services?: AIServiceType[];
        skipCache?: boolean;
        priority?: 'low' | 'normal' | 'high';
      }
    ): Promise<UnifiedAIResponse> => {
      const startTime = Date.now();
      const cacheKey = prompt.toLowerCase().trim();

      // ÌÜµÍ≥Ñ ÏóÖÎç∞Ïù¥Ìä∏
      setState((prev) => ({
        ...prev,
        stats: {
          ...prev.stats,
          totalRequests: prev.stats.totalRequests + 1,
        },
      }));

      // 1. Î°úÏª¨ Ï∫êÏãú ÌôïÏù∏
      if (!options?.skipCache) {
        const cached = localCache.current.get(cacheKey);
        if (cached && Date.now() - cached.timestamp < CONFIG.localCacheTTL) {
          setState((prev) => ({
            ...prev,
            response: cached.response,
            stats: {
              ...prev.stats,
              cacheHits: prev.stats.cacheHits + 1,
            },
          }));
          return cached.response;
        }
      }

      // 2. ÏöîÏ≤≠ Ï§ÄÎπÑ
      setState((prev) => ({
        ...prev,
        isLoading: true,
        error: null,
        thinkingSteps: [],
      }));

      return new Promise((resolve, reject) => {
        // ÎÜíÏùÄ Ïö∞ÏÑ†ÏàúÏúÑÎäî Ï¶âÏãú Ï≤òÎ¶¨
        if (options?.priority === 'high') {
          (async () => {
            try {
              const sessionId = crypto.randomUUID();
              connectThinkingStream(sessionId);

              const response = await fetch(CONFIG.apiEndpoint, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  query: prompt,
                  userId: session?.user?.id,
                  sessionId,
                  services: options.services || [
                    'redis-cache',
                    'supabase-rag',
                    'gcp-korean-nlp',
                  ],
                  parallel: true,
                }),
              });

              if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
              }

              const data: UnifiedAIResponse = await response.json();

              // ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏
              const responseTime = Date.now() - startTime;
              responseTimes.current.push(responseTime);
              if (responseTimes.current.length > 100) {
                responseTimes.current.shift();
              }

              setState((prev) => ({
                ...prev,
                isLoading: false,
                response: data,
                stats: {
                  ...prev.stats,
                  avgResponseTime:
                    responseTimes.current.reduce((a, b) => a + b, 0) /
                    responseTimes.current.length,
                },
              }));

              resolve(data);
            } catch (error) {
              const errorMessage =
                error instanceof Error ? error.message : 'Unknown error';
              setState((prev) => ({
                ...prev,
                isLoading: false,
                error: errorMessage,
              }));
              reject(error);
            }
          })();
        } else {
          // Î∞∞Ïπò ÌÅêÏóê Ï∂îÍ∞Ä
          batchQueue.current.push({ query: prompt, resolve, reject });

          // Î∞∞Ïπò ÌÉÄÏù¥Î®∏ ÏÑ§Ï†ï
          if (batchTimer.current) {
            clearTimeout(batchTimer.current);
          }

          batchTimer.current = setTimeout(() => {
            processBatch();
          }, CONFIG.batchDelay);

          // ÌÅêÍ∞Ä Í∞ÄÎìù Ï∞®Î©¥ Ï¶âÏãú Ï≤òÎ¶¨
          if (batchQueue.current.length >= CONFIG.maxBatchSize) {
            clearTimeout(batchTimer.current);
            processBatch();
          }
        }
      });
    },
    [session, connectThinkingStream, processBatch]
  );

  // Ï∫êÏãú ÌÅ¥Î¶¨Ïñ¥
  const clearCache = useCallback(() => {
    localCache.current.clear();
    setState((prev) => ({
      ...prev,
      stats: {
        ...prev.stats,
        cacheHits: 0,
      },
    }));
  }, []);

  // ÌÜµÍ≥Ñ Î¶¨ÏÖã
  const resetStats = useCallback(() => {
    responseTimes.current = [];
    setState((prev) => ({
      ...prev,
      stats: {
        cacheHits: 0,
        totalRequests: 0,
        avgResponseTime: 0,
      },
    }));
  }, []);

  // Cleanup
  useEffect(() => {
    return () => {
      if (eventSource.current) {
        eventSource.current.close();
      }
      if (batchTimer.current) {
        clearTimeout(batchTimer.current);
      }
    };
  }, []);

  return {
    // ÏÉÅÌÉú
    ...state,

    // Î©îÏÑúÎìú
    query,
    clearCache,
    resetStats,

    // Ïú†Ìã∏Î¶¨Ìã∞
    isStreaming: !!eventSource.current,
    cacheSize: localCache.current.size,
    queueSize: batchQueue.current.length,
  };
}
