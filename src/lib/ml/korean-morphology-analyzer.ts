/**
 * ğŸ‡°ğŸ‡· í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (ê·œì¹™ ê¸°ë°˜)
 *
 * KoNLPy ëŒ€ì²´ìš© ê²½ëŸ‰ í˜•íƒœì†Œ ë¶„ì„
 * - ì„œë²„ ëª¨ë‹ˆí„°ë§ ë„ë©”ì¸ íŠ¹í™”
 * - ì¡°ì‚¬/ì–´ë¯¸ ë¶„ë¦¬
 * - ì–´ê°„ ì¶”ì¶œ
 */

export interface MorphemeResult {
  surface: string; // í‘œë©´í˜•
  pos: string; // í’ˆì‚¬
  stem: string; // ì–´ê°„
  feature: string; // ì˜ë¯¸ì  íŠ¹ì§•
}

export interface KoreanAnalysisResult {
  morphemes: MorphemeResult[];
  stems: string[];
  keywords: string[];
  entities: string[];
  confidence: number;
}

export class KoreanMorphologyAnalyzer {
  // ì¡°ì‚¬ íŒ¨í„´
  private readonly particles = [
    'ì´',
    'ê°€',
    'ì€',
    'ëŠ”',
    'ì„',
    'ë¥¼',
    'ì˜',
    'ì—',
    'ì—ì„œ',
    'ë¡œ',
    'ìœ¼ë¡œ',
    'ì™€',
    'ê³¼',
    'ë„',
    'ë§Œ',
    'ë¶€í„°',
    'ê¹Œì§€',
    'ê»˜ì„œ',
    'ì—ê²Œ',
    'í•œí…Œ',
  ];

  // ì–´ë¯¸ íŒ¨í„´
  private readonly endings = [
    'ë‹¤',
    'ìš”',
    'ì–´ìš”',
    'ì•„ìš”',
    'ì—ìš”',
    'í•´ìš”',
    'ì„¸ìš”',
    'ì£ ',
    'ì—ˆë‹¤',
    'ì•˜ë‹¤',
    'í–ˆë‹¤',
    'ê² ë‹¤',
    'ëœë‹¤',
    'í•œë‹¤',
    'ì´ë‹¤',
  ];

  // ì„œë²„ ëª¨ë‹ˆí„°ë§ ë„ë©”ì¸ ì–´íœ˜
  private readonly domainVocab = {
    server: ['ì„œë²„', 'ì‹œìŠ¤í…œ', 'í˜¸ìŠ¤íŠ¸', 'ì¸ìŠ¤í„´ìŠ¤', 'ë…¸ë“œ', 'í´ëŸ¬ìŠ¤í„°'],
    performance: ['ì„±ëŠ¥', 'ì†ë„', 'ì§€ì—°', 'ì‘ë‹µì‹œê°„', 'ì²˜ë¦¬ëŸ‰', 'ëŒ€ì—­í­'],
    resource: ['CPU', 'ë©”ëª¨ë¦¬', 'ë””ìŠ¤í¬', 'ë„¤íŠ¸ì›Œí¬', 'ìŠ¤í† ë¦¬ì§€'],
    status: ['ìƒíƒœ', 'í—¬ìŠ¤', 'ì •ìƒ', 'ë¹„ì •ìƒ', 'ì‘ë™', 'ì¤‘ë‹¨', 'ì¥ì• '],
    action: ['í™•ì¸', 'ì²´í¬', 'ë¶„ì„', 'ëª¨ë‹ˆí„°ë§', 'ê²€ì‚¬', 'ì ê²€', 'ìµœì í™”'],
    database: ['ë°ì´í„°ë² ì´ìŠ¤', 'DB', 'MySQL', 'PostgreSQL', 'Redis', 'ìºì‹œ'],
  };

  /**
   * ğŸ” í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„
   */
  analyze(text: string): KoreanAnalysisResult {
    // 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    const normalized = this.normalizeText(text);

    // 2. í† í°í™”
    const tokens = this.tokenize(normalized);

    // 3. í˜•íƒœì†Œ ë¶„ì„
    const morphemes = this.analyzeMorphemes(tokens);

    // 4. ì–´ê°„ ì¶”ì¶œ
    const stems = morphemes.map(m => m.stem).filter((s: any) => s.length > 1);

    // 5. í‚¤ì›Œë“œ ì¶”ì¶œ
    const keywords = this.extractKeywords(morphemes);

    // 6. ê°œì²´ëª… ì¸ì‹
    const entities = this.recognizeEntities(morphemes);

    // 7. ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°
    const confidence = this.calculateConfidence(morphemes);

    return {
      morphemes,
      stems,
      keywords,
      entities,
      confidence,
    };
  }

  /**
   * ğŸ”§ í…ìŠ¤íŠ¸ ì •ê·œí™”
   */
  private normalizeText(text: string): string {
    return (
      text
        .trim()
        .replace(/\s+/g, ' ')
        // íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ë¬¸, ìˆ«ì, í•˜ì´í”ˆ, ì , ê³µë°±ë§Œ ìœ ì§€)
        .replace(/[^\w\sã„±-ã…ã…-ã…£ê°€-í£.-]/g, ' ')
        .replace(/\s+/g, ' ')
    );
  }

  /**
   * âœ‚ï¸ í† í°í™”
   */
  private tokenize(text: string): string[] {
    return text.split(/\s+/).filter(token => token.length > 0);
  }

  /**
   * ğŸ”¬ í˜•íƒœì†Œ ë¶„ì„
   */
  private analyzeMorphemes(tokens: string[]): MorphemeResult[] {
    const morphemes: MorphemeResult[] = [];

    for (const token of tokens) {
      if (/[ê°€-í£]/.test(token)) {
        // í•œê¸€ í† í° ì²˜ë¦¬
        const result = this.analyzeKoreanToken(token);
        morphemes.push(...result);
      } else {
        // ì˜ë¬¸/ìˆ«ì í† í° ì²˜ë¦¬ (ëŒ€ì†Œë¬¸ì ìœ ì§€)
        morphemes.push({
          surface: token,
          pos: this.classifyEnglishToken(token),
          stem: token, // ëŒ€ì†Œë¬¸ì ìœ ì§€
          feature: this.getSemanticFeature(token),
        });
      }
    }

    return morphemes;
  }

  /**
   * ğŸ‡°ğŸ‡· í•œêµ­ì–´ í† í° ë¶„ì„
   */
  private analyzeKoreanToken(token: string): MorphemeResult[] {
    const results: MorphemeResult[] = [];
    let remaining = token;

    // ì¡°ì‚¬ ë¶„ë¦¬
    const particle = this.extractParticle(remaining);
    if (particle) {
      remaining = remaining.slice(0, -particle.length);
      results.push({
        surface: particle,
        pos: 'JKS', // ì¡°ì‚¬
        stem: particle,
        feature: 'particle',
      });
    }

    // ì–´ë¯¸ ë¶„ë¦¬
    const ending = this.extractEnding(remaining);
    if (ending) {
      remaining = remaining.slice(0, -ending.length);
      results.push({
        surface: ending,
        pos: 'EF', // ì–´ë¯¸
        stem: ending,
        feature: 'ending',
      });
    }

    // ì–´ê°„ ì²˜ë¦¬
    if (remaining.length > 0) {
      results.unshift({
        surface: remaining,
        pos: this.classifyKoreanStem(remaining),
        stem: remaining,
        feature: this.getSemanticFeature(remaining),
      });
    }

    // ì „ì²´ í† í°ì´ ë¶„ì„ë˜ì§€ ì•Šì€ ê²½ìš° ì›í˜• ìœ ì§€
    if (results.length === 0) {
      results.push({
        surface: token,
        pos: 'NNG', // ì¼ë°˜ëª…ì‚¬ë¡œ ë¶„ë¥˜
        stem: token,
        feature: this.getSemanticFeature(token),
      });
    }

    return results;
  }

  /**
   * ğŸ¯ ì¡°ì‚¬ ì¶”ì¶œ
   */
  private extractParticle(token: string): string | null {
    for (const particle of this.particles) {
      if (token.endsWith(particle) && token.length > particle.length) {
        return particle;
      }
    }
    return null;
  }

  /**
   * ğŸ¯ ì–´ë¯¸ ì¶”ì¶œ
   */
  private extractEnding(token: string): string | null {
    for (const ending of this.endings) {
      if (token.endsWith(ending) && token.length > ending.length) {
        return ending;
      }
    }
    return null;
  }

  /**
   * ğŸ·ï¸ í•œêµ­ì–´ ì–´ê°„ í’ˆì‚¬ ë¶„ë¥˜
   */
  private classifyKoreanStem(stem: string): string {
    // ë„ë©”ì¸ ì–´íœ˜ ê¸°ë°˜ ë¶„ë¥˜
    for (const [category, words] of Object.entries(this.domainVocab)) {
      if (words.some(word => stem.includes(word) || word.includes(stem))) {
        switch (category) {
          case 'server':
          case 'database':
            return 'NNG'; // ì¼ë°˜ëª…ì‚¬
          case 'performance':
          case 'resource':
            return 'NNG'; // ì¼ë°˜ëª…ì‚¬
          case 'status':
            return 'NNG'; // ì¼ë°˜ëª…ì‚¬
          case 'action':
            return 'VV'; // ë™ì‚¬
        }
      }
    }

    // ê¸°ë³¸ ë¶„ë¥˜ (ê¸¸ì´ ê¸°ë°˜)
    if (stem.length >= 3) {
      return 'NNG'; // ì¼ë°˜ëª…ì‚¬
    } else {
      return 'MM'; // ê´€í˜•ì‚¬
    }
  }

  /**
   * ğŸ·ï¸ ì˜ë¬¸ í† í° ë¶„ë¥˜
   */
  private classifyEnglishToken(token: string): string {
    if (/^\d+$/.test(token)) {
      return 'SN'; // ìˆ«ì
    }
    if (/^[A-Z]+$/.test(token)) {
      return 'SL'; // ì™¸êµ­ì–´ (ì•½ì–´)
    }
    return 'SL'; // ì™¸êµ­ì–´
  }

  /**
   * ğŸ¨ ì˜ë¯¸ì  íŠ¹ì§• ì¶”ì¶œ
   */
  private getSemanticFeature(word: string): string {
    const lowerWord = word.toLowerCase();

    for (const [category, words] of Object.entries(this.domainVocab)) {
      if (
        words.some(
          w =>
            lowerWord.includes(w.toLowerCase()) ||
            w.toLowerCase().includes(lowerWord)
        )
      ) {
        return category;
      }
    }

    return 'general';
  }

  /**
   * ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œ
   */
  private extractKeywords(morphemes: MorphemeResult[]): string[] {
    return morphemes
      .filter(
        m =>
          ['NNG', 'VV', 'SL'].includes(m.pos) && // ëª…ì‚¬, ë™ì‚¬, ì™¸êµ­ì–´
          m.stem.length > 1 &&
          m.feature !== 'general'
      )
      .map(m => m.stem)
      .filter((stem, index, array) => array.indexOf(stem) === index); // ì¤‘ë³µ ì œê±°
  }

  /**
   * ğŸ¢ ê°œì²´ëª… ì¸ì‹
   */
  private recognizeEntities(morphemes: MorphemeResult[]): string[] {
    const entities: string[] = [];

    for (const morpheme of morphemes) {
      // ì„œë²„ëª… íŒ¨í„´ (server-web-001 í˜•íƒœ)
      if (/^[a-z]+-[a-z]+-\d+$/i.test(morpheme.surface)) {
        entities.push(morpheme.surface);
      }

      // IP ì£¼ì†Œ íŒ¨í„´
      if (/^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$/.test(morpheme.surface)) {
        entities.push(morpheme.surface);
      }

      // í¬íŠ¸ ë²ˆí˜¸ íŒ¨í„´
      if (/^:\d{2,5}$/.test(morpheme.surface)) {
        entities.push(morpheme.surface);
      }

      // ë„ë©”ì¸ íŠ¹í™” ê°œì²´ëª…
      if (
        ['server', 'database', 'performance'].includes(morpheme.feature) &&
        morpheme.stem.length >= 3
      ) {
        entities.push(morpheme.stem);
      }
    }

    return [...new Set(entities)]; // ì¤‘ë³µ ì œê±°
  }

  /**
   * ğŸ“Š ë¶„ì„ ì‹ ë¢°ë„ ê³„ì‚°
   */
  private calculateConfidence(morphemes: MorphemeResult[]): number {
    if (morphemes.length === 0) return 0.1;

    // ë„ë©”ì¸ íŠ¹í™” í˜•íƒœì†Œ ë¹„ìœ¨
    const domainMorphemes = morphemes.filter(m => m.feature !== 'general');
    const domainRatio = domainMorphemes.length / morphemes.length;

    // í’ˆì‚¬ ë‹¤ì–‘ì„±
    const posTypes = new Set(morphemes.map(m => m.pos));
    const diversityBonus = Math.min(0.2, posTypes.size * 0.05);

    // ê¸°ë³¸ ì‹ ë¢°ë„ + ë„ë©”ì¸ ë¹„ìœ¨ + ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
    const confidence = 0.5 + domainRatio * 0.3 + diversityBonus;

    return Math.min(0.95, confidence);
  }

  /**
   * ğŸ” ì–´ê°„ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
   */
  calculateStemSimilarity(text1: string, text2: string): number {
    const analysis1 = this.analyze(text1);
    const analysis2 = this.analyze(text2);

    // ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë¹„êµ
    const stems1 = new Set(analysis1.stems.map((s: any) => s.toLowerCase()));
    const stems2 = new Set(analysis2.stems.map((s: any) => s.toLowerCase()));

    const intersection = new Set([...stems1].filter(x => stems2.has(x)));
    const union = new Set([...stems1, ...stems2]);

    // ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ê³„ìˆ˜)
    const jaccardSimilarity =
      union.size > 0 ? intersection.size / union.size : 0;

    // ë™ì˜ì–´ ë§¤í•‘ì„ í†µí•œ ì˜ë¯¸ì  ìœ ì‚¬ë„ ì¶”ê°€
    const semanticBoost = this.calculateSemanticSimilarity(
      analysis1.stems,
      analysis2.stems
    );

    return Math.min(1.0, jaccardSimilarity + semanticBoost * 0.3);
  }

  /**
   * ğŸ¯ ë™ì˜ì–´ ê¸°ë°˜ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
   */
  private calculateSemanticSimilarity(
    stems1: string[],
    stems2: string[]
  ): number {
    let semanticMatches = 0;
    let totalComparisons = 0;

    for (const stem1 of stems1) {
      for (const stem2 of stems2) {
        totalComparisons++;

        // ë™ì˜ì–´ ê·¸ë£¹ì—ì„œ ë§¤ì¹­ í™•ì¸
        const stem1Lower = stem1.toLowerCase();
        const stem2Lower = stem2.toLowerCase();

        // ë™ì˜ì–´ ë§¤í•‘ í™•ì¸
        const synonymGroups: Record<string, string[]> = {
          ì„±ëŠ¥: ['ì†ë„', 'í¼í¬ë¨¼ìŠ¤', 'ì²˜ë¦¬ëŠ¥ë ¥'],
          ëŠë¦¼: ['ì§€ì—°', 'ëŠ¦ìŒ', 'ë”ë”¤'],
          ë¹ ë¦„: ['ì‹ ì†', 'ê³ ì†', 'ë¹ ë¥¸ì†ë„'],
          í™•ì¸: ['ì²´í¬', 'ì ê²€', 'ê²€ì‚¬'],
          ë¬¸ì œ: ['ì´ìŠˆ', 'ì¥ì• ', 'ì˜¤ë¥˜', 'ì—ëŸ¬'],
          ì„œë²„: ['ì‹œìŠ¤í…œ', 'í˜¸ìŠ¤íŠ¸', 'ë¨¸ì‹ '],
          ë°ì´í„°ë² ì´ìŠ¤: ['DB', 'ë””ë¹„', 'ì €ì¥ì†Œ'],
        };

        for (const [main, synonyms] of Object.entries(synonymGroups)) {
          const group = [main, ...synonyms];

          const stem1InGroup = group.some(
            word => stem1Lower.includes(word) || word.includes(stem1Lower)
          );
          const stem2InGroup = group.some(
            word => stem2Lower.includes(word) || word.includes(stem2Lower)
          );

          if (stem1InGroup && stem2InGroup) {
            semanticMatches++;
            break;
          }
        }
      }
    }

    return totalComparisons > 0 ? semanticMatches / totalComparisons : 0;
  }

  /**
   * ğŸ¯ ì˜ë„ ë¶„ì„ (í˜•íƒœì†Œ ê¸°ë°˜)
   */
  analyzeIntent(text: string): {
    intent: string;
    confidence: number;
    evidence: string[];
  } {
    const analysis = this.analyze(text);
    const evidence: string[] = [];

    // ì˜ë„ë³„ í‚¤ì›Œë“œ íŒ¨í„´
    const intentPatterns = {
      status_check: ['ìƒíƒœ', 'í™•ì¸', 'ì²´í¬', 'í—¬ìŠ¤', 'ì ê²€'],
      performance_analysis: ['ì„±ëŠ¥', 'ì†ë„', 'ì§€ì—°', 'ì‘ë‹µì‹œê°„', 'ìµœì í™”'],
      error_diagnosis: ['ì˜¤ë¥˜', 'ì—ëŸ¬', 'ì¥ì• ', 'ë¬¸ì œ', 'ì‹¤íŒ¨'],
      monitoring: ['ëª¨ë‹ˆí„°ë§', 'ê°ì‹œ', 'ì¶”ì ', 'ê´€ì°°'],
      resource_check: ['CPU', 'ë©”ëª¨ë¦¬', 'ë””ìŠ¤í¬', 'ë„¤íŠ¸ì›Œí¬'],
    };

    let bestIntent = 'general';
    let maxScore = 0;

    for (const [intent, keywords] of Object.entries(intentPatterns)) {
      let score = 0;
      const matches: string[] = [];

      for (const stem of analysis.stems) {
        for (const keyword of keywords) {
          if (stem.includes(keyword) || keyword.includes(stem)) {
            score += 1;
            matches.push(stem);
          }
        }
      }

      if (score > maxScore) {
        maxScore = score;
        bestIntent = intent;
        evidence.length = 0;
        evidence.push(...matches);
      }
    }

    const confidence = maxScore > 0 ? Math.min(0.9, 0.4 + maxScore * 0.2) : 0.3;

    return {
      intent: bestIntent,
      confidence,
      evidence,
    };
  }
}

export const koreanMorphologyAnalyzer = new KoreanMorphologyAnalyzer();
