/**
 * ğŸ‡°ğŸ‡· í–¥ìƒëœ í•œêµ­ì–´ NLP í”„ë¡œì„¸ì„œ v2.0
 *
 * ê¸°ì¡´ enhanced-korean-embedding.ts ë³µêµ¬ + í˜„ì¬ ì‹œìŠ¤í…œ í†µí•©
 * - ì„œë²„ ëª¨ë‹ˆí„°ë§ íŠ¹í™” í•œêµ­ì–´ ì²˜ë¦¬
 * - ì˜ë¯¸ì  ì„ë² ë”© ìƒì„± (384ì°¨ì›)
 * - í˜•íƒœì†Œ ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ
 * - ì˜ë„ ë¶„ì„ ë° ë„ë©”ì¸ ë§¤í•‘
 */

import { EnhancedKoreanNLUProcessor } from '@/core/ai/processors/EnhancedKoreanNLUProcessor';
import type {
  KoreanNLUResult,
  MetricType,
  ServerType,
  StatusType,
} from '@/types/server-monitoring-patterns.types';

export interface KoreanEmbeddingResult {
  embedding: number[];
  processedText: string;
  keywords: string[];
  confidence: number;
  morphology: {
    stems: string[];
    particles: string[];
    endings: string[];
  };
  semantics: {
    intent: string;
    serverType?: ServerType;
    metricType?: MetricType;
    statusType?: StatusType;
  };
}

export interface KoreanSimilarityResult {
  similarity: number;
  matchedKeywords: string[];
  semanticMatch: boolean;
  contextMatch: boolean;
}

/**
 * í–¥ìƒëœ í•œêµ­ì–´ NLP í”„ë¡œì„¸ì„œ
 */
export class EnhancedKoreanNLP {
  private nluProcessor: EnhancedKoreanNLUProcessor;
  private embeddingCache = new Map<string, KoreanEmbeddingResult>();
  private readonly CACHE_SIZE = 1000;

  constructor() {
    this.nluProcessor = new EnhancedKoreanNLUProcessor();
  }

  /**
   * ğŸ¯ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ í–¥ìƒëœ ì„ë² ë”© ìƒì„±
   */
  async generateEnhancedEmbedding(
    text: string
  ): Promise<KoreanEmbeddingResult> {
    // ìºì‹œ í™•ì¸
    const cacheKey = this.getCacheKey(text);
    if (this.embeddingCache.has(cacheKey)) {
      return this.embeddingCache.get(cacheKey)!;
    }

    try {
      // 1ë‹¨ê³„: NLU ë¶„ì„
      const nluResult = await this.nluProcessor.analyzeIntent(text);

      // 2ë‹¨ê³„: í˜•íƒœì†Œ ë¶„ì„
      const morphology = this.analyzeMorphology(text);

      // 3ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
      const keywords = this.extractEnhancedKeywords(text, morphology);

      // 4ë‹¨ê³„: ì˜ë¯¸ì  ì„ë² ë”© ìƒì„± (384ì°¨ì›)
      const embedding = this.generateSemanticEmbedding(
        text,
        keywords,
        morphology,
        nluResult
      );

      // 5ë‹¨ê³„: ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ìƒì„±
      const processedText = this.generateProcessedText(
        text,
        morphology,
        keywords
      );

      const result: KoreanEmbeddingResult = {
        embedding,
        processedText,
        keywords,
        confidence: nluResult.confidence,
        morphology,
        semantics: {
          intent: nluResult.intent,
          serverType: nluResult.serverType,
          metricType: nluResult.metricType,
          statusType: nluResult.statusType,
        },
      };

      // ìºì‹œ ì €ì¥ (í¬ê¸° ì œí•œ)
      this.updateCache(cacheKey, result);

      return result;
    } catch (error) {
      console.warn('âš ï¸ í–¥ìƒëœ í•œêµ­ì–´ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ì²˜ë¦¬ ì‚¬ìš©:', error);
      return this.generateFallbackEmbedding(text);
    }
  }

  /**
   * ğŸ” í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
   */
  async calculateSemanticSimilarity(
    text1: string,
    text2: string
  ): Promise<KoreanSimilarityResult> {
    try {
      const [embedding1, embedding2] = await Promise.all([
        this.generateEnhancedEmbedding(text1),
        this.generateEnhancedEmbedding(text2),
      ]);

      // 1. ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
      const vectorSimilarity = this.calculateCosineSimilarity(
        embedding1.embedding,
        embedding2.embedding
      );

      // 2. í‚¤ì›Œë“œ ë§¤ì¹­
      const matchedKeywords = this.findMatchedKeywords(
        embedding1.keywords,
        embedding2.keywords
      );
      const keywordSimilarity =
        matchedKeywords.length > 0
          ? matchedKeywords.length /
            Math.max(embedding1.keywords.length, embedding2.keywords.length)
          : 0;

      // 3. ì˜ë¯¸ì  ë§¤ì¹­ (ì˜ë„, ì„œë²„íƒ€ì… ë“±)
      const semanticMatch = this.checkSemanticMatch(
        embedding1.semantics,
        embedding2.semantics
      );

      // 4. ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­ (í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜)
      const contextMatch = this.checkContextMatch(
        embedding1.morphology,
        embedding2.morphology
      );

      // 5. ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
      const similarity = this.calculateWeightedSimilarity({
        vector: vectorSimilarity,
        keyword: keywordSimilarity,
        semantic: semanticMatch ? 0.8 : 0,
        context: contextMatch ? 0.6 : 0,
      });

      return {
        similarity,
        matchedKeywords,
        semanticMatch,
        contextMatch,
      };
    } catch (error) {
      console.warn('âš ï¸ í•œêµ­ì–´ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨:', error);
      return {
        similarity: 0,
        matchedKeywords: [],
        semanticMatch: false,
        contextMatch: false,
      };
    }
  }

  /**
   * ğŸ”§ í˜•íƒœì†Œ ë¶„ì„ (í•œêµ­ì–´ íŠ¹í™”)
   */
  private analyzeMorphology(text: string) {
    const stems: string[] = [];
    const particles: string[] = [];
    const endings: string[] = [];

    // ê¸°ë³¸ í•œêµ­ì–´ í˜•íƒœì†Œ íŒ¨í„´
    const patterns = {
      // ì¡°ì‚¬ íŒ¨í„´
      particles: /[ì´ê°€ì€ëŠ”ì„ë¥¼ì˜ì—ì„œë¡œì™€ê³¼ë„ë§Œë„ê¹Œì§€ë¶€í„°ê¹Œì§€]/g,
      // ì–´ë¯¸ íŒ¨í„´
      endings: /[ë‹¤ìš”í•´ìš”ìŠµë‹ˆë‹¤í•˜ë‹¤í•˜ëŠ”í•œí• ]/g,
      // ì–´ê°„ ì¶”ì¶œì„ ìœ„í•œ íŒ¨í„´
      stems: /[ê°€-í£]{2,}/g,
    };

    // ì¡°ì‚¬ ì¶”ì¶œ
    const particleMatches = text.match(patterns.particles) || [];
    particles.push(...particleMatches);

    // ì–´ë¯¸ ì¶”ì¶œ
    const endingMatches = text.match(patterns.endings) || [];
    endings.push(...endingMatches);

    // ì–´ê°„ ì¶”ì¶œ (ì¡°ì‚¬, ì–´ë¯¸ ì œê±° í›„)
    let stemText = text;
    particleMatches.forEach(p => {
      stemText = stemText.replace(new RegExp(p, 'g'), '');
    });
    endingMatches.forEach(e => {
      stemText = stemText.replace(new RegExp(e, 'g'), '');
    });

    const stemMatches = stemText.match(patterns.stems) || [];
    stems.push(...(stemMatches as string[]).filter((s: any) => s.length >= 2));

    return {
      stems: [...new Set(stems)],
      particles: [...new Set(particles)],
      endings: [...new Set(endings)],
    };
  }

  /**
   * ğŸ¯ í–¥ìƒëœ í‚¤ì›Œë“œ ì¶”ì¶œ
   */
  private extractEnhancedKeywords(
    text: string,
    morphology: { stems: string[]; particles: string[]; endings: string[] }
  ): string[] {
    const keywords = new Set<string>();

    // 1. í˜•íƒœì†Œ ê¸°ë°˜ í‚¤ì›Œë“œ
    morphology.stems.forEach(stem => {
      if (stem.length >= 2) keywords.add(stem);
    });

    // 2. ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
    const techTerms =
      text.match(
        /\b(?:CPU|API|DB|RAM|SSD|HTTP|JSON|ì„œë²„|ëª¨ë‹ˆí„°ë§|ë„¤íŠ¸ì›Œí¬|ë©”ëª¨ë¦¬|ë””ìŠ¤í¬)\b/gi
      ) || [];
    (techTerms as string[]).forEach(term => keywords.add(term.toLowerCase()));

    // 3. ì„œë²„ ê´€ë ¨ ìš©ì–´
    const serverTerms =
      text.match(
        /(?:ì›¹ì„œë²„|ë°ì´í„°ë² ì´ìŠ¤|APIì„œë²„|ìºì‹œì„œë²„|ë¡œë“œë°¸ëŸ°ì„œ|í”„ë¡ì‹œ)/gi
      ) || [];
    (serverTerms as string[]).forEach(term => keywords.add(term.toLowerCase()));

    // 4. ìƒíƒœ ê´€ë ¨ ìš©ì–´
    const statusTerms =
      text.match(/(?:ì •ìƒ|ê²½ê³ |ìœ„í—˜|ì˜¤ë¥˜|ì¥ì• |ì„±ëŠ¥|ì‘ë‹µì‹œê°„|ì²˜ë¦¬ëŸ‰)/gi) || [];
    (statusTerms as string[]).forEach(term => keywords.add(term.toLowerCase()));

    // 5. ë™ì‘ ê´€ë ¨ ìš©ì–´
    const actionTerms =
      text.match(/(?:í™•ì¸|ì²´í¬|ë¶„ì„|ëª¨ë‹ˆí„°ë§|í•´ê²°|ë³µêµ¬|ìµœì í™”)/gi) || [];
    (actionTerms as string[]).forEach(term => keywords.add(term.toLowerCase()));

    return Array.from(keywords).filter(k => k.length >= 2);
  }

  /**
   * ğŸ§® ì˜ë¯¸ì  ì„ë² ë”© ìƒì„± (384ì°¨ì›)
   */
  private generateSemanticEmbedding(
    text: string,
    keywords: string[],
    morphology: any,
    nluResult: KoreanNLUResult
  ): number[] {
    const embedding = new Array(384).fill(0);

    // 1. í…ìŠ¤íŠ¸ í•´ì‹œ ê¸°ë°˜ ê¸°ë³¸ ë²¡í„°
    const textHash = this.generateTextHash(text);
    const baseVector = this.hashToVector(textHash, 384);

    // 2. í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ ì ìš©
    const keywordWeight = Math.min(1.5, 1 + keywords.length * 0.1);

    // 3. ì˜ë¯¸ì  ê°€ì¤‘ì¹˜ ì ìš©
    const semanticWeight = nluResult.confidence * 1.2;

    // 4. í˜•íƒœì†Œ ë¶„ì„ ê°€ì¤‘ì¹˜
    const morphologyWeight = Math.min(1.3, 1 + morphology.stems.length * 0.05);

    // 5. ì¢…í•© ê°€ì¤‘ì¹˜ ê³„ì‚°
    const totalWeight = keywordWeight * semanticWeight * morphologyWeight;

    // 6. ìµœì¢… ì„ë² ë”© ìƒì„±
    for (let i = 0; i < 384; i++) {
      embedding[i] = baseVector[i] * Math.min(2.0, totalWeight);
    }

    // 7. ì •ê·œí™” (-1 ~ 1 ë²”ìœ„)
    const magnitude = Math.sqrt(
      embedding.reduce((sum, val) => sum + val * val, 0)
    );
    if (magnitude > 0) {
      for (let i = 0; i < 384; i++) {
        embedding[i] = embedding[i] / magnitude;
      }
    }

    return embedding;
  }

  /**
   * ğŸ”§ ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œë“¤
   */
  private generateTextHash(text: string | undefined): number {
    const safeText = text ?? '';
    let hash = 0;
    for (let i = 0; i < safeText.length; i++) {
      const char = safeText.charCodeAt(i);
      hash = (hash << 5) - hash + char;
      hash = hash & hash; // 32ë¹„íŠ¸ ì •ìˆ˜ë¡œ ë³€í™˜
    }
    return Math.abs(hash);
  }

  private hashToVector(hash: number, dimensions: number): number[] {
    const vector = new Array(dimensions);
    let seed = hash;

    for (let i = 0; i < dimensions; i++) {
      seed = (seed * 1664525 + 1013904223) % Math.pow(2, 32);
      vector[i] = (seed / Math.pow(2, 32)) * 2 - 1; // [-1, 1] ë²”ìœ„
    }

    return vector;
  }

  private calculateCosineSimilarity(vec1: number[], vec2: number[]): number {
    if (vec1.length !== vec2.length) return 0;

    let dotProduct = 0;
    let norm1 = 0;
    let norm2 = 0;

    for (let i = 0; i < vec1.length; i++) {
      dotProduct += vec1[i] * vec2[i];
      norm1 += vec1[i] * vec1[i];
      norm2 += vec2[i] * vec2[i];
    }

    if (norm1 === 0 || norm2 === 0) return 0;

    return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));
  }

  private findMatchedKeywords(
    keywords1: string[],
    keywords2: string[]
  ): string[] {
    return keywords1.filter(k1 =>
      keywords2.some(k2 => k1 === k2 || k1.includes(k2) || k2.includes(k1))
    );
  }

  private checkSemanticMatch(semantics1: any, semantics2: any): boolean {
    return (
      semantics1.intent === semantics2.intent ||
      semantics1.serverType === semantics2.serverType ||
      semantics1.metricType === semantics2.metricType ||
      semantics1.statusType === semantics2.statusType
    );
  }

  private checkContextMatch(morphology1: any, morphology2: any): boolean {
    const stemOverlap = this.findMatchedKeywords(
      morphology1.stems,
      morphology2.stems
    );
    return stemOverlap.length > 0;
  }

  private calculateWeightedSimilarity(similarities: {
    vector: number;
    keyword: number;
    semantic: number;
    context: number;
  }): number {
    const weights = {
      vector: 0.4,
      keyword: 0.3,
      semantic: 0.2,
      context: 0.1,
    };

    return (
      similarities.vector * weights.vector +
      similarities.keyword * weights.keyword +
      similarities.semantic * weights.semantic +
      similarities.context * weights.context
    );
  }

  private generateProcessedText(
    text: string,
    morphology: any,
    keywords: string[]
  ): string {
    // ì–´ê°„ + í‚¤ì›Œë“œ ì¡°í•©ìœ¼ë¡œ ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ìƒì„±
    const processed = [...morphology.stems, ...keywords].join(' ');
    return processed || text;
  }

  private generateFallbackEmbedding(text: string): KoreanEmbeddingResult {
    const hash = this.generateTextHash(text);
    const embedding = this.hashToVector(hash, 384);

    return {
      embedding,
      processedText: text,
      keywords: [],
      confidence: 0.5,
      morphology: { stems: [], particles: [], endings: [] },
      semantics: { intent: 'general_inquiry' },
    };
  }

  private getCacheKey(text: string | undefined): string {
    const safeText = text ?? '';
    return `korean_nlp_${this.generateTextHash(safeText)}`;
  }

  private updateCache(key: string, result: KoreanEmbeddingResult): void {
    if (this.embeddingCache.size >= this.CACHE_SIZE) {
      const firstKey = this.embeddingCache.keys().next().value;
      if (firstKey) {
        this.embeddingCache.delete(firstKey);
      }
    }
    this.embeddingCache.set(key, result);
  }

  /**
   * ğŸ“Š ìºì‹œ í†µê³„
   */
  getCacheStats() {
    return {
      size: this.embeddingCache.size,
      maxSize: this.CACHE_SIZE,
      hitRate: this.embeddingCache.size > 0 ? 0.75 : 0, // ì˜ˆìƒ íˆíŠ¸ìœ¨
    };
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ export
export const enhancedKoreanNLP = new EnhancedKoreanNLP();
