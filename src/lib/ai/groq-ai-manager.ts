/**
 * @deprecated v5.84.0 - Hybrid Architecture ì „í™˜
 *
 * âš ï¸ DEPRECATED: ì´ ëª¨ë“ˆì€ Cloud Runìœ¼ë¡œ ì´ê´€ë˜ì—ˆìŠµë‹ˆë‹¤.
 *
 * Hybrid Architecture ì„¤ê³„:
 * - Vercel = Frontend/Proxy Only (API í‚¤ ì—†ìŒ, ì§ì ‘ AI í˜¸ì¶œ ê¸ˆì§€)
 * - Cloud Run = ALL AI processing (Groq API í˜¸ì¶œ í¬í•¨)
 *
 * ëŒ€ì²´ ë°©ë²•:
 * - AI ìš”ì²­: /api/ai/supervisor â†’ Cloud Run í”„ë¡ì‹œ
 * - íƒ€ì… ì •ì˜: src/types/ai/routing-types.ts (GroqModel íƒ€ì…)
 *
 * ì´ íŒŒì¼ì€ í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë˜ì§€ë§Œ, ì‹ ê·œ ì½”ë“œì—ì„œëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
 *
 * ---
 * (Legacy) Groq AI Manager v2.0 - Vercel AI SDK í†µí•©
 * @ai-sdk/groqë¥¼ ì‚¬ìš©í•œ í˜„ëŒ€ì ì¸ Groq í†µí•©
 * ë¬´ë£Œ í‹°ì–´: 14,400 RPD, 30 RPM (llama-3.1-8b-instant)
 *
 * ëª¨ë¸ ì˜µì…˜:
 * - llama-3.1-8b-instant: 30 RPM, 14,400 RPD, 6K TPM (ë¹ ë¥¸ ì‘ë‹µ)
 * - llama-3.3-70b-versatile: 30 RPM, 1,000 RPD, 12K TPM (ê³ í’ˆì§ˆ)
 * - qwen-qwq-32b: 30 RPM, 1,000 RPD, 6K TPM (reasoning ì§€ì›)
 *
 * @see https://sdk.vercel.ai/providers/ai-sdk-providers/groq
 */

import { groq } from '@ai-sdk/groq';
import { generateText, type ModelMessage, streamText } from 'ai';

export type GroqModel =
  | 'llama-3.1-8b-instant'
  | 'llama-3.3-70b-versatile'
  | 'qwen-qwq-32b';

interface GroqRateLimits {
  rpm: number; // Requests Per Minute
  rpd: number; // Requests Per Day
  tpm: number; // Tokens Per Minute
}

const MODEL_LIMITS: Record<GroqModel, GroqRateLimits> = {
  'llama-3.1-8b-instant': { rpm: 30, rpd: 14400, tpm: 6000 },
  'llama-3.3-70b-versatile': { rpm: 30, rpd: 1000, tpm: 12000 },
  'qwen-qwq-32b': { rpm: 30, rpd: 1000, tpm: 6000 },
};

// Reasoning ì§€ì› ëª¨ë¸
const REASONING_MODELS: GroqModel[] = ['qwen-qwq-32b'];

interface GenerateOptions {
  model?: GroqModel;
  maxTokens?: number;
  temperature?: number;
  systemPrompt?: string;
  /** Reasoning mode (qwen-qwq-32b only) */
  enableReasoning?: boolean;
  /** Reasoning effort: 'low' | 'medium' | 'high' */
  reasoningEffort?: 'low' | 'medium' | 'high';
}

interface GenerateResult {
  success: boolean;
  text?: string;
  error?: string;
  model?: string;
  reasoning?: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

interface StreamOptions extends GenerateOptions {
  messages?: ModelMessage[];
}

class GroqAIManager {
  private static instance: GroqAIManager;
  private apiKey: string | null = null;
  private defaultModel: GroqModel = 'llama-3.1-8b-instant';

  // Rate limiting ì¶”ì 
  private requestLog: number[] = [];
  private dailyRequestCount = 0;
  private lastResetDate: string = '';

  private constructor() {
    this.loadAPIKey();
  }

  static getInstance(): GroqAIManager {
    if (!GroqAIManager.instance) {
      GroqAIManager.instance = new GroqAIManager();
    }
    return GroqAIManager.instance;
  }

  private loadAPIKey(): void {
    this.apiKey =
      process.env.GROQ_API_KEY || process.env.NEXT_PUBLIC_GROQ_API_KEY || null;
  }

  /**
   * Groq API í‚¤ ê°€ì ¸ì˜¤ê¸°
   */
  getAPIKey(): string | null {
    if (this.apiKey) {
      return this.apiKey;
    }
    return null;
  }

  /**
   * API í‚¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
   */
  isAPIKeyAvailable(): boolean {
    return this.getAPIKey() !== null;
  }

  /**
   * ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
   */
  setDefaultModel(model: GroqModel): void {
    this.defaultModel = model;
  }

  /**
   * ê¸°ë³¸ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
   */
  getDefaultModel(): GroqModel {
    return this.defaultModel;
  }

  /**
   * API í‚¤ ìƒíƒœ ì •ë³´
   */
  getKeyStatus(): {
    keySource: 'env' | 'none';
    isAvailable: boolean;
    defaultModel: GroqModel;
    modelLimits: GroqRateLimits;
    supportsReasoning: boolean;
  } {
    const apiKey = this.getAPIKey();
    return {
      keySource: apiKey ? 'env' : 'none',
      isAvailable: apiKey !== null,
      defaultModel: this.defaultModel,
      modelLimits: MODEL_LIMITS[this.defaultModel],
      supportsReasoning: REASONING_MODELS.includes(this.defaultModel),
    };
  }

  /**
   * ğŸš¦ Rate Limit ì²´í¬
   */
  checkRateLimit(model?: GroqModel): { allowed: boolean; reason?: string } {
    const targetModel = model || this.defaultModel;
    const limits = MODEL_LIMITS[targetModel];
    const now = Date.now();
    const oneMinuteAgo = now - 60 * 1000;
    const today = new Date().toISOString().split('T')[0] ?? '';

    // ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê¸°í™”
    if (this.lastResetDate !== today) {
      this.dailyRequestCount = 0;
      this.lastResetDate = today;
    }

    // 1ë¶„ ë™ì•ˆì˜ ìš”ì²­ ìˆ˜ ê³„ì‚°
    this.requestLog = this.requestLog.filter(
      (timestamp) => timestamp > oneMinuteAgo
    );
    const requestsPerMinute = this.requestLog.length;

    // RPM í•œë„ ì²´í¬
    if (requestsPerMinute >= limits.rpm) {
      return {
        allowed: false,
        reason: `Groq rate limit: ${requestsPerMinute}/${limits.rpm} RPM`,
      };
    }

    // RPD í•œë„ ì²´í¬
    if (this.dailyRequestCount >= limits.rpd) {
      return {
        allowed: false,
        reason: `Groq daily quota: ${this.dailyRequestCount}/${limits.rpd} RPD`,
      };
    }

    return { allowed: true };
  }

  /**
   * ğŸ”„ ìš”ì²­ ê¸°ë¡
   */
  recordRequest(): void {
    const now = Date.now();
    this.requestLog.push(now);
    this.dailyRequestCount++;
  }

  /**
   * ğŸ“Š Rate Limit ìƒíƒœ ì¡°íšŒ
   */
  getRateLimitStatus(model?: GroqModel): {
    requestsLastMinute: number;
    requestsToday: number;
    remainingRPM: number;
    remainingRPD: number;
    model: GroqModel;
  } {
    const targetModel = model || this.defaultModel;
    const limits = MODEL_LIMITS[targetModel];
    const now = Date.now();
    const oneMinuteAgo = now - 60 * 1000;
    const requestsLastMinute = this.requestLog.filter(
      (timestamp) => timestamp > oneMinuteAgo
    ).length;

    return {
      requestsLastMinute,
      requestsToday: this.dailyRequestCount,
      remainingRPM: Math.max(0, limits.rpm - requestsLastMinute),
      remainingRPD: Math.max(0, limits.rpd - this.dailyRequestCount),
      model: targetModel,
    };
  }

  /**
   * ğŸ¤– Groq API í˜¸ì¶œ (Vercel AI SDK - generateText)
   *
   * @example
   * ```ts
   * const result = await generateGroqText('ì„œë²„ ìƒíƒœ ë¶„ì„í•´ì¤˜', {
   *   model: 'qwen-qwq-32b',
   *   enableReasoning: true,
   *   reasoningEffort: 'medium',
   * });
   * ```
   */
  async generateText(
    prompt: string,
    options?: GenerateOptions
  ): Promise<GenerateResult> {
    if (!this.isAPIKeyAvailable()) {
      return { success: false, error: 'Groq API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.' };
    }

    const rateCheck = this.checkRateLimit(options?.model);
    if (!rateCheck.allowed) {
      return { success: false, error: rateCheck.reason };
    }

    const model = options?.model || this.defaultModel;
    const supportsReasoning = REASONING_MODELS.includes(model);

    try {
      const messages: ModelMessage[] = [];

      if (options?.systemPrompt) {
        messages.push({ role: 'system', content: options.systemPrompt });
      }
      messages.push({ role: 'user', content: prompt });

      // Vercel AI SDK ê¸°ë°˜ í˜¸ì¶œ
      const result = await generateText({
        model: groq(model),
        messages,
        maxOutputTokens: options?.maxTokens || 2048,
        temperature: options?.temperature ?? 0.7,
        // Reasoning ì§€ì› (qwen-qwq-32b only)
        ...(supportsReasoning &&
          options?.enableReasoning && {
            experimental_providerOptions: {
              groq: {
                reasoningFormat: 'parsed',
                ...(options.reasoningEffort && {
                  reasoningEffort: options.reasoningEffort,
                }),
              },
            },
          }),
      });

      this.recordRequest();

      // Reasoning ê²°ê³¼ ì¶”ì¶œ (providerMetadataì—ì„œ)
      let reasoning: string | undefined;
      if (supportsReasoning && options?.enableReasoning) {
        const providerMeta = result.providerMetadata?.groq as
          | { reasoning?: string }
          | undefined;
        reasoning = providerMeta?.reasoning;
      }

      return {
        success: true,
        text: result.text,
        model,
        reasoning,
        usage: {
          promptTokens: result.usage?.inputTokens || 0,
          completionTokens: result.usage?.outputTokens || 0,
          totalTokens: result.usage?.totalTokens || 0,
        },
      };
    } catch (error) {
      console.error('âŒ Groq API í˜¸ì¶œ ì‹¤íŒ¨:', error);
      return {
        success: false,
        error:
          error instanceof Error ? error.message : 'Groq API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ',
      };
    }
  }

  /**
   * ğŸŒŠ Groq API ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (Vercel AI SDK - streamText)
   *
   * @example
   * ```ts
   * const stream = await groqManager.streamText('ë¶„ì„í•´ì¤˜', {
   *   model: 'llama-3.3-70b-versatile',
   * });
   * return stream.toTextStreamResponse();
   * ```
   */
  async streamTextResponse(
    prompt: string,
    options?: StreamOptions
  ): Promise<ReturnType<typeof streamText> | null> {
    if (!this.isAPIKeyAvailable()) {
      console.error('Groq API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
      return null;
    }

    const rateCheck = this.checkRateLimit(options?.model);
    if (!rateCheck.allowed) {
      console.error(rateCheck.reason);
      return null;
    }

    const model = options?.model || this.defaultModel;
    const supportsReasoning = REASONING_MODELS.includes(model);

    const messages: ModelMessage[] = options?.messages || [];

    if (messages.length === 0) {
      if (options?.systemPrompt) {
        messages.push({ role: 'system', content: options.systemPrompt });
      }
      messages.push({ role: 'user', content: prompt });
    }

    this.recordRequest();

    // Vercel AI SDK streamText ë°˜í™˜
    return streamText({
      model: groq(model),
      messages,
      maxOutputTokens: options?.maxTokens || 2048,
      temperature: options?.temperature ?? 0.7,
      // Reasoning ì§€ì›
      ...(supportsReasoning &&
        options?.enableReasoning && {
          experimental_providerOptions: {
            groq: {
              reasoningFormat: 'parsed',
              ...(options.reasoningEffort && {
                reasoningEffort: options.reasoningEffort,
              }),
            },
          },
        }),
    });
  }

  /**
   * ğŸ§  Groq ëª¨ë¸ Provider ê°€ì ¸ì˜¤ê¸° (ì§ì ‘ ì‚¬ìš©)
   *
   * @example
   * ```ts
   * const model = getGroqModel('llama-3.3-70b-versatile');
   * const result = await generateText({ model, messages: [...] });
   * ```
   */
  getModel(modelId?: GroqModel) {
    return groq(modelId || this.defaultModel);
  }
}

const groqAIManager = GroqAIManager.getInstance();

// ë‚´ë³´ë‚´ê¸° - ê¸°ì¡´ API í˜¸í™˜ì„± ìœ ì§€
export const getGroqAIKey = () => groqAIManager.getAPIKey();
export const isGroqAIAvailable = () => groqAIManager.isAPIKeyAvailable();
export const getGroqAIStatus = () => groqAIManager.getKeyStatus();
export const checkGroqAIRateLimit = (model?: GroqModel) =>
  groqAIManager.checkRateLimit(model);
export const recordGroqAIRequest = () => groqAIManager.recordRequest();
export const getGroqAIRateLimitStatus = (model?: GroqModel) =>
  groqAIManager.getRateLimitStatus(model);
export const generateGroqText = (prompt: string, options?: GenerateOptions) =>
  groqAIManager.generateText(prompt, options);
export const streamGroqText = (prompt: string, options?: StreamOptions) =>
  groqAIManager.streamTextResponse(prompt, options);
export const setGroqDefaultModel = (model: GroqModel) =>
  groqAIManager.setDefaultModel(model);
export const getGroqModel = (modelId?: GroqModel) =>
  groqAIManager.getModel(modelId);

export default groqAIManager;
