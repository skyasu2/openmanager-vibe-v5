/**
 * ğŸ¤– Google AI Unified Engine
 *
 * í•µì‹¬ ì—­í• :
 * 1. Provider ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (RAG, ML, Rule)
 * 2. Google AI API í†µí•©
 * 3. ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ ë¼ìš°íŒ…
 * 4. ìºì‹± ë° ì„±ëŠ¥ ìµœì í™”
 *
 * ì•„í‚¤í…ì²˜ ì›ì¹™:
 * - Google AI API = ì„¤ëª…Â·ìš”ì•½Â·ë³´ê³ ì„œ ìƒì„±
 * - Cloud Functions = ì „ì²˜ë¦¬Â·MLÂ·ë£° ê¸°ë°˜ ì²˜ë¦¬
 * - Provider = ì»¨í…ìŠ¤íŠ¸ ì œê³µì (RAG/ML/Rule)
 */

import type {
  IUnifiedEngine,
  UnifiedQueryRequest,
  UnifiedQueryResponse,
  ProviderContexts,
  IContextProvider,
  EngineHealthStatus,
  EngineConfig,
  ThinkingStep,
  GoogleAIPrompt,
} from './types';

import { GoogleAIError } from './types';

// Provider imports
import { RAGProvider } from '../providers/rag-provider';
import { MLProvider } from '../providers/ml-provider';
import { KoreanNLPProvider } from '../providers/korean-nlp-provider';

// PromptBuilder import
import { promptBuilder } from './prompt-builder';

// Usage Tracker import (í• ë‹¹ëŸ‰ ë³´í˜¸)
import { getGoogleAIUsageTracker } from '@/services/ai/GoogleAIUsageTracker';
import type { GoogleAIModel } from '@/services/ai/QueryDifficultyAnalyzer';

// ============================================================================
// Cache Entry
// ============================================================================

interface CacheEntry {
  response: UnifiedQueryResponse;
  timestamp: number;
}

// ============================================================================
// Google AI Unified Engine
// ============================================================================

export class GoogleAiUnifiedEngine implements IUnifiedEngine {
  // Provider ê´€ë¦¬
  private providers: Map<string, IContextProvider>;

  // ìºì‹œ
  private cache = new Map<string, CacheEntry>();
  private readonly cacheTTL: number;

  // ì„¤ì •
  private config: EngineConfig;

  // í†µê³„
  private stats = {
    totalRequests: 0,
    cacheHits: 0,
    cacheMisses: 0,
    errors: 0,
  };

  // í• ë‹¹ëŸ‰ ì¶”ì ê¸° (429 ì—ëŸ¬ ë°©ì§€)
  private usageTracker = getGoogleAIUsageTracker();

  constructor(config?: Partial<EngineConfig>) {
    // ê¸°ë³¸ ì„¤ì •
    this.config = {
      model: config?.model || 'gemini-2.0-flash-lite',
      temperature: config?.temperature ?? 0.7,
      maxTokens: config?.maxTokens || 2048,
      timeout: config?.timeout || 30000,
      cache: {
        enabled: config?.cache?.enabled ?? true,
        ttl: config?.cache?.ttl || 5 * 60 * 1000, // 5ë¶„
        maxSize: config?.cache?.maxSize || 100,
      },
      providers: {
        rag: {
          enabled: config?.providers?.rag?.enabled ?? true,
          maxResults: config?.providers?.rag?.maxResults || 5,
          threshold: config?.providers?.rag?.threshold || 0.7,
        },
        ml: {
          enabled: config?.providers?.ml?.enabled ?? true,
          models: config?.providers?.ml?.models || [
            'anomaly-detection',
            'trend-analysis',
          ],
        },
        rule: {
          enabled: config?.providers?.rule?.enabled ?? true,
          confidenceThreshold:
            config?.providers?.rule?.confidenceThreshold || 0.6,
        },
      },
      enableMcp: config?.enableMcp ?? false,
    };

    this.cacheTTL = this.config.cache.ttl;

    // Provider ì´ˆê¸°í™”
    this.providers = new Map<
      string,
      RAGProvider | MLProvider | KoreanNLPProvider
    >([
      ['rag', new RAGProvider()],
      ['ml', new MLProvider()],
      ['rule', new KoreanNLPProvider()], // Korean NLP = Rule Provider
    ]);
  }

  /**
   * ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì„œë“œ
   */
  async query(request: UnifiedQueryRequest): Promise<UnifiedQueryResponse> {
    const startTime = Date.now();
    const thinkingSteps: ThinkingStep[] = [];

    try {
      this.stats.totalRequests++;

      // Step 1: ìºì‹œ í™•ì¸
      if (this.config.cache.enabled && request.options?.cached !== false) {
        const cached = this.getFromCache(request);
        if (cached) {
          this.stats.cacheHits++;
          return {
            ...cached.response,
            metadata: {
              ...cached.response.metadata,
              cacheHit: true,
              processingTime: Date.now() - startTime,
            },
          };
        }
      }
      this.stats.cacheMisses++;

      // Step 2: ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ Provider í™œì„±í™” ê²°ì •
      thinkingSteps.push({
        step: 'provider-selection',
        description: `ì‹œë‚˜ë¦¬ì˜¤ "${request.scenario}"ì— ë§ëŠ” Provider ì„ íƒ`,
        status: 'completed',
        timestamp: Date.now(),
        duration: Date.now() - startTime,
      });

      const enabledProviders = this.selectProviders(request);

      // Step 3: ë³‘ë ¬ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
      const contextStartTime = Date.now();
      thinkingSteps.push({
        step: 'context-collection',
        description: `${enabledProviders.length}ê°œ Providerì—ì„œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘`,
        status: 'pending',
        timestamp: Date.now(),
      });

      const contexts = await this.collectContexts(request, enabledProviders);

      const lastContextStep = thinkingSteps[thinkingSteps.length - 1];
      if (lastContextStep) {
        lastContextStep.status = 'completed';
        lastContextStep.duration = Date.now() - contextStartTime;
      }

      // Step 4: í”„ë¡¬í”„íŠ¸ ìƒì„±
      const promptStartTime = Date.now();
      thinkingSteps.push({
        step: 'prompt-generation',
        description: 'ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±',
        status: 'pending',
        timestamp: Date.now(),
      });

      const prompt = this.buildPrompt(request, contexts);

      const lastPromptStep = thinkingSteps[thinkingSteps.length - 1];
      if (lastPromptStep) {
        lastPromptStep.status = 'completed';
        lastPromptStep.duration = Date.now() - promptStartTime;
      }

      // ğŸ›¡ï¸ í• ë‹¹ëŸ‰ ë³´í˜¸ ë¡œì§ - API í˜¸ì¶œ ì „ ì²´í¬
      let selectedModel = this.config.model as GoogleAIModel;

      if (!this.usageTracker.canUseModel(selectedModel)) {
        console.warn(
          `âš ï¸ [GoogleAiUnifiedEngine] ${selectedModel} í• ë‹¹ëŸ‰ ì´ˆê³¼, ëŒ€ì²´ ëª¨ë¸ í™•ì¸ ì¤‘...`
        );

        // ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ì²´ ëª¨ë¸ ì°¾ê¸°
        const availableModels = this.usageTracker.getAvailableModels();

        if (availableModels.length === 0) {
          // ëª¨ë“  ëª¨ë¸ í• ë‹¹ëŸ‰ ì´ˆê³¼ - ì—ëŸ¬ ë°˜í™˜
          const errorMsg =
            'Google AI ëª¨ë“  ëª¨ë¸ì˜ í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
          console.error(`âŒ [GoogleAiUnifiedEngine] ${errorMsg}`);
          throw new GoogleAIError(errorMsg);
        }

        // ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ì „í™˜
        const fallbackModel = availableModels.at(0);
        if (!fallbackModel) {
          throw new GoogleAIError(
            'Internal error: availableModels array is empty after length check'
          );
        }

        console.log(
          `âœ… [GoogleAiUnifiedEngine] ëŒ€ì²´ ëª¨ë¸ ì‚¬ìš©: ${selectedModel} â†’ ${fallbackModel}`
        );
        selectedModel = fallbackModel;
        this.config.model = selectedModel;
      }

      // Step 5: Google AI API í˜¸ì¶œ
      const aiStartTime = Date.now();
      thinkingSteps.push({
        step: 'google-ai-call',
        description: `${this.config.model} ëª¨ë¸ í˜¸ì¶œ`,
        status: 'pending',
        timestamp: Date.now(),
      });

      const aiResponse = await this.callGoogleAI(
        prompt,
        request.options?.temperature
      );
      const aiLatency = Date.now() - aiStartTime;
      const tokensUsed = this.estimateTokens(
        prompt.systemInstruction + prompt.userMessage + aiResponse
      );

      // ğŸ“Š ì‚¬ìš©ëŸ‰ ì¶”ì  (í• ë‹¹ëŸ‰ ê´€ë¦¬)
      this.usageTracker.recordUsage({
        model: selectedModel,
        timestamp: Date.now(),
        requestCount: 1,
        tokenCount: tokensUsed,
        latency: aiLatency,
        success: true,
      });

      const lastAIStep = thinkingSteps[thinkingSteps.length - 1];
      if (lastAIStep) {
        lastAIStep.status = 'completed';
        lastAIStep.duration = Date.now() - aiStartTime;
      }

      // Step 6: ì‘ë‹µ ìƒì„±
      const response: UnifiedQueryResponse = {
        success: true,
        response: aiResponse,
        scenario: request.scenario,
        metadata: {
          engine: 'google-ai-unified',
          model: this.config.model,
          tokensUsed,
          processingTime: Date.now() - startTime,
          cacheHit: false,
          providersUsed: enabledProviders.map((p) => p.name),
          timestamp: new Date(),
        },
        contexts: this.sanitizeContexts(contexts),
        thinkingSteps: request.options?.includeThinking
          ? thinkingSteps
          : undefined,
      };

      // Step 7: ìºì‹œ ì €ì¥
      if (this.config.cache.enabled) {
        this.setCache(request, response);
      }

      return response;
    } catch (error) {
      this.stats.errors++;

      // ì—ëŸ¬ë¥¼ ThinkingStepìœ¼ë¡œ ê¸°ë¡
      if (thinkingSteps.length > 0) {
        const lastStep = thinkingSteps[thinkingSteps.length - 1];
        if (lastStep && lastStep.status === 'pending') {
          lastStep.status = 'failed';
          lastStep.error =
            error instanceof Error ? error.message : String(error);
        }
      }

      return {
        success: false,
        response: '',
        scenario: request.scenario,
        metadata: {
          engine: 'google-ai-unified',
          model: this.config.model,
          tokensUsed: 0,
          processingTime: Date.now() - startTime,
          cacheHit: false,
          providersUsed: [],
          timestamp: new Date(),
        },
        thinkingSteps: request.options?.includeThinking
          ? thinkingSteps
          : undefined,
        error:
          error instanceof Error
            ? error.message
            : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
      };
    }
  }

  /**
   * Provider ì„ íƒ (ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜)
   */
  private selectProviders(request: UnifiedQueryRequest): IContextProvider[] {
    const providers: IContextProvider[] = [];

    for (const [_key, provider] of this.providers.entries()) {
      // 1. Providerê°€ ì‹œë‚˜ë¦¬ì˜¤ì— í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
      if (!provider.isEnabled(request.scenario)) {
        continue;
      }

      // 2. ì‚¬ìš©ì ì˜µì…˜ í™•ì¸ (ìˆ˜ë™ ë¹„í™œì„±í™”)
      if (_key === 'rag' && request.options?.enableRAG === false) continue;
      if (_key === 'ml' && request.options?.enableML === false) continue;
      if (_key === 'rule' && request.options?.enableRules === false) continue;

      // 3. ì—”ì§„ ì„¤ì • í™•ì¸ (ì „ì—­ ë¹„í™œì„±í™”)
      if (_key === 'rag' && !this.config.providers.rag.enabled) continue;
      if (_key === 'ml' && !this.config.providers.ml.enabled) continue;
      if (_key === 'rule' && !this.config.providers.rule.enabled) continue;

      providers.push(provider);
    }

    return providers;
  }

  /**
   * ë³‘ë ¬ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
   */
  private async collectContexts(
    request: UnifiedQueryRequest,
    providers: IContextProvider[]
  ): Promise<ProviderContexts> {
    const contexts: ProviderContexts = {};

    // ë³‘ë ¬ ì‹¤í–‰
    const results = await Promise.allSettled(
      providers.map((provider) =>
        provider.getContext(request.query, {
          maxResults: this.config.providers.rag.maxResults,
          threshold: this.config.providers.rag.threshold,
          timeoutMs: this.config.timeout,
          ...request.options,
        })
      )
    );

    // ê²°ê³¼ ìˆ˜ì§‘
    providers.forEach((provider, index) => {
      const result = results[index];
      if (result && result.status === 'fulfilled') {
        const context = result.value;
        if (context.type === 'rag') contexts.rag = context;
        if (context.type === 'ml') contexts.ml = context;
        if (context.type === 'rule') contexts.rule = context;
      } else if (result && result.status === 'rejected') {
        console.warn(
          `[GoogleAiUnifiedEngine] Provider ${provider.name} failed:`,
          result.reason
        );
      }
    });

    return contexts;
  }

  /**
   * í”„ë¡¬í”„íŠ¸ ìƒì„± (PromptBuilder ì‚¬ìš©)
   */
  private buildPrompt(
    request: UnifiedQueryRequest,
    contexts: ProviderContexts
  ): GoogleAIPrompt {
    return promptBuilder.build(
      {
        query: request.query,
        contexts,
        options: request.options,
      },
      request.scenario
    );
  }

  /**
   * Google AI API í˜¸ì¶œ (GoogleAIPrompt ì‚¬ìš©)
   */
  private async callGoogleAI(
    prompt: GoogleAIPrompt,
    temperature?: number
  ): Promise<string> {
    const apiKey =
      process.env.GOOGLE_AI_API_KEY ||
      process.env.NEXT_PUBLIC_GOOGLE_AI_API_KEY;

    if (!apiKey) {
      throw new GoogleAIError(
        'Google AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_AI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.'
      );
    }

    try {
      const response = await fetch(
        `https://generativelanguage.googleapis.com/v1beta/models/${this.config.model}:generateContent?key=${apiKey}`,
        {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            system_instruction: {
              parts: [{ text: prompt.systemInstruction }],
            },
            contents: [
              {
                role: 'user',
                parts: [{ text: prompt.userMessage }],
              },
            ],
            generationConfig: {
              temperature: temperature ?? this.config.temperature,
              maxOutputTokens: this.config.maxTokens,
            },
          }),
          signal: AbortSignal.timeout(this.config.timeout),
        }
      );

      if (!response.ok) {
        const errorText = await response.text();
        throw new GoogleAIError(
          `Google AI API ì˜¤ë¥˜: ${response.status}`,
          response.status,
          errorText
        );
      }

      const data = await response.json();

      if (!data.candidates || data.candidates.length === 0) {
        throw new GoogleAIError(
          'Google AI API ì‘ë‹µì— ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.',
          undefined,
          data
        );
      }

      const candidate = data.candidates[0];
      const text = candidate.content?.parts?.[0]?.text;

      if (!text) {
        throw new GoogleAIError(
          'Google AI API ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.',
          undefined,
          data
        );
      }

      return text;
    } catch (error) {
      if (error instanceof GoogleAIError) {
        throw error;
      }

      throw new GoogleAIError(
        error instanceof Error ? error.message : 'Google AI API í˜¸ì¶œ ì‹¤íŒ¨',
        undefined,
        error
      );
    }
  }

  /**
   * í† í° ìˆ˜ ì¶”ì • (ê°„ë‹¨í•œ ë²„ì „)
   */
  private estimateTokens(text: string): number {
    // ê°„ë‹¨í•œ ì¶”ì •: 1 í† í° â‰ˆ 4 ê¸€ì
    return Math.ceil(text.length / 4);
  }

  /**
   * ì»¨í…ìŠ¤íŠ¸ ì •ë¦¬ (ë¯¼ê°í•œ ì •ë³´ ì œê±°)
   */
  private sanitizeContexts(contexts: ProviderContexts): ProviderContexts {
    // í˜„ì¬ëŠ” ê·¸ëŒ€ë¡œ ë°˜í™˜, í–¥í›„ í•„ìš” ì‹œ ë¯¼ê°í•œ ì •ë³´ ì œê±° ë¡œì§ ì¶”ê°€
    return contexts;
  }

  /**
   * ìºì‹œ í‚¤ ìƒì„±
   */
  private getCacheKey(request: UnifiedQueryRequest): string {
    const normalized = request.query.trim().toLowerCase();
    const scenarioNormalized = request.scenario.toLowerCase();
    return `unified:${scenarioNormalized}:${normalized}`;
  }

  /**
   * ìºì‹œ ì¡°íšŒ
   */
  private getFromCache(request: UnifiedQueryRequest): CacheEntry | null {
    const key = this.getCacheKey(request);
    const cached = this.cache.get(key);

    if (!cached) return null;

    const now = Date.now();
    if (now - cached.timestamp > this.cacheTTL) {
      this.cache.delete(key);
      return null;
    }

    // LRU: ìºì‹œ íˆíŠ¸ ì‹œ ìµœì‹  í•­ëª©ìœ¼ë¡œ ê°±ì‹  (ì¬ì‚½ì…)
    this.cache.delete(key);
    this.cache.set(key, cached);

    return cached;
  }

  /**
   * ìºì‹œ ì €ì¥
   */
  private setCache(
    request: UnifiedQueryRequest,
    response: UnifiedQueryResponse
  ): void {
    const cacheKey = this.getCacheKey(request);
    this.cache.set(cacheKey, { response, timestamp: Date.now() });

    // ìºì‹œ í¬ê¸° ì œí•œ
    if (this.cache.size > this.config.cache.maxSize) {
      const firstKey = this.cache.keys().next().value;
      if (firstKey) this.cache.delete(firstKey);
    }
  }

  /**
   * í—¬ìŠ¤ ì²´í¬
   */
  async healthCheck(): Promise<EngineHealthStatus> {
    const providers: Array<{
      name: string;
      type: 'rag' | 'ml' | 'rule';
      healthy: boolean;
      responseTime?: number;
      error?: string;
    }> = [];

    // Provider ìƒíƒœ í™•ì¸ (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬)
    for (const [_key, provider] of this.providers.entries()) {
      const startTime = Date.now();
      try {
        await provider.getContext('health check', { timeoutMs: 5000 });
        providers.push({
          name: provider.name,
          type: provider.type,
          healthy: true,
          responseTime: Date.now() - startTime,
        });
      } catch (error) {
        providers.push({
          name: provider.name,
          type: provider.type,
          healthy: false,
          error: error instanceof Error ? error.message : String(error),
        });
      }
    }

    // Google AI ìƒíƒœ í™•ì¸
    let googleAIStatus: {
      available: boolean;
      latency?: number;
      error?: string;
    };
    const aiStartTime = Date.now();
    try {
      await this.callGoogleAI({
        systemInstruction: 'Health check',
        userMessage: 'ping',
        estimatedTokens: 10,
      });
      googleAIStatus = {
        available: true,
        latency: Date.now() - aiStartTime,
      };
    } catch (error) {
      googleAIStatus = {
        available: false,
        error: error instanceof Error ? error.message : String(error),
      };
    }

    // ìºì‹œ í†µê³„
    const hitRate =
      this.stats.totalRequests > 0
        ? this.stats.cacheHits / this.stats.totalRequests
        : 0;

    return {
      healthy: providers.every((p) => p.healthy) && googleAIStatus.available,
      message:
        providers.every((p) => p.healthy) && googleAIStatus.available
          ? 'All systems operational'
          : 'Some systems are unavailable',
      providers,
      googleAIStatus,
      cacheStatus: {
        hitRate,
        size: this.cache.size,
        maxSize: this.config.cache.maxSize,
      },
      timestamp: new Date(),
    };
  }

  /**
   * ì—”ì§„ ì„¤ì • ì—…ë°ì´íŠ¸
   */
  configure(config: Partial<EngineConfig>): void {
    this.config = {
      ...this.config,
      ...config,
      cache: { ...this.config.cache, ...config.cache },
      providers: {
        rag: { ...this.config.providers.rag, ...config.providers?.rag },
        ml: { ...this.config.providers.ml, ...config.providers?.ml },
        rule: { ...this.config.providers.rule, ...config.providers?.rule },
      },
    };
  }

  /**
   * í†µê³„ ì¡°íšŒ
   */
  getStats() {
    return {
      ...this.stats,
      hitRate:
        this.stats.totalRequests > 0
          ? this.stats.cacheHits / this.stats.totalRequests
          : 0,
    };
  }
}
