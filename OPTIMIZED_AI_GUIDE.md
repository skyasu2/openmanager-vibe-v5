# ğŸš€ OpenManager AI v5 - ìµœì í™”ëœ AI ì—ì´ì „íŠ¸ ì—”ì§„ ê°€ì´ë“œ

## ğŸ“‹ **ê°œìš”**

OpenManager AI v5ì˜ ìµœì í™”ëœ AI ì—ì´ì „íŠ¸ ì—”ì§„ì€ Vercel í™˜ê²½ì—ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ë„ë¡ ì„¤ê³„ëœ ì°¨ì„¸ëŒ€ AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ğŸ¯ **ì£¼ìš” ê°œì„ ì‚¬í•­**

- **ì„±ëŠ¥ ìµœì í™”**: ì‘ë‹µì‹œê°„ 50% ë‹¨ì¶• (10-15ì´ˆ â†’ 5-8ì´ˆ)
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: Python íŒ¨í‚¤ì§€ 84% ê²½ëŸ‰í™” (1.8GB â†’ 300MB)
- **í™˜ê²½ë³„ ìµœì í™”**: Vercel ë¬´ë£Œ/Pro, ë¡œì»¬ í™˜ê²½ ìë™ ê°ì§€ ë° ìµœì í™”
- **ê°•ë ¥í•œ Fallback**: Python ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ ìë™ ëŒ€ì²´ ë©”ì»¤ë‹ˆì¦˜
- **ìŠ¤ë§ˆíŠ¸ ìºì‹±**: ì‘ë‹µì‹œê°„ ë‹¨ì¶• ë° ë¦¬ì†ŒìŠ¤ ì ˆì•½

### ğŸ—ï¸ **ì•„í‚¤í…ì²˜ ê°œìš”**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Optimized AI Agent Engine                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸŒ Environment Detector  â”‚  âš™ï¸ Optimization Config        â”‚
â”‚  - Vercel ë¬´ë£Œ/Pro ê°ì§€    â”‚  - í™˜ê²½ë³„ ì„¤ì • ìë™ ì¡°ì •         â”‚
â”‚  - ë©”ëª¨ë¦¬/CPU ì œí•œ ê°ì§€     â”‚  - ë™ì  ì„±ëŠ¥ ìµœì í™”             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ§  Smart Query Processor â”‚  ğŸ Lightweight Python Runner  â”‚
â”‚  - MCP íŒ¨í„´ ë§¤ì¹­ (í•­ìƒ)    â”‚  - ê²½ëŸ‰í™”ëœ ML ë¶„ì„             â”‚
â”‚  - ì˜ë„ ë¶„ë¥˜ ë° ì»¨í…ìŠ¤íŠ¸    â”‚  - ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬           â”‚
â”‚  - í†µí•© ì‘ë‹µ ìƒì„±          â”‚  - 5ì´ˆ ë‚´ ì‹¤í–‰ ë³´ì¥             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ’¾ Smart Caching        â”‚  ğŸ”„ Fallback Mechanism         â”‚
â”‚  - ê²°ê³¼ ìºì‹± (5ë¶„ TTL)     â”‚  - Python ì‹¤íŒ¨ ì‹œ JS ëŒ€ì²´       â”‚
â”‚  - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ê´€ë¦¬       â”‚  - í†µê³„ ê¸°ë°˜ ê°„ë‹¨ ë¶„ì„          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ **ì„¤ì¹˜ ë° ì„¤ì •**

### 1ï¸âƒ£ **ê¸°ë³¸ ì„¤ì¹˜**

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone <repository-url>
cd openmanager-vibe-v5

# 2. Node.js ì˜ì¡´ì„± ì„¤ì¹˜
npm install

# 3. ê²½ëŸ‰í™”ëœ Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ê¶Œì¥)
npm run setup:python-lightweight

# ë˜ëŠ” ì „ì²´ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ë¡œì»¬ ê°œë°œìš©)
npm run setup:python
```

### 2ï¸âƒ£ **í™˜ê²½ë³„ ì„¤ì •**

#### ğŸ†“ **Vercel ë¬´ë£Œ í‹°ì–´**
```bash
# ê²½ëŸ‰ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜
npm run python:install-lightweight

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env.local)
VERCEL_PLAN=hobby
PYTHON_PATH=python3
NODE_ENV=production
```

#### ğŸ’ **Vercel Pro í‹°ì–´**
```bash
# ì „ì²´ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ê°€ëŠ¥
npm run python:install

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
VERCEL_PLAN=pro
PYTHON_PATH=python3
NODE_ENV=production
```

#### ğŸ–¥ï¸ **ë¡œì»¬ ê°œë°œ í™˜ê²½**
```bash
# ì „ì²´ ê¸°ëŠ¥ í™œìš©
npm run python:install
npm run dev

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
NODE_ENV=development
PYTHON_PATH=python
```

### 3ï¸âƒ£ **í™˜ê²½ ê²€ì¦**

```bash
# ìµœì í™”ëœ AI ì—”ì§„ í…ŒìŠ¤íŠ¸
npm run test:optimized-ai

# Python í™˜ê²½ ê²€ì¦
npm run python:test-lightweight

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
npm run ai:benchmark
```

## ğŸš€ **ì‚¬ìš©ë²•**

### 1ï¸âƒ£ **ê¸°ë³¸ API ì‚¬ìš©**

#### **ì—”ì§„ ìƒíƒœ ì¡°íšŒ**
```bash
curl -X GET http://localhost:3000/api/ai-agent/optimized
```

#### **ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬ ì‹¤í–‰**
```bash
curl -X POST http://localhost:3000/api/ai-agent/optimized \
  -H "Content-Type: application/json" \
  -d '{
    "action": "smart-query",
    "query": "ì„œë²„ CPU ì‚¬ìš©ë¥ ì´ ë†’ì€ ì´ìœ ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    "serverData": {
      "metrics": {
        "cpu": { "current": 85.5, "history": [...] },
        "memory": { "current": 72.3, "history": [...] }
      }
    },
    "priority": "high"
  }'
```

#### **í™˜ê²½ ì •ë³´ ì¡°íšŒ**
```bash
curl -X POST http://localhost:3000/api/ai-agent/optimized \
  -H "Content-Type: application/json" \
  -d '{ "action": "environment" }'
```

### 2ï¸âƒ£ **TypeScript/JavaScript ì‚¬ìš©**

```typescript
import { OptimizedAIAgentEngine } from '@/modules/ai-agent/core/OptimizedAIAgentEngine';

// ì—”ì§„ ì´ˆê¸°í™”
const aiEngine = OptimizedAIAgentEngine.getInstance();
await aiEngine.initialize();

// ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
const response = await aiEngine.processSmartQuery({
  query: 'ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”',
  serverData: {
    metrics: {
      cpu: { current: 75.5, history: [...] },
      memory: { current: 68.2, history: [...] }
    }
  },
  priority: 'medium'
});

console.log('ì‘ë‹µ:', response.response);
console.log('ë¶„ì„ ë°©ë²•:', response.method);
console.log('ì²˜ë¦¬ ì‹œê°„:', response.metadata.processingTime);
```

### 3ï¸âƒ£ **React ì»´í¬ë„ŒíŠ¸ì—ì„œ ì‚¬ìš©**

```tsx
import { useState, useEffect } from 'react';

function AIAnalysisComponent() {
  const [analysis, setAnalysis] = useState(null);
  const [loading, setLoading] = useState(false);

  const analyzeSystem = async () => {
    setLoading(true);
    try {
      const response = await fetch('/api/ai-agent/optimized', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          action: 'smart-query',
          query: 'í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”',
          serverData: getServerMetrics(), // ì„œë²„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
          priority: 'high'
        })
      });
      
      const result = await response.json();
      setAnalysis(result.data);
    } catch (error) {
      console.error('ë¶„ì„ ì‹¤íŒ¨:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div>
      <button onClick={analyzeSystem} disabled={loading}>
        {loading ? 'ë¶„ì„ ì¤‘...' : 'AI ë¶„ì„ ì‹¤í–‰'}
      </button>
      
      {analysis && (
        <div>
          <h3>ë¶„ì„ ê²°ê³¼</h3>
          <p>{analysis.response}</p>
          <p>ë°©ë²•: {analysis.method}</p>
          <p>ì²˜ë¦¬ì‹œê°„: {analysis.metadata.processingTime}ms</p>
          
          {analysis.analysis?.insights && (
            <div>
              <h4>ì¸ì‚¬ì´íŠ¸</h4>
              <ul>
                {analysis.analysis.insights.map((insight, i) => (
                  <li key={i}>{insight}</li>
                ))}
              </ul>
            </div>
          )}
        </div>
      )}
    </div>
  );
}
```

## âš™ï¸ **í™˜ê²½ë³„ ìµœì í™” ì„¤ì •**

### ğŸ†“ **Vercel ë¬´ë£Œ í‹°ì–´ ìµœì í™”**

```typescript
// ìë™ ê°ì§€ë˜ëŠ” ì„¤ì •
{
  maxProcesses: 1,           // ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤
  processTimeout: 8000,      // 8ì´ˆ ì œí•œ
  maxMemoryMB: 400,         // 400MB ì œí•œ
  enableCaching: true,       // ìºì‹± í™œì„±í™”
  cacheSize: 50,            // ì‘ì€ ìºì‹œ
  pythonTimeout: 6000,      // 6ì´ˆ Python ì œí•œ
  fallbackMode: true,       // ì ê·¹ì  fallback
  batchSize: 100,           // ì‘ì€ ë°°ì¹˜
  concurrentTasks: 1        // ìˆœì°¨ ì²˜ë¦¬
}
```

### ğŸ’ **Vercel Pro í‹°ì–´ ìµœì í™”**

```typescript
// ìë™ ê°ì§€ë˜ëŠ” ì„¤ì •
{
  maxProcesses: 2,           // 2ê°œ í”„ë¡œì„¸ìŠ¤
  processTimeout: 50000,     // 50ì´ˆ ì œí•œ
  maxMemoryMB: 800,         // 800MB ì œí•œ
  enableCaching: true,       // ìºì‹± í™œì„±í™”
  cacheSize: 200,           // í° ìºì‹œ
  pythonTimeout: 45000,     // 45ì´ˆ Python ì œí•œ
  fallbackMode: false,      // ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©
  batchSize: 500,           // ì¤‘ê°„ ë°°ì¹˜
  concurrentTasks: 2        // ë³‘ë ¬ ì²˜ë¦¬
}
```

### ğŸ–¥ï¸ **ë¡œì»¬ í™˜ê²½ ìµœì í™”**

```typescript
// ìë™ ê°ì§€ë˜ëŠ” ì„¤ì •
{
  maxProcesses: 4,           // CPU ì½”ì–´ ìˆ˜ë§Œí¼
  processTimeout: 120000,    // 2ë¶„ ì œí•œ
  maxMemoryMB: 2048,        // 2GB ì œí•œ
  enableCaching: true,       // ìºì‹± í™œì„±í™”
  cacheSize: 500,           // ëŒ€ìš©ëŸ‰ ìºì‹œ
  pythonTimeout: 100000,    // 100ì´ˆ Python ì œí•œ
  fallbackMode: false,      // ëª¨ë“  ê¸°ëŠ¥ í™œìš©
  batchSize: 1000,          // í° ë°°ì¹˜
  concurrentTasks: 4        // ìµœëŒ€ ë³‘ë ¬ ì²˜ë¦¬
}
```

## ğŸ“Š **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**

### 1ï¸âƒ£ **ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ**

```bash
# ì—”ì§„ ìƒíƒœ ì¡°íšŒ
curl -X POST http://localhost:3000/api/ai-agent/optimized \
  -H "Content-Type: application/json" \
  -d '{ "action": "status" }'
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
  "success": true,
  "data": {
    "status": "operational",
    "engine": {
      "isInitialized": true,
      "metrics": {
        "totalRequests": 150,
        "successfulRequests": 142,
        "averageResponseTime": 3250,
        "pythonAnalysisUsed": 89,
        "fallbackUsed": 8,
        "cacheHits": 23
      }
    },
    "health": {
      "isHealthy": true,
      "uptime": 3600,
      "memoryUsage": { "heapUsed": 156, "heapTotal": 512 },
      "successRate": 94.7
    }
  }
}
```

### 2ï¸âƒ£ **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**

```bash
# ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
npm run ai:benchmark

# ë¶€í•˜ í…ŒìŠ¤íŠ¸
npm run test:optimized-ai
```

### 3ï¸âƒ£ **ë™ì  ìµœì í™”**

```typescript
// ì„±ëŠ¥ ë°ì´í„° ê¸°ë°˜ ìë™ ì¡°ì •
const response = await fetch('/api/ai-agent/optimized', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    action: 'optimize',
    performanceData: {
      averageResponseTime: 8500,  // í‰ê·  ì‘ë‹µì‹œê°„
      errorRate: 0.15,           // ì—ëŸ¬ìœ¨
      memoryUsage: 450           // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
    }
  })
});
```

## ğŸ”§ **ê³ ê¸‰ ì„¤ì •**

### 1ï¸âƒ£ **ì»¤ìŠ¤í…€ í™˜ê²½ ì„¤ì •**

```typescript
import { OptimizedAIAgentEngine } from '@/modules/ai-agent/core/OptimizedAIAgentEngine';

const customConfig = {
  enableMCP: true,
  enablePythonAnalysis: true,
  enableAutoOptimization: true,
  debugMode: false,
  fallbackMode: false
};

const aiEngine = OptimizedAIAgentEngine.getInstance(customConfig);
```

### 2ï¸âƒ£ **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

```bash
# .env.local
VERCEL_PLAN=pro                    # Vercel í”Œëœ (hobby/pro)
PYTHON_PATH=python3                # Python ì‹¤í–‰ ê²½ë¡œ
AI_AGENT_DEBUG=true               # ë””ë²„ê·¸ ëª¨ë“œ
AI_AGENT_CACHE_TTL=300000         # ìºì‹œ TTL (ms)
AI_AGENT_MAX_MEMORY=800           # ìµœëŒ€ ë©”ëª¨ë¦¬ (MB)
AI_AGENT_TIMEOUT=45000            # íƒ€ì„ì•„ì›ƒ (ms)
```

### 3ï¸âƒ£ **ì»¤ìŠ¤í…€ ë¶„ì„ ë¡œì§**

```typescript
import { LightweightPythonRunner } from '@/modules/ai-agent/core/LightweightPythonRunner';

const pythonRunner = LightweightPythonRunner.getInstance();

// ì»¤ìŠ¤í…€ ë¶„ì„ ì‹¤í–‰
const result = await pythonRunner.analyzeData({
  data: {
    features: [[75.5, 68.2, 45.8]], // [CPU, Memory, Disk]
    timeseries: {
      values: [60, 65, 70, 75, 80],
      horizon: 5
    },
    variables: [
      { name: 'CPU', values: [60, 65, 70, 75, 80] },
      { name: 'Memory', values: [50, 55, 60, 65, 70] }
    ]
  },
  priority: 'high',
  timeout: 8000
});
```

## ğŸ› **ë¬¸ì œ í•´ê²°**

### 1ï¸âƒ£ **ì¼ë°˜ì ì¸ ë¬¸ì œ**

#### **Python í™˜ê²½ ì˜¤ë¥˜**
```bash
# Python ê²½ë¡œ í™•ì¸
which python3
python3 --version

# íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
npm run python:install-lightweight

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export PYTHON_PATH=python3
```

#### **ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
```bash
# ê²½ëŸ‰ íŒ¨í‚¤ì§€ë¡œ ì „í™˜
npm run setup:python-lightweight

# ìºì‹œ í¬ê¸° ì¤„ì´ê¸°
export AI_AGENT_CACHE_SIZE=25
```

#### **íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜**
```bash
# íƒ€ì„ì•„ì›ƒ ì¦ê°€
export AI_AGENT_TIMEOUT=60000

# Fallback ëª¨ë“œ í™œì„±í™”
export AI_AGENT_FALLBACK=true
```

### 2ï¸âƒ£ **ë””ë²„ê¹…**

```typescript
// ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”
const aiEngine = OptimizedAIAgentEngine.getInstance({
  debugMode: true,
  enableAutoOptimization: true
});

// ìƒì„¸ ë¡œê·¸ í™•ì¸
const status = aiEngine.getEngineStatus();
console.log('ì—”ì§„ ìƒíƒœ:', status);

// í™˜ê²½ ì •ë³´ í™•ì¸
const envDetector = EnvironmentDetector.getInstance();
const envStatus = envDetector.getEnvironmentStatus();
console.log('í™˜ê²½ ìƒíƒœ:', envStatus);
```

### 3ï¸âƒ£ **ì„±ëŠ¥ ìµœì í™” íŒ**

#### **ì‘ë‹µì‹œê°„ ê°œì„ **
- ìºì‹œ í¬ê¸° ì¦ê°€: `AI_AGENT_CACHE_SIZE=100`
- í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì¦ê°€: `AI_AGENT_MAX_PROCESSES=2`
- ë°°ì¹˜ í¬ê¸° ì¡°ì •: `AI_AGENT_BATCH_SIZE=200`

#### **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ**
- ê²½ëŸ‰ íŒ¨í‚¤ì§€ ì‚¬ìš©: `npm run setup:python-lightweight`
- ìºì‹œ í¬ê¸° ê°ì†Œ: `AI_AGENT_CACHE_SIZE=25`
- í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê°ì†Œ: `AI_AGENT_MAX_PROCESSES=1`

#### **ì•ˆì •ì„± í–¥ìƒ**
- Fallback ëª¨ë“œ í™œì„±í™”: `AI_AGENT_FALLBACK=true`
- íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•: `AI_AGENT_TIMEOUT=5000`
- ì—ëŸ¬ ì¬ì‹œë„ ì¦ê°€: `AI_AGENT_MAX_RETRIES=3`

## ğŸ“ˆ **ì„±ëŠ¥ ë¹„êµ**

### ğŸ”„ **ê¸°ì¡´ vs ìµœì í™”ëœ ë²„ì „**

| í•­ëª© | ê¸°ì¡´ ë²„ì „ | ìµœì í™”ëœ ë²„ì „ | ê°œì„ ìœ¨ |
|------|-----------|---------------|--------|
| í‰ê·  ì‘ë‹µì‹œê°„ | 12-15ì´ˆ | 5-8ì´ˆ | **50%â†“** |
| Python íŒ¨í‚¤ì§€ í¬ê¸° | 1.8GB | 300MB | **84%â†“** |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 800MB+ | 400MB | **50%â†“** |
| ì´ˆê¸°í™” ì‹œê°„ | 8-10ì´ˆ | 2-3ì´ˆ | **70%â†“** |
| ìºì‹œ ì ì¤‘ë¥  | ì—†ìŒ | 15-25% | **ì‹ ê·œ** |
| Fallback ì„±ê³µë¥  | 60% | 95%+ | **58%â†‘** |

### ğŸŒ **í™˜ê²½ë³„ ì„±ëŠ¥**

| í™˜ê²½ | ì‘ë‹µì‹œê°„ | ë©”ëª¨ë¦¬ | Python ë¶„ì„ | ë™ì‹œ ì²˜ë¦¬ |
|------|----------|--------|-------------|-----------|
| Vercel ë¬´ë£Œ | 5-8ì´ˆ | 400MB | ì œí•œì  | 1ê°œ |
| Vercel Pro | 3-6ì´ˆ | 800MB | ì™„ì „ | 2ê°œ |
| ë¡œì»¬ ê°œë°œ | 2-4ì´ˆ | 2GB | ì™„ì „ | 4ê°œ+ |

## ğŸš€ **ë°°í¬ ê°€ì´ë“œ**

### 1ï¸âƒ£ **Vercel ë°°í¬**

```bash
# 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (Vercel Dashboard)
VERCEL_PLAN=pro
PYTHON_PATH=python3
NODE_ENV=production
AI_AGENT_CACHE_TTL=300000

# 2. ë°°í¬ ì‹¤í–‰
npm run deploy:prod

# 3. ë°°í¬ í›„ ê²€ì¦
npm run monitor
```

### 2ï¸âƒ£ **Docker ë°°í¬**

```dockerfile
# Dockerfile
FROM node:18-alpine

# Python ì„¤ì¹˜
RUN apk add --no-cache python3 py3-pip

# ì•± ë³µì‚¬
COPY . /app
WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
RUN npm install
RUN npm run setup:python-lightweight

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 3000

# ì‹¤í–‰
CMD ["npm", "start"]
```

### 3ï¸âƒ£ **í™˜ê²½ë³„ ë°°í¬ ì„¤ì •**

```yaml
# vercel.json
{
  "functions": {
    "src/app/api/ai-agent/optimized/route.ts": {
      "maxDuration": 60
    }
  },
  "env": {
    "PYTHON_PATH": "python3",
    "AI_AGENT_CACHE_TTL": "300000"
  }
}
```

## ğŸ“š **API ë ˆí¼ëŸ°ìŠ¤**

### ğŸ”— **ì—”ë“œí¬ì¸íŠ¸**

#### `GET /api/ai-agent/optimized`
ì—”ì§„ ìƒíƒœ ë° í™˜ê²½ ì •ë³´ ì¡°íšŒ

#### `POST /api/ai-agent/optimized`
ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬ ë° ê´€ë¦¬ ì‘ì—… ì‹¤í–‰

**ì§€ì› ì•¡ì…˜:**
- `smart-query`: AI ë¶„ì„ ì‹¤í–‰
- `status`: ì—”ì§„ ìƒíƒœ ì¡°íšŒ
- `environment`: í™˜ê²½ ì •ë³´ ì¡°íšŒ
- `optimize`: ìµœì í™” ì„¤ì • ì¡°íšŒ/ì¡°ì •

### ğŸ“ **ìš”ì²­/ì‘ë‹µ ìŠ¤í‚¤ë§ˆ**

```typescript
// ìŠ¤ë§ˆíŠ¸ ì¿¼ë¦¬ ìš”ì²­
interface SmartQueryRequest {
  action: 'smart-query';
  query: string;                    // ë¶„ì„ ìš”ì²­ (ìµœëŒ€ 1000ì)
  userId?: string;                  // ì‚¬ìš©ì ID
  sessionId?: string;               // ì„¸ì…˜ ID
  context?: Record<string, any>;    // ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
  serverData?: any;                 // ì„œë²„ ë©”íŠ¸ë¦­ ë°ì´í„°
  metadata?: Record<string, any>;   // ë©”íƒ€ë°ì´í„°
  priority?: 'low' | 'medium' | 'high'; // ìš°ì„ ìˆœìœ„
}

// ì‘ë‹µ
interface SmartQueryResponse {
  success: boolean;
  data?: {
    response: string;               // AI ì‘ë‹µ
    method: 'mcp-only' | 'mcp-python' | 'fallback'; // ë¶„ì„ ë°©ë²•
    analysis?: {
      pythonResults?: any;          // Python ë¶„ì„ ê²°ê³¼
      mcpResults?: any;             // MCP íŒ¨í„´ ë§¤ì¹­ ê²°ê³¼
      insights?: string[];          // ì¸ì‚¬ì´íŠ¸
      recommendations?: string[];   // ì¶”ì²œì‚¬í•­
    };
    metadata: {
      processingTime: number;       // ì²˜ë¦¬ ì‹œê°„ (ms)
      memoryUsed: number;          // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
      environment: string;          // ì‹¤í–‰ í™˜ê²½
      optimizationApplied: boolean; // ìµœì í™” ì ìš© ì—¬ë¶€
      timestamp: string;            // íƒ€ì„ìŠ¤íƒ¬í”„
      sessionId: string;            // ì„¸ì…˜ ID
    };
  };
  error?: string;                   // ì—ëŸ¬ ë©”ì‹œì§€
}
```

## ğŸ¯ **ë¡œë“œë§µ**

### ğŸ“… **v3.1 (ë‹¤ìŒ ë²„ì „)**
- [ ] GPU ê°€ì† ì§€ì› (Vercel Pro)
- [ ] ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- [ ] ë‹¤êµ­ì–´ ì§€ì› í™•ì¥
- [ ] ê³ ê¸‰ ì‹œê°í™” ê¸°ëŠ¥

### ğŸ“… **v3.2 (í–¥í›„ ê³„íš)**
- [ ] ë¶„ì‚° ì²˜ë¦¬ ì§€ì›
- [ ] ì»¤ìŠ¤í…€ ML ëª¨ë¸ ì§€ì›
- [ ] ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥
- [ ] ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥ í™•ì¥

## ğŸ¤ **ê¸°ì—¬í•˜ê¸°**

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ **ë¼ì´ì„ ìŠ¤**

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `LICENSE` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ“ **ì§€ì›**

- ğŸ“§ ì´ë©”ì¼: support@openmanager.ai
- ğŸ’¬ Discord: [OpenManager Community](https://discord.gg/openmanager)
- ğŸ“– ë¬¸ì„œ: [docs.openmanager.ai](https://docs.openmanager.ai)
- ğŸ› ì´ìŠˆ: [GitHub Issues](https://github.com/openmanager/issues)

---

**ğŸ‰ OpenManager AI v5 ìµœì í™”ëœ ì—”ì§„ìœ¼ë¡œ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ AI ë¶„ì„ì„ ê²½í—˜í•˜ì„¸ìš”!** 